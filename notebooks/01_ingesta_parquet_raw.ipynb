{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec82c7b-d491-40bf-94ab-451ea7672ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark listo \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"pz4-ingesta-test\")\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark listo \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba23b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas en raw.t_ping = 2\n",
      "+---+------+--------------------------+\n",
      "|id |status|ts_utc                    |\n",
      "+---+------+--------------------------+\n",
      "|1  |ok    |2025-11-08 22:30:49.271737|\n",
      "|2  |ok    |2025-11-08 22:30:49.271737|\n",
      "+---+------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# === 1) Vars de conexión desde el entorno ===\n",
    "PG_HOST = os.environ.get(\"PG_HOST\", \"postgres\")\n",
    "PG_PORT = os.environ.get(\"PG_PORT\", \"5432\")\n",
    "PG_DB   = os.environ.get(\"PG_DB\", \"nyc_taxi\")\n",
    "PG_USER = os.environ.get(\"PG_USER\", \"nyc_user\")\n",
    "PG_PWD  = os.environ.get(\"PG_PASSWORD\", \"nyc_password\")\n",
    "RAW     = os.environ.get(\"PG_SCHEMA_RAW\", \"raw\")\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    "props = {\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PWD,\n",
    "    \"stringtype\": \"unspecified\",  # evita issues con types en write\n",
    "    \"sslmode\": \"disable\"\n",
    "}\n",
    "\n",
    "# === 2) Escribimos una tabla mínima de prueba en raw.t_ping ===\n",
    "df = spark.createDataFrame(\n",
    "    [(1, \"ok\"), (2, \"ok\")],\n",
    "    [\"id\", \"status\"]\n",
    ").withColumn(\"ts_utc\", F.current_timestamp())\n",
    "\n",
    "target_table = f\"{RAW}.t_ping\"\n",
    "\n",
    "# drop previo por si existe (no falla si no existe)\n",
    "spark.read.jdbc(jdbc_url, \"information_schema.tables\", properties=props)  # fuerza el driver\n",
    "spark.sql(f\"select 1\").collect()  # no-op para calentar\n",
    "\n",
    "# write (overwrite) vía JDBC\n",
    "(df.write\n",
    "   .mode(\"overwrite\")\n",
    "   .jdbc(jdbc_url, target_table, properties=props))\n",
    "\n",
    "# === 3) Leemos de vuelta para validar ===\n",
    "out = spark.read.jdbc(jdbc_url, target_table, properties=props)\n",
    "print(\"Filas en raw.t_ping =\", out.count())\n",
    "out.orderBy(\"id\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354e51ac-f4ac-4e83-b2f4-967ebd078d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK: 3.5.3 | Ping: 1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"pz4-reinit\")\n",
    "         .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.7.4\")\n",
    "         .getOrCreate())\n",
    "print(\"Spark OK:\", spark.version, \"| Ping:\", spark.range(1).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa91f56-9655-49d9-a0c8-6805a541ac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT = /home/jovyan/work/data/trip-data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "DATA_ROOT = os.environ.get(\"DATA_ROOT\")\n",
    "RAW = os.environ.get(\"PG_SCHEMA_RAW\", \"raw\")\n",
    "\n",
    "print(\"DATA_ROOT =\", DATA_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92421ef3-98a0-4bd5-9aca-b17618135e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/green_tripdata_2015-01.parquet\n",
      "✅ green 2015-01 cargado en raw.green_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"green\"\n",
    "year = 2015\n",
    "month = 1\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (spark.read.parquet(file)\n",
    "      .withColumn(\"service_type\", F.lit(service))\n",
    "      .withColumn(\"source_year\",  F.lit(year))\n",
    "      .withColumn(\"source_month\", F.lit(month)))\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "\n",
    "# Escribe en lotes moderados y pocas particiones para no tumbar la JVM\n",
    "(df.coalesce(8)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(\"✅ green 2015-01 cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0428eba0-d821-4b64-a7a9-8679d08a01fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/yellow_tripdata_2015-02.parquet\n",
      "✅ yellow 2015-02 cargado en raw.yellow_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"yellow\"\n",
    "year = 2015\n",
    "month = 2\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (spark.read.parquet(file)\n",
    "      .withColumn(\"service_type\", F.lit(service))\n",
    "      .withColumn(\"source_year\",  F.lit(year))\n",
    "      .withColumn(\"source_month\", F.lit(month)))\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "df = df.withColumn(\"source_path\", F.lit(file))\n",
    "# micro-batch seguro\n",
    "(df.coalesce(8)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc565441-d52f-4671-aec7-108ce64186ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/yellow_tripdata_2015-03.parquet\n",
      "✅ yellow 2015-03 cargado en raw.yellow_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"yellow\"\n",
    "year = 2015\n",
    "month = 3\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (spark.read.parquet(file)\n",
    "      .withColumn(\"service_type\", F.lit(service))\n",
    "      .withColumn(\"source_year\",  F.lit(year))\n",
    "      .withColumn(\"source_month\", F.lit(month))\n",
    "      .withColumn(\"source_path\",  F.lit(file))  # trazabilidad requerida por la tabla\n",
    ")\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "\n",
    "(df.coalesce(8)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e318fad5-c79d-4e5f-be8f-061472acb9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/green_tripdata_2015-02.parquet\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- lpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- lpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- ehail_fee: integer (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- trip_type: double (nullable = true)\n",
      " |-- congestion_surcharge: integer (nullable = true)\n",
      " |-- service_type: string (nullable = false)\n",
      " |-- source_year: integer (nullable = false)\n",
      " |-- source_month: integer (nullable = false)\n",
      " |-- source_path: string (nullable = false)\n",
      "\n",
      "✅ green 2015-02 cargado en raw.green_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"green\"   # ← estás cargando green 2015-02\n",
    "year = 2015\n",
    "month = 2\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "# 1) Reconstruimos df en una sola cadena, incluyendo source_path\n",
    "df = (\n",
    "    spark.read.parquet(file)\n",
    "         .withColumn(\"service_type\", F.lit(service))\n",
    "         .withColumn(\"source_year\",  F.lit(year))\n",
    "         .withColumn(\"source_month\", F.lit(month))\n",
    "         .withColumn(\"source_path\",  F.lit(file))\n",
    ")\n",
    "\n",
    "# 2) Validamos que source_path SÍ esté en el DF\n",
    "df.printSchema()\n",
    "\n",
    "# 3) Insert\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "(df.coalesce(4)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ebad84-ec08-4320-bea9-0137f6f47e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/green_tripdata_2015-03.parquet\n",
      "✅ green 2015-03 cargado en raw.green_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"green\"\n",
    "year = 2015\n",
    "month = 3\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (\n",
    "    spark.read.parquet(file)\n",
    "         .withColumn(\"service_type\", F.lit(service))\n",
    "         .withColumn(\"source_year\",  F.lit(year))\n",
    "         .withColumn(\"source_month\", F.lit(month))\n",
    "         .withColumn(\"source_path\",  F.lit(file))\n",
    ")\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "\n",
    "(df.coalesce(4)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce73ab0e-1d67-42ae-971b-0a3b3d6dad18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/yellow_tripdata_2015-04.parquet\n",
      "✅ yellow 2015-04 cargado en raw.yellow_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"yellow\"\n",
    "year = 2015\n",
    "month = 4\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (\n",
    "    spark.read.parquet(file)\n",
    "         .withColumn(\"service_type\", F.lit(service))\n",
    "         .withColumn(\"source_year\",  F.lit(year))\n",
    "         .withColumn(\"source_month\", F.lit(month))\n",
    "         .withColumn(\"source_path\",  F.lit(file))\n",
    ")\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "\n",
    "(df.coalesce(8)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fd46c27-c51d-4dc6-85a8-db562406c780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo: /home/jovyan/work/data/trip-data/green_tripdata_2015-04.parquet\n",
      "✅ green 2015-04 cargado en raw.green_taxi_trip\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "service = \"green\"\n",
    "year = 2015\n",
    "month = 4\n",
    "file = f\"{DATA_ROOT}/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "print(\"Leyendo:\", file)\n",
    "\n",
    "df = (\n",
    "    spark.read.parquet(file)\n",
    "         .withColumn(\"service_type\", F.lit(service))\n",
    "         .withColumn(\"source_year\",  F.lit(year))\n",
    "         .withColumn(\"source_month\", F.lit(month))\n",
    "         .withColumn(\"source_path\",  F.lit(file))\n",
    ")\n",
    "\n",
    "target = f\"{RAW}.{service}_taxi_trip\"\n",
    "\n",
    "(df.coalesce(4)\n",
    "   .write\n",
    "   .mode(\"append\")\n",
    "   .option(\"truncate\", \"false\")\n",
    "   .option(\"batchsize\", \"20000\")\n",
    "   .jdbc(jdbc_url, target, properties=props))\n",
    "\n",
    "print(f\"✅ {service} {year}-{month:02d} cargado en\", target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4bf348-726e-4623-b9b3-ccade2b1c08c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
