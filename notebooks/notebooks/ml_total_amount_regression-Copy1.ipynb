{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac7e2e8-c965-4d24-824d-754ea7012f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.11)\n"
     ]
    }
   ],
   "source": [
    "# Instalar el driver de Postgres para Python\n",
    "!pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97449808-4256-448f-b8b0-86bd292c668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando consulta a Postgres...\n",
      "Shape del dataframe: (1691441, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_dow</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>service_type</th>\n",
       "      <th>pu_borough</th>\n",
       "      <th>do_borough</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>7.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>11.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>7.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_hour  pickup_dow  month  year  trip_distance  passenger_count  \\\n",
       "0           11           6      8  2018           0.84              1.0   \n",
       "1           12           3      8  2018           0.70              2.0   \n",
       "2           12           3      8  2018           2.04              1.0   \n",
       "3            0           5      8  2018          10.62              1.0   \n",
       "4            0           5      8  2018           0.80              1.0   \n",
       "\n",
       "  service_type pu_borough do_borough  total_amount  \n",
       "0       yellow  Manhattan  Manhattan          7.56  \n",
       "1       yellow  Manhattan  Manhattan          8.80  \n",
       "2       yellow  Manhattan  Manhattan         11.80  \n",
       "3       yellow     Queens     Queens         32.80  \n",
       "4       yellow  Manhattan  Manhattan          7.80  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. Cargar una MUESTRA pickup-only desde Postgres\n",
    "#    (sin leakage, 2018-2023, ~0.5% de las filas)\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Parámetros de conexión (los mismos que en los otros notebooks)\n",
    "PG_HOST = os.getenv(\"PG_HOST\", \"postgres\")\n",
    "PG_PORT = os.getenv(\"PG_PORT\", \"5432\")\n",
    "PG_DB   = os.getenv(\"PG_DB\", \"nyctaxi\")\n",
    "PG_USER = os.getenv(\"PG_USER\", \"pset\")\n",
    "PG_PWD  = os.getenv(\"PG_PASSWORD\", \"pset_password\")\n",
    "\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{PG_USER}:{PG_PWD}@{PG_HOST}:{PG_PORT}/{PG_DB}\"\n",
    ")\n",
    "\n",
    "# ⚠️ IMPORTANTE:\n",
    "# - Solo columnas permitidas en pickup (nada de fare_amount, tip_pct, etc.)\n",
    "# - Muestra aleatoria con random() < 0.005 (0.5%)\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    pickup_hour,\n",
    "    pickup_dow,\n",
    "    month,\n",
    "    year,\n",
    "    trip_distance,\n",
    "    passenger_count,\n",
    "    service_type,\n",
    "    pu_borough,\n",
    "    do_borough,\n",
    "    total_amount\n",
    "FROM analytics.obt_trips\n",
    "WHERE source_year BETWEEN 2018 AND 2023\n",
    "  AND random() < 0.005\n",
    "\"\"\"\n",
    "\n",
    "print(\"Ejecutando consulta a Postgres...\")\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "print(\"Shape del dataframe:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5031b6d-a7ca-4455-9333-6feea24d28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape final df_clean: (1608337, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_dow</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>service_type</th>\n",
       "      <th>pu_borough</th>\n",
       "      <th>do_borough</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>7.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>8.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>11.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Queens</td>\n",
       "      <td>Queens</td>\n",
       "      <td>32.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yellow</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>7.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_hour  pickup_dow  month  year  trip_distance  passenger_count  \\\n",
       "0           11           6      8  2018           0.84              1.0   \n",
       "1           12           3      8  2018           0.70              2.0   \n",
       "2           12           3      8  2018           2.04              1.0   \n",
       "3            0           5      8  2018          10.62              1.0   \n",
       "4            0           5      8  2018           0.80              1.0   \n",
       "\n",
       "  service_type pu_borough do_borough  total_amount  \n",
       "0       yellow  Manhattan  Manhattan          7.56  \n",
       "1       yellow  Manhattan  Manhattan          8.80  \n",
       "2       yellow  Manhattan  Manhattan         11.80  \n",
       "3       yellow     Queens     Queens         32.80  \n",
       "4       yellow  Manhattan  Manhattan          7.80  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Limpieza básica (pickup-only, sin leakage)\n",
    "# ============================================\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Filtros sugeridos y permitidos según el PDF\n",
    "df_clean = df_clean[\n",
    "    (df_clean[\"trip_distance\"] > 0) &\n",
    "    (df_clean[\"trip_distance\"] < 100) &\n",
    "    (df_clean[\"passenger_count\"] > 0) &\n",
    "    (df_clean[\"passenger_count\"] < 7) &\n",
    "    (df_clean[\"total_amount\"] > 0) &\n",
    "    (df_clean[\"total_amount\"] < 500)\n",
    "]\n",
    "\n",
    "# Eliminar nulos\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Convertir categóricas\n",
    "cat_cols = [\"service_type\", \"pu_borough\", \"do_borough\"]\n",
    "for c in cat_cols:\n",
    "    df_clean[c] = df_clean[c].astype(\"category\")\n",
    "\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "print(\"Shape final df_clean:\", df_clean.shape)\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c303dbea-86e4-46dc-aaff-ddc73e2a7e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X: (1608337, 24)\n",
      "Shape y: (1608337,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_dow</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>service_type_green</th>\n",
       "      <th>service_type_yellow</th>\n",
       "      <th>pu_borough_Bronx</th>\n",
       "      <th>pu_borough_Brooklyn</th>\n",
       "      <th>...</th>\n",
       "      <th>pu_borough_Staten Island</th>\n",
       "      <th>pu_borough_Unknown</th>\n",
       "      <th>do_borough_Bronx</th>\n",
       "      <th>do_borough_Brooklyn</th>\n",
       "      <th>do_borough_EWR</th>\n",
       "      <th>do_borough_Manhattan</th>\n",
       "      <th>do_borough_N/A</th>\n",
       "      <th>do_borough_Queens</th>\n",
       "      <th>do_borough_Staten Island</th>\n",
       "      <th>do_borough_Unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>10.62</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2018</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickup_hour  pickup_dow  month  year  trip_distance  passenger_count  \\\n",
       "0           11           6      8  2018           0.84              1.0   \n",
       "1           12           3      8  2018           0.70              2.0   \n",
       "2           12           3      8  2018           2.04              1.0   \n",
       "3            0           5      8  2018          10.62              1.0   \n",
       "4            0           5      8  2018           0.80              1.0   \n",
       "\n",
       "   service_type_green  service_type_yellow  pu_borough_Bronx  \\\n",
       "0               False                 True             False   \n",
       "1               False                 True             False   \n",
       "2               False                 True             False   \n",
       "3               False                 True             False   \n",
       "4               False                 True             False   \n",
       "\n",
       "   pu_borough_Brooklyn  ...  pu_borough_Staten Island  pu_borough_Unknown  \\\n",
       "0                False  ...                     False               False   \n",
       "1                False  ...                     False               False   \n",
       "2                False  ...                     False               False   \n",
       "3                False  ...                     False               False   \n",
       "4                False  ...                     False               False   \n",
       "\n",
       "   do_borough_Bronx  do_borough_Brooklyn  do_borough_EWR  \\\n",
       "0             False                False           False   \n",
       "1             False                False           False   \n",
       "2             False                False           False   \n",
       "3             False                False           False   \n",
       "4             False                False           False   \n",
       "\n",
       "   do_borough_Manhattan  do_borough_N/A  do_borough_Queens  \\\n",
       "0                  True           False              False   \n",
       "1                  True           False              False   \n",
       "2                  True           False              False   \n",
       "3                 False           False               True   \n",
       "4                  True           False              False   \n",
       "\n",
       "   do_borough_Staten Island  do_borough_Unknown  \n",
       "0                     False               False  \n",
       "1                     False               False  \n",
       "2                     False               False  \n",
       "3                     False               False  \n",
       "4                     False               False  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. One-Hot Encoding + Separar X / y\n",
    "# ============================================\n",
    "\n",
    "df_ml = df_clean.copy()\n",
    "\n",
    "# Columnas numéricas\n",
    "num_cols = [\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_dow\",\n",
    "    \"month\",\n",
    "    \"year\",\n",
    "    \"trip_distance\",\n",
    "    \"passenger_count\",\n",
    "]\n",
    "\n",
    "# Columnas categóricas\n",
    "cat_cols = [\"service_type\", \"pu_borough\", \"do_borough\"]\n",
    "\n",
    "# Target\n",
    "target_col = \"total_amount\"\n",
    "\n",
    "# OHE (drop_first=False para no perder información)\n",
    "df_ml = pd.get_dummies(df_ml, columns=cat_cols, drop_first=False)\n",
    "\n",
    "# Separar X e y\n",
    "X = df_ml.drop(columns=[target_col])\n",
    "y = df_ml[target_col]\n",
    "\n",
    "print(\"Shape X:\", X.shape)\n",
    "print(\"Shape y:\", y.shape)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e0e3e9d-f4e7-4868-a7af-0d33e0512e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1105692, 24) (1105692,)\n",
      "Val:   (143920, 24) (143920,)\n",
      "Test:  (358725, 24) (358725,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. Split temporal (obligatorio según PDF)\n",
    "# ============================================\n",
    "\n",
    "# Necesitamos la columna 'year' del df_ml ya convertido\n",
    "years = df_ml[\"year\"].values\n",
    "\n",
    "# Train: 2018–2020\n",
    "train_mask = (years >= 2018) & (years <= 2020)\n",
    "\n",
    "# Validation: 2021\n",
    "val_mask = (years == 2021)\n",
    "\n",
    "# Test: 2022–2023\n",
    "test_mask = (years >= 2022) & (years <= 2023)\n",
    "\n",
    "X_train = X[train_mask]\n",
    "y_train = y[train_mask]\n",
    "\n",
    "X_val   = X[val_mask]\n",
    "y_val   = y[val_mask]\n",
    "\n",
    "X_test  = X[test_mask]\n",
    "y_test  = y[test_mask]\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val:  \", X_val.shape, y_val.shape)\n",
    "print(\"Test: \", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c124122-7778-440f-9d6d-b326430fe5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes X (scaled): (1105692, 24) (143920, 24) (358725, 24)\n",
      "Shapes y: (1105692,) (143920,) (358725,)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. StandardScaler from-scratch + NumPy arrays\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class StandardScalerScratch:\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        X: matriz NumPy de shape (n_muestras, n_features)\n",
    "        \"\"\"\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        self.std_  = X.std(axis=0)\n",
    "        # evitar división por cero\n",
    "        self.std_[self.std_ == 0] = 1.0\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.std_\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "# Pasar X a NumPy\n",
    "X_train_np = X_train.values.astype(np.float64)\n",
    "X_val_np   = X_val.values.astype(np.float64)\n",
    "X_test_np  = X_test.values.astype(np.float64)\n",
    "\n",
    "# Pasar y a NumPy (reshape a vector 1D)\n",
    "y_train_np = y_train.values.astype(np.float64)\n",
    "y_val_np   = y_val.values.astype(np.float64)\n",
    "y_test_np  = y_test.values.astype(np.float64)\n",
    "\n",
    "# Ajustar scaler SOLO con train y aplicar a todo\n",
    "scaler = StandardScalerScratch()\n",
    "X_train_scaled = scaler.fit_transform(X_train_np)\n",
    "X_val_scaled   = scaler.transform(X_val_np)\n",
    "X_test_scaled  = scaler.transform(X_test_np)\n",
    "\n",
    "print(\"Shapes X (scaled):\",\n",
    "      X_train_scaled.shape,\n",
    "      X_val_scaled.shape,\n",
    "      X_test_scaled.shape)\n",
    "\n",
    "print(\"Shapes y:\",\n",
    "      y_train_np.shape,\n",
    "      y_val_np.shape,\n",
    "      y_test_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60fca5e5-83a4-4cd1-8a75-cee916933af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7. Modelo Lineal desde cero (sin regularización)\n",
    "# ============================================\n",
    "\n",
    "class LinearRegressorScratch:\n",
    "    def __init__(self, learning_rate=1e-3, max_iter=20, batch_size=1024, random_state=42):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _init_params(self, n_features):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.w = rng.normal(0, 0.01, size=n_features)\n",
    "        self.b = 0.0\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w + self.b\n",
    "\n",
    "    def _mse(self, y, y_pred):\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self._init_params(n_features)\n",
    "\n",
    "        self.history_ = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            # Mini-batch SGD\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_train = X_train[idx]\n",
    "            y_train = y_train[idx]\n",
    "\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                xb = X_train[start:end]\n",
    "                yb = y_train[start:end]\n",
    "\n",
    "                y_pred = xb @ self.w + self.b\n",
    "\n",
    "                # Gradientes\n",
    "                grad_w = -2 * xb.T @ (yb - y_pred) / len(xb)\n",
    "                grad_b = -2 * np.mean(yb - y_pred)\n",
    "\n",
    "                # Actualización\n",
    "                self.w -= self.learning_rate * grad_w\n",
    "                self.b -= self.learning_rate * grad_b\n",
    "\n",
    "            # Registrar pérdidas\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            train_loss = self._mse(y_train, y_train_pred)\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if X_val is not None:\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self._mse(y_val, y_val_pred)\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.max_iter} - \"\n",
    "                  f\"Train MSE: {train_loss:.4f} - \"\n",
    "                  f\"Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04112a2c-063b-4e5f-a13e-ec40edbba8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train MSE: 79.2585 - Val MSE: 66.4898\n",
      "Epoch 2/20 - Train MSE: 35.0379 - Val MSE: 31.7351\n",
      "Epoch 3/20 - Train MSE: 27.5988 - Val MSE: 32.0477\n",
      "Epoch 4/20 - Train MSE: 25.7881 - Val MSE: 33.6679\n",
      "Epoch 5/20 - Train MSE: 25.2214 - Val MSE: 34.4668\n",
      "Epoch 6/20 - Train MSE: 25.0180 - Val MSE: 34.8835\n",
      "Epoch 7/20 - Train MSE: 24.9438 - Val MSE: 35.1233\n",
      "Epoch 8/20 - Train MSE: 24.9140 - Val MSE: 35.2447\n",
      "Epoch 9/20 - Train MSE: 24.9032 - Val MSE: 35.2786\n",
      "Epoch 10/20 - Train MSE: 24.8988 - Val MSE: 35.2903\n",
      "Epoch 11/20 - Train MSE: 24.8972 - Val MSE: 35.3210\n",
      "Epoch 12/20 - Train MSE: 24.8961 - Val MSE: 35.3619\n",
      "Epoch 13/20 - Train MSE: 24.8969 - Val MSE: 35.3666\n",
      "Epoch 14/20 - Train MSE: 24.8966 - Val MSE: 35.3728\n",
      "Epoch 15/20 - Train MSE: 24.8961 - Val MSE: 35.3960\n",
      "Epoch 16/20 - Train MSE: 24.8972 - Val MSE: 35.4096\n",
      "Epoch 17/20 - Train MSE: 24.8962 - Val MSE: 35.3813\n",
      "Epoch 18/20 - Train MSE: 24.8958 - Val MSE: 35.3481\n",
      "Epoch 19/20 - Train MSE: 24.8960 - Val MSE: 35.3754\n",
      "Epoch 20/20 - Train MSE: 24.8964 - Val MSE: 35.3392\n",
      "=== Lineal base (sin regularización) ===\n",
      "RMSE_train: 4.9896\n",
      "R2_train  : 0.871929\n",
      "RMSE_val  : 5.9447\n",
      "R2_val    : 0.825245\n",
      "\n",
      "Primeras 5 pérdidas (train): [79.25847607348109, 35.03788204120961, 27.598755687392288, 25.78813454272103, 25.221431329526574]\n",
      "Primeras 5 pérdidas (val):   [66.48975133937037, 31.73513031826786, 32.047737987525416, 33.66786893121203, 34.46684472521296]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8. Entrenar modelo lineal base (sin regularización)\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Instanciar modelo base\n",
    "lin_base = LinearRegressorScratch(\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=20,       # luego podemos subir si hace falta\n",
    "    batch_size=2048,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Entrenar usando train y validar con val\n",
    "lin_base.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_np,\n",
    "    X_val=X_val_scaled,\n",
    "    y_val=y_val_np\n",
    ")\n",
    "\n",
    "# Predicciones\n",
    "y_train_pred = lin_base.predict(X_train_scaled)\n",
    "y_val_pred   = lin_base.predict(X_val_scaled)\n",
    "\n",
    "# Métricas\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train_np, y_train_pred))\n",
    "rmse_val   = np.sqrt(mean_squared_error(y_val_np, y_val_pred))\n",
    "\n",
    "r2_train = r2_score(y_train_np, y_train_pred)\n",
    "r2_val   = r2_score(y_val_np, y_val_pred)\n",
    "\n",
    "print(\"=== Lineal base (sin regularización) ===\")\n",
    "print(f\"RMSE_train: {rmse_train:.4f}\")\n",
    "print(f\"R2_train  : {r2_train:.6f}\")\n",
    "print(f\"RMSE_val  : {rmse_val:.4f}\")\n",
    "print(f\"R2_val    : {r2_val:.6f}\")\n",
    "\n",
    "print(\"\\nPrimeras 5 pérdidas (train):\", lin_base.history_[\"train_loss\"][:5])\n",
    "print(\"Primeras 5 pérdidas (val):  \", lin_base.history_[\"val_loss\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ffe5ba4-aebc-4211-8aaa-97c98aa85e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# 9. Redefinir LinearRegressorScratch con regularización\n",
    "#     (Ridge, Lasso, ElasticNet)\n",
    "# ======================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressorScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate=1e-3,\n",
    "        max_iter=20,\n",
    "        batch_size=1024,\n",
    "        penalty=None,      # None, \"l2\", \"l1\", \"elasticnet\"\n",
    "        alpha=0.0,         # fuerza de regularización (lambda)\n",
    "        l1_ratio=0.5,      # solo para elasticnet\n",
    "        random_state=42,\n",
    "    ):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.l1_ratio = l1_ratio\n",
    "        self.random_state = random_state\n",
    "        self.history_ = {}\n",
    "\n",
    "    def _init_params(self, n_features):\n",
    "        rng = np.random.default_rng(self.random_state)\n",
    "        self.w_ = rng.normal(0, 0.01, size=n_features)\n",
    "        self.b_ = 0.0\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X @ self.w_ + self.b_\n",
    "\n",
    "    def _mse(self, y, y_pred):\n",
    "        return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "    def _reg_loss(self):\n",
    "        \"\"\"Término de regularización para sumar a la pérdida.\"\"\"\n",
    "        if self.penalty is None or self.alpha == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.penalty == \"l2\":\n",
    "            return self.alpha * np.sum(self.w_ ** 2)\n",
    "\n",
    "        if self.penalty == \"l1\":\n",
    "            return self.alpha * np.sum(np.abs(self.w_))\n",
    "\n",
    "        if self.penalty == \"elasticnet\":\n",
    "            l1 = self.l1_ratio * np.sum(np.abs(self.w_))\n",
    "            l2 = (1.0 - self.l1_ratio) * np.sum(self.w_ ** 2)\n",
    "            return self.alpha * (l1 + l2)\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def _reg_gradient(self):\n",
    "        \"\"\"Gradiente de la parte de regularización respecto a w_.\"\"\"\n",
    "        if self.penalty is None or self.alpha == 0.0:\n",
    "            return 0.0\n",
    "\n",
    "        if self.penalty == \"l2\":\n",
    "            return 2.0 * self.alpha * self.w_\n",
    "\n",
    "        if self.penalty == \"l1\":\n",
    "            return self.alpha * np.sign(self.w_)\n",
    "\n",
    "        if self.penalty == \"elasticnet\":\n",
    "            grad_l2 = 2.0 * (1.0 - self.l1_ratio) * self.alpha * self.w_\n",
    "            grad_l1 = self.l1_ratio * self.alpha * np.sign(self.w_)\n",
    "            return grad_l1 + grad_l2\n",
    "\n",
    "        return 0.0\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        X_train = X_train.astype(float)\n",
    "        y_train = y_train.astype(float)\n",
    "\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self._init_params(n_features)\n",
    "\n",
    "        self.history_ = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            # barajar\n",
    "            idx = np.random.permutation(n_samples)\n",
    "            X_train = X_train[idx]\n",
    "            y_train = y_train[idx]\n",
    "\n",
    "            # mini-batch SGD\n",
    "            for start in range(0, n_samples, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                xb = X_train[start:end]\n",
    "                yb = y_train[start:end]\n",
    "\n",
    "                if xb.shape[0] == 0:\n",
    "                    continue\n",
    "\n",
    "                y_pred = xb @ self.w_ + self.b_\n",
    "                error = y_pred - yb\n",
    "\n",
    "                # gradiente MSE puro\n",
    "                grad_w = (2.0 / xb.shape[0]) * xb.T @ error\n",
    "                grad_b = 2.0 * np.mean(error)\n",
    "\n",
    "                # añadir gradiente de regularización\n",
    "                grad_w += self._reg_gradient()\n",
    "\n",
    "                # actualización\n",
    "                self.w_ -= self.learning_rate * grad_w\n",
    "                self.b_ -= self.learning_rate * grad_b\n",
    "\n",
    "            # pérdidas al final de la época\n",
    "            y_train_pred = self.predict(X_train)\n",
    "            train_loss = self._mse(y_train, y_train_pred) + self._reg_loss()\n",
    "            self.history_[\"train_loss\"].append(train_loss)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self.predict(X_val)\n",
    "                val_loss = self._mse(y_val, y_val_pred) + self._reg_loss()\n",
    "                self.history_[\"val_loss\"].append(val_loss)\n",
    "            else:\n",
    "                val_loss = None\n",
    "\n",
    "            msg = f\"Epoch {epoch+1}/{self.max_iter} - Train loss: {train_loss:.4f}\"\n",
    "            if val_loss is not None:\n",
    "                msg += f\" - Val loss: {val_loss:.4f}\"\n",
    "            print(msg)\n",
    "\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72ff640-84c8-43f2-8395-9e5866b20217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Train loss: 79.2068 - Val loss: 66.4953\n",
      "Epoch 2/40 - Train loss: 35.0494 - Val loss: 31.7334\n",
      "Epoch 3/40 - Train loss: 27.6126 - Val loss: 32.0499\n",
      "Epoch 4/40 - Train loss: 25.8089 - Val loss: 33.6518\n",
      "Epoch 5/40 - Train loss: 25.2387 - Val loss: 34.4822\n",
      "Epoch 6/40 - Train loss: 25.0344 - Val loss: 34.9358\n",
      "Epoch 7/40 - Train loss: 24.9596 - Val loss: 35.1330\n",
      "Epoch 8/40 - Train loss: 24.9309 - Val loss: 35.2106\n",
      "Epoch 9/40 - Train loss: 24.9195 - Val loss: 35.3041\n",
      "Epoch 10/40 - Train loss: 24.9149 - Val loss: 35.3450\n",
      "Epoch 11/40 - Train loss: 24.9135 - Val loss: 35.3429\n",
      "Epoch 12/40 - Train loss: 24.9125 - Val loss: 35.3557\n",
      "Epoch 13/40 - Train loss: 24.9125 - Val loss: 35.3701\n",
      "Epoch 14/40 - Train loss: 24.9127 - Val loss: 35.3857\n",
      "Epoch 15/40 - Train loss: 24.9122 - Val loss: 35.4132\n",
      "Epoch 16/40 - Train loss: 24.9124 - Val loss: 35.4124\n",
      "Epoch 17/40 - Train loss: 24.9128 - Val loss: 35.4059\n",
      "Epoch 18/40 - Train loss: 24.9127 - Val loss: 35.4016\n",
      "Epoch 19/40 - Train loss: 24.9125 - Val loss: 35.3872\n",
      "Epoch 20/40 - Train loss: 24.9122 - Val loss: 35.3918\n",
      "Epoch 21/40 - Train loss: 24.9125 - Val loss: 35.3709\n",
      "Epoch 22/40 - Train loss: 24.9127 - Val loss: 35.4009\n",
      "Epoch 23/40 - Train loss: 24.9126 - Val loss: 35.4042\n",
      "Epoch 24/40 - Train loss: 24.9126 - Val loss: 35.4119\n",
      "Epoch 25/40 - Train loss: 24.9122 - Val loss: 35.3935\n",
      "Epoch 26/40 - Train loss: 24.9134 - Val loss: 35.4189\n",
      "Epoch 27/40 - Train loss: 24.9130 - Val loss: 35.4093\n",
      "Epoch 28/40 - Train loss: 24.9129 - Val loss: 35.3966\n",
      "Epoch 29/40 - Train loss: 24.9124 - Val loss: 35.3561\n",
      "Epoch 30/40 - Train loss: 24.9124 - Val loss: 35.3734\n",
      "Epoch 31/40 - Train loss: 24.9122 - Val loss: 35.3871\n",
      "Epoch 32/40 - Train loss: 24.9126 - Val loss: 35.3833\n",
      "Epoch 33/40 - Train loss: 24.9142 - Val loss: 35.3726\n",
      "Epoch 34/40 - Train loss: 24.9128 - Val loss: 35.4172\n",
      "Epoch 35/40 - Train loss: 24.9126 - Val loss: 35.3944\n",
      "Epoch 36/40 - Train loss: 24.9134 - Val loss: 35.4266\n",
      "Epoch 37/40 - Train loss: 24.9127 - Val loss: 35.3846\n",
      "Epoch 38/40 - Train loss: 24.9124 - Val loss: 35.3875\n",
      "Epoch 39/40 - Train loss: 24.9131 - Val loss: 35.3600\n",
      "Epoch 40/40 - Train loss: 24.9123 - Val loss: 35.3929\n",
      "=== Ridge (from-scratch) ===\n",
      "RMSE_train: 4.9896\n",
      "R2_train  : 0.8719\n",
      "RMSE_val  : 5.9478\n",
      "R2_val    : 0.8251\n",
      "\n",
      "Primeras 5 pérdidas (train): [79.20677218753995, 35.04939776359097, 27.612628603466202, 25.80892993736343, 25.238705841065496]\n",
      "Primeras 5 pérdidas (val):   [66.49530936098083, 31.73338245602399, 32.04993301562683, 33.651777093347825, 34.48215333536478]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 10. Entrenar Ridge (modelo scratch) - Pickup Only\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "\n",
    "def r2(y, y_pred):\n",
    "    ss_res = np.sum((y - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y - y.mean()) ** 2)\n",
    "    return 1 - ss_res / ss_tot\n",
    "\n",
    "\n",
    "ridge = LinearRegressorScratch(\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=40,        # un poco más que el lineal base\n",
    "    batch_size=2048,\n",
    "    penalty=\"l2\",       # <<< Ridge activado\n",
    "    alpha=1e-4,         # fuerza de regularización (pequeña y estable)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ridge.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_np,\n",
    "    X_val=X_val_scaled,\n",
    "    y_val=y_val_np\n",
    ")\n",
    "\n",
    "# ========================\n",
    "# Métricas\n",
    "# ========================\n",
    "\n",
    "y_train_pred = ridge.predict(X_train_scaled)\n",
    "y_val_pred   = ridge.predict(X_val_scaled)\n",
    "\n",
    "print(\"=== Ridge (from-scratch) ===\")\n",
    "print(f\"RMSE_train: {rmse(y_train_np, y_train_pred):.4f}\")\n",
    "print(f\"R2_train  : {r2(y_train_np, y_train_pred):.4f}\")\n",
    "print(f\"RMSE_val  : {rmse(y_val_np, y_val_pred):.4f}\")\n",
    "print(f\"R2_val    : {r2(y_val_np, y_val_pred):.4f}\")\n",
    "\n",
    "print(\"\\nPrimeras 5 pérdidas (train):\", ridge.history_[\"train_loss\"][:5])\n",
    "print(\"Primeras 5 pérdidas (val):  \", ridge.history_[\"val_loss\"][:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60fe16ef-6c72-435a-9ca6-4dedfa92ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Ridge (L2) sobre distintos alpha:\n",
      "\n",
      "--- alpha = 0.0 ---\n",
      "Epoch 1/40 - Train loss: 79.1536 - Val loss: 66.4652\n",
      "Epoch 2/40 - Train loss: 35.0228 - Val loss: 31.7208\n",
      "Epoch 3/40 - Train loss: 27.5935 - Val loss: 32.0536\n",
      "Epoch 4/40 - Train loss: 25.7891 - Val loss: 33.6548\n",
      "Epoch 5/40 - Train loss: 25.2223 - Val loss: 34.5015\n",
      "Epoch 6/40 - Train loss: 25.0197 - Val loss: 34.9002\n",
      "Epoch 7/40 - Train loss: 24.9436 - Val loss: 35.0857\n",
      "Epoch 8/40 - Train loss: 24.9149 - Val loss: 35.2093\n",
      "Epoch 9/40 - Train loss: 24.9032 - Val loss: 35.2844\n",
      "Epoch 10/40 - Train loss: 24.8990 - Val loss: 35.3270\n",
      "Epoch 11/40 - Train loss: 24.8974 - Val loss: 35.3572\n",
      "Epoch 12/40 - Train loss: 24.8967 - Val loss: 35.4068\n",
      "Epoch 13/40 - Train loss: 24.8961 - Val loss: 35.3680\n",
      "Epoch 14/40 - Train loss: 24.8966 - Val loss: 35.4041\n",
      "Epoch 15/40 - Train loss: 24.8961 - Val loss: 35.3792\n",
      "Epoch 16/40 - Train loss: 24.8964 - Val loss: 35.3774\n",
      "Epoch 17/40 - Train loss: 24.8968 - Val loss: 35.3892\n",
      "Epoch 18/40 - Train loss: 24.8974 - Val loss: 35.3984\n",
      "Epoch 19/40 - Train loss: 24.8962 - Val loss: 35.3745\n",
      "Epoch 20/40 - Train loss: 24.8961 - Val loss: 35.4175\n",
      "Epoch 21/40 - Train loss: 24.8961 - Val loss: 35.4008\n",
      "Epoch 22/40 - Train loss: 24.8960 - Val loss: 35.3904\n",
      "Epoch 23/40 - Train loss: 24.8959 - Val loss: 35.3648\n",
      "Epoch 24/40 - Train loss: 24.8961 - Val loss: 35.3871\n",
      "Epoch 25/40 - Train loss: 24.8961 - Val loss: 35.3536\n",
      "Epoch 26/40 - Train loss: 24.8959 - Val loss: 35.3570\n",
      "Epoch 27/40 - Train loss: 24.8970 - Val loss: 35.3811\n",
      "Epoch 28/40 - Train loss: 24.8961 - Val loss: 35.3460\n",
      "Epoch 29/40 - Train loss: 24.8958 - Val loss: 35.3628\n",
      "Epoch 30/40 - Train loss: 24.8968 - Val loss: 35.3816\n",
      "Epoch 31/40 - Train loss: 24.8962 - Val loss: 35.4031\n",
      "Epoch 32/40 - Train loss: 24.8961 - Val loss: 35.3696\n",
      "Epoch 33/40 - Train loss: 24.8978 - Val loss: 35.3848\n",
      "Epoch 34/40 - Train loss: 24.8962 - Val loss: 35.4037\n",
      "Epoch 35/40 - Train loss: 24.8959 - Val loss: 35.3854\n",
      "Epoch 36/40 - Train loss: 24.8960 - Val loss: 35.4230\n",
      "Epoch 37/40 - Train loss: 24.8963 - Val loss: 35.3871\n",
      "Epoch 38/40 - Train loss: 24.8959 - Val loss: 35.3560\n",
      "Epoch 39/40 - Train loss: 24.8960 - Val loss: 35.3826\n",
      "Epoch 40/40 - Train loss: 24.8960 - Val loss: 35.4065\n",
      "RMSE_train=4.9896 | RMSE_val=5.9503 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "--- alpha = 1e-05 ---\n",
      "Epoch 1/40 - Train loss: 79.2208 - Val loss: 66.4959\n",
      "Epoch 2/40 - Train loss: 35.0411 - Val loss: 31.7301\n",
      "Epoch 3/40 - Train loss: 27.6060 - Val loss: 32.0505\n",
      "Epoch 4/40 - Train loss: 25.7952 - Val loss: 33.6696\n",
      "Epoch 5/40 - Train loss: 25.2264 - Val loss: 34.4510\n",
      "Epoch 6/40 - Train loss: 25.0204 - Val loss: 34.9196\n",
      "Epoch 7/40 - Train loss: 24.9443 - Val loss: 35.1029\n",
      "Epoch 8/40 - Train loss: 24.9155 - Val loss: 35.1975\n",
      "Epoch 9/40 - Train loss: 24.9046 - Val loss: 35.2907\n",
      "Epoch 10/40 - Train loss: 24.9006 - Val loss: 35.3333\n",
      "Epoch 11/40 - Train loss: 24.8988 - Val loss: 35.3518\n",
      "Epoch 12/40 - Train loss: 24.8981 - Val loss: 35.3618\n",
      "Epoch 13/40 - Train loss: 24.8993 - Val loss: 35.3554\n",
      "Epoch 14/40 - Train loss: 24.8985 - Val loss: 35.3401\n",
      "Epoch 15/40 - Train loss: 24.8975 - Val loss: 35.3588\n",
      "Epoch 16/40 - Train loss: 24.8976 - Val loss: 35.3707\n",
      "Epoch 17/40 - Train loss: 24.8977 - Val loss: 35.3811\n",
      "Epoch 18/40 - Train loss: 24.8982 - Val loss: 35.3681\n",
      "Epoch 19/40 - Train loss: 24.8981 - Val loss: 35.3655\n",
      "Epoch 20/40 - Train loss: 24.8982 - Val loss: 35.3521\n",
      "Epoch 21/40 - Train loss: 24.8978 - Val loss: 35.3820\n",
      "Epoch 22/40 - Train loss: 24.8978 - Val loss: 35.3864\n",
      "Epoch 23/40 - Train loss: 24.8990 - Val loss: 35.4159\n",
      "Epoch 24/40 - Train loss: 24.8983 - Val loss: 35.4083\n",
      "Epoch 25/40 - Train loss: 24.8980 - Val loss: 35.3960\n",
      "Epoch 26/40 - Train loss: 24.8984 - Val loss: 35.3687\n",
      "Epoch 27/40 - Train loss: 24.8977 - Val loss: 35.3971\n",
      "Epoch 28/40 - Train loss: 24.8977 - Val loss: 35.3838\n",
      "Epoch 29/40 - Train loss: 24.8974 - Val loss: 35.3761\n",
      "Epoch 30/40 - Train loss: 24.8975 - Val loss: 35.3897\n",
      "Epoch 31/40 - Train loss: 24.8977 - Val loss: 35.3933\n",
      "Epoch 32/40 - Train loss: 24.8974 - Val loss: 35.4182\n",
      "Epoch 33/40 - Train loss: 24.8981 - Val loss: 35.4265\n",
      "Epoch 34/40 - Train loss: 24.8975 - Val loss: 35.4278\n",
      "Epoch 35/40 - Train loss: 24.8982 - Val loss: 35.3617\n",
      "Epoch 36/40 - Train loss: 24.8976 - Val loss: 35.3599\n",
      "Epoch 37/40 - Train loss: 24.8975 - Val loss: 35.3838\n",
      "Epoch 38/40 - Train loss: 24.8978 - Val loss: 35.3760\n",
      "Epoch 39/40 - Train loss: 24.8985 - Val loss: 35.3615\n",
      "Epoch 40/40 - Train loss: 24.8993 - Val loss: 35.4192\n",
      "RMSE_train=4.9898 | RMSE_val=5.9513 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "--- alpha = 5e-05 ---\n",
      "Epoch 1/40 - Train loss: 79.1753 - Val loss: 66.4285\n",
      "Epoch 2/40 - Train loss: 35.0208 - Val loss: 31.7190\n",
      "Epoch 3/40 - Train loss: 27.5922 - Val loss: 32.0502\n",
      "Epoch 4/40 - Train loss: 25.7958 - Val loss: 33.7017\n",
      "Epoch 5/40 - Train loss: 25.2272 - Val loss: 34.5285\n",
      "Epoch 6/40 - Train loss: 25.0277 - Val loss: 34.9234\n",
      "Epoch 7/40 - Train loss: 24.9514 - Val loss: 35.1454\n",
      "Epoch 8/40 - Train loss: 24.9219 - Val loss: 35.2514\n",
      "Epoch 9/40 - Train loss: 24.9108 - Val loss: 35.3134\n",
      "Epoch 10/40 - Train loss: 24.9078 - Val loss: 35.3563\n",
      "Epoch 11/40 - Train loss: 24.9054 - Val loss: 35.3810\n",
      "Epoch 12/40 - Train loss: 24.9043 - Val loss: 35.3726\n",
      "Epoch 13/40 - Train loss: 24.9043 - Val loss: 35.3933\n",
      "Epoch 14/40 - Train loss: 24.9041 - Val loss: 35.4135\n",
      "Epoch 15/40 - Train loss: 24.9045 - Val loss: 35.4239\n",
      "Epoch 16/40 - Train loss: 24.9042 - Val loss: 35.4115\n",
      "Epoch 17/40 - Train loss: 24.9046 - Val loss: 35.3678\n",
      "Epoch 18/40 - Train loss: 24.9042 - Val loss: 35.3408\n",
      "Epoch 19/40 - Train loss: 24.9053 - Val loss: 35.3786\n",
      "Epoch 20/40 - Train loss: 24.9041 - Val loss: 35.3895\n",
      "Epoch 21/40 - Train loss: 24.9039 - Val loss: 35.4025\n",
      "Epoch 22/40 - Train loss: 24.9041 - Val loss: 35.3940\n",
      "Epoch 23/40 - Train loss: 24.9048 - Val loss: 35.3639\n",
      "Epoch 24/40 - Train loss: 24.9045 - Val loss: 35.3984\n",
      "Epoch 25/40 - Train loss: 24.9045 - Val loss: 35.3927\n",
      "Epoch 26/40 - Train loss: 24.9054 - Val loss: 35.3626\n",
      "Epoch 27/40 - Train loss: 24.9040 - Val loss: 35.3993\n",
      "Epoch 28/40 - Train loss: 24.9045 - Val loss: 35.3638\n",
      "Epoch 29/40 - Train loss: 24.9044 - Val loss: 35.3408\n",
      "Epoch 30/40 - Train loss: 24.9043 - Val loss: 35.3641\n",
      "Epoch 31/40 - Train loss: 24.9049 - Val loss: 35.3778\n",
      "Epoch 32/40 - Train loss: 24.9050 - Val loss: 35.3632\n",
      "Epoch 33/40 - Train loss: 24.9040 - Val loss: 35.4156\n",
      "Epoch 34/40 - Train loss: 24.9039 - Val loss: 35.3760\n",
      "Epoch 35/40 - Train loss: 24.9041 - Val loss: 35.4049\n",
      "Epoch 36/40 - Train loss: 24.9044 - Val loss: 35.4336\n",
      "Epoch 37/40 - Train loss: 24.9045 - Val loss: 35.3792\n",
      "Epoch 38/40 - Train loss: 24.9042 - Val loss: 35.3774\n",
      "Epoch 39/40 - Train loss: 24.9047 - Val loss: 35.4143\n",
      "Epoch 40/40 - Train loss: 24.9046 - Val loss: 35.4047\n",
      "RMSE_train=4.9896 | RMSE_val=5.9495 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha = 0.0001 ---\n",
      "Epoch 1/40 - Train loss: 79.1924 - Val loss: 66.4862\n",
      "Epoch 2/40 - Train loss: 35.0553 - Val loss: 31.7469\n",
      "Epoch 3/40 - Train loss: 27.6057 - Val loss: 32.0675\n",
      "Epoch 4/40 - Train loss: 25.8078 - Val loss: 33.6463\n",
      "Epoch 5/40 - Train loss: 25.2388 - Val loss: 34.4712\n",
      "Epoch 6/40 - Train loss: 25.0357 - Val loss: 34.8922\n",
      "Epoch 7/40 - Train loss: 24.9595 - Val loss: 35.1096\n",
      "Epoch 8/40 - Train loss: 24.9318 - Val loss: 35.2531\n",
      "Epoch 9/40 - Train loss: 24.9205 - Val loss: 35.2889\n",
      "Epoch 10/40 - Train loss: 24.9148 - Val loss: 35.3297\n",
      "Epoch 11/40 - Train loss: 24.9132 - Val loss: 35.3657\n",
      "Epoch 12/40 - Train loss: 24.9126 - Val loss: 35.3689\n",
      "Epoch 13/40 - Train loss: 24.9130 - Val loss: 35.3683\n",
      "Epoch 14/40 - Train loss: 24.9127 - Val loss: 35.4260\n",
      "Epoch 15/40 - Train loss: 24.9123 - Val loss: 35.3626\n",
      "Epoch 16/40 - Train loss: 24.9122 - Val loss: 35.3845\n",
      "Epoch 17/40 - Train loss: 24.9127 - Val loss: 35.3856\n",
      "Epoch 18/40 - Train loss: 24.9139 - Val loss: 35.3680\n",
      "Epoch 19/40 - Train loss: 24.9124 - Val loss: 35.3701\n",
      "Epoch 20/40 - Train loss: 24.9121 - Val loss: 35.3715\n",
      "Epoch 21/40 - Train loss: 24.9123 - Val loss: 35.3822\n",
      "Epoch 22/40 - Train loss: 24.9127 - Val loss: 35.3528\n",
      "Epoch 23/40 - Train loss: 24.9130 - Val loss: 35.4203\n",
      "Epoch 24/40 - Train loss: 24.9121 - Val loss: 35.3898\n",
      "Epoch 25/40 - Train loss: 24.9121 - Val loss: 35.3646\n",
      "Epoch 26/40 - Train loss: 24.9129 - Val loss: 35.4061\n",
      "Epoch 27/40 - Train loss: 24.9125 - Val loss: 35.3804\n",
      "Epoch 28/40 - Train loss: 24.9132 - Val loss: 35.4120\n",
      "Epoch 29/40 - Train loss: 24.9122 - Val loss: 35.4189\n",
      "Epoch 30/40 - Train loss: 24.9128 - Val loss: 35.4115\n",
      "Epoch 31/40 - Train loss: 24.9130 - Val loss: 35.3535\n",
      "Epoch 32/40 - Train loss: 24.9121 - Val loss: 35.4230\n",
      "Epoch 33/40 - Train loss: 24.9122 - Val loss: 35.4231\n",
      "Epoch 34/40 - Train loss: 24.9122 - Val loss: 35.3878\n",
      "Epoch 35/40 - Train loss: 24.9130 - Val loss: 35.4042\n",
      "Epoch 36/40 - Train loss: 24.9135 - Val loss: 35.4128\n",
      "Epoch 37/40 - Train loss: 24.9125 - Val loss: 35.3726\n",
      "Epoch 38/40 - Train loss: 24.9125 - Val loss: 35.3702\n",
      "Epoch 39/40 - Train loss: 24.9125 - Val loss: 35.4075\n",
      "Epoch 40/40 - Train loss: 24.9122 - Val loss: 35.3896\n",
      "RMSE_train=4.9896 | RMSE_val=5.9475 | R2_train=0.8719 | R2_val=0.8251\n",
      "\n",
      "--- alpha = 0.0005 ---\n",
      "Epoch 1/40 - Train loss: 79.2530 - Val loss: 66.5796\n",
      "Epoch 2/40 - Train loss: 35.0971 - Val loss: 31.7939\n",
      "Epoch 3/40 - Train loss: 27.6672 - Val loss: 32.1092\n",
      "Epoch 4/40 - Train loss: 25.8711 - Val loss: 33.7194\n",
      "Epoch 5/40 - Train loss: 25.3014 - Val loss: 34.5875\n",
      "Epoch 6/40 - Train loss: 25.0992 - Val loss: 34.9498\n",
      "Epoch 7/40 - Train loss: 25.0248 - Val loss: 35.1693\n",
      "Epoch 8/40 - Train loss: 24.9954 - Val loss: 35.2079\n",
      "Epoch 9/40 - Train loss: 24.9847 - Val loss: 35.3401\n",
      "Epoch 10/40 - Train loss: 24.9807 - Val loss: 35.3846\n",
      "Epoch 11/40 - Train loss: 24.9800 - Val loss: 35.4248\n",
      "Epoch 12/40 - Train loss: 24.9777 - Val loss: 35.4338\n",
      "Epoch 13/40 - Train loss: 24.9787 - Val loss: 35.4625\n",
      "Epoch 14/40 - Train loss: 24.9777 - Val loss: 35.4655\n",
      "Epoch 15/40 - Train loss: 24.9778 - Val loss: 35.4640\n",
      "Epoch 16/40 - Train loss: 24.9772 - Val loss: 35.4541\n",
      "Epoch 17/40 - Train loss: 24.9785 - Val loss: 35.4360\n",
      "Epoch 18/40 - Train loss: 24.9780 - Val loss: 35.4483\n",
      "Epoch 19/40 - Train loss: 24.9775 - Val loss: 35.4718\n",
      "Epoch 20/40 - Train loss: 24.9778 - Val loss: 35.4409\n",
      "Epoch 21/40 - Train loss: 24.9772 - Val loss: 35.4425\n",
      "Epoch 22/40 - Train loss: 24.9775 - Val loss: 35.4324\n",
      "Epoch 23/40 - Train loss: 24.9773 - Val loss: 35.4203\n",
      "Epoch 24/40 - Train loss: 24.9774 - Val loss: 35.4870\n",
      "Epoch 25/40 - Train loss: 24.9774 - Val loss: 35.4599\n",
      "Epoch 26/40 - Train loss: 24.9778 - Val loss: 35.4482\n",
      "Epoch 27/40 - Train loss: 24.9775 - Val loss: 35.4508\n",
      "Epoch 28/40 - Train loss: 24.9780 - Val loss: 35.4404\n",
      "Epoch 29/40 - Train loss: 24.9774 - Val loss: 35.4476\n",
      "Epoch 30/40 - Train loss: 24.9773 - Val loss: 35.4656\n",
      "Epoch 31/40 - Train loss: 24.9789 - Val loss: 35.4226\n",
      "Epoch 32/40 - Train loss: 24.9786 - Val loss: 35.4292\n",
      "Epoch 33/40 - Train loss: 24.9773 - Val loss: 35.4404\n",
      "Epoch 34/40 - Train loss: 24.9777 - Val loss: 35.4299\n",
      "Epoch 35/40 - Train loss: 24.9774 - Val loss: 35.4292\n",
      "Epoch 36/40 - Train loss: 24.9775 - Val loss: 35.4420\n",
      "Epoch 37/40 - Train loss: 24.9777 - Val loss: 35.4596\n",
      "Epoch 38/40 - Train loss: 24.9777 - Val loss: 35.4384\n",
      "Epoch 39/40 - Train loss: 24.9777 - Val loss: 35.4106\n",
      "Epoch 40/40 - Train loss: 24.9774 - Val loss: 35.4045\n",
      "RMSE_train=4.9896 | RMSE_val=5.9433 | R2_train=0.8719 | R2_val=0.8253\n",
      "\n",
      "--- alpha = 0.001 ---\n",
      "Epoch 1/40 - Train loss: 79.3194 - Val loss: 66.5805\n",
      "Epoch 2/40 - Train loss: 35.1723 - Val loss: 31.8471\n",
      "Epoch 3/40 - Train loss: 27.7436 - Val loss: 32.1434\n",
      "Epoch 4/40 - Train loss: 25.9482 - Val loss: 33.7580\n",
      "Epoch 5/40 - Train loss: 25.3831 - Val loss: 34.6161\n",
      "Epoch 6/40 - Train loss: 25.1808 - Val loss: 35.0354\n",
      "Epoch 7/40 - Train loss: 25.1058 - Val loss: 35.2517\n",
      "Epoch 8/40 - Train loss: 25.0761 - Val loss: 35.3550\n",
      "Epoch 9/40 - Train loss: 25.0657 - Val loss: 35.3986\n",
      "Epoch 10/40 - Train loss: 25.0613 - Val loss: 35.3793\n",
      "Epoch 11/40 - Train loss: 25.0598 - Val loss: 35.4320\n",
      "Epoch 12/40 - Train loss: 25.0594 - Val loss: 35.4242\n",
      "Epoch 13/40 - Train loss: 25.0595 - Val loss: 35.4955\n",
      "Epoch 14/40 - Train loss: 25.0590 - Val loss: 35.5118\n",
      "Epoch 15/40 - Train loss: 25.0586 - Val loss: 35.4983\n",
      "Epoch 16/40 - Train loss: 25.0586 - Val loss: 35.5175\n",
      "Epoch 17/40 - Train loss: 25.0597 - Val loss: 35.4724\n",
      "Epoch 18/40 - Train loss: 25.0592 - Val loss: 35.5069\n",
      "Epoch 19/40 - Train loss: 25.0599 - Val loss: 35.4633\n",
      "Epoch 20/40 - Train loss: 25.0591 - Val loss: 35.4929\n",
      "Epoch 21/40 - Train loss: 25.0603 - Val loss: 35.4756\n",
      "Epoch 22/40 - Train loss: 25.0588 - Val loss: 35.5165\n",
      "Epoch 23/40 - Train loss: 25.0594 - Val loss: 35.5363\n",
      "Epoch 24/40 - Train loss: 25.0594 - Val loss: 35.5442\n",
      "Epoch 25/40 - Train loss: 25.0587 - Val loss: 35.5434\n",
      "Epoch 26/40 - Train loss: 25.0587 - Val loss: 35.5065\n",
      "Epoch 27/40 - Train loss: 25.0585 - Val loss: 35.5323\n",
      "Epoch 28/40 - Train loss: 25.0590 - Val loss: 35.5376\n",
      "Epoch 29/40 - Train loss: 25.0587 - Val loss: 35.5067\n",
      "Epoch 30/40 - Train loss: 25.0594 - Val loss: 35.5235\n",
      "Epoch 31/40 - Train loss: 25.0592 - Val loss: 35.5605\n",
      "Epoch 32/40 - Train loss: 25.0588 - Val loss: 35.5333\n",
      "Epoch 33/40 - Train loss: 25.0586 - Val loss: 35.4908\n",
      "Epoch 34/40 - Train loss: 25.0592 - Val loss: 35.5452\n",
      "Epoch 35/40 - Train loss: 25.0593 - Val loss: 35.5309\n",
      "Epoch 36/40 - Train loss: 25.0593 - Val loss: 35.4887\n",
      "Epoch 37/40 - Train loss: 25.0591 - Val loss: 35.4853\n",
      "Epoch 38/40 - Train loss: 25.0589 - Val loss: 35.5129\n",
      "Epoch 39/40 - Train loss: 25.0588 - Val loss: 35.4964\n",
      "Epoch 40/40 - Train loss: 25.0588 - Val loss: 35.4788\n",
      "RMSE_train=4.9896 | RMSE_val=5.9427 | R2_train=0.8719 | R2_val=0.8254\n",
      "\n",
      "\n",
      "=== Resumen Ridge ordenado por RMSE_val ===\n",
      "alpha=0.001    rmse_train=4.9896 rmse_val=5.9427 r2_train=0.8719 r2_val=0.8254\n",
      "alpha=0.0005   rmse_train=4.9896 rmse_val=5.9433 r2_train=0.8719 r2_val=0.8253\n",
      "alpha=0.0001   rmse_train=4.9896 rmse_val=5.9475 r2_train=0.8719 r2_val=0.8251\n",
      "alpha=5e-05    rmse_train=4.9896 rmse_val=5.9495 r2_train=0.8719 r2_val=0.8250\n",
      "alpha=0.0      rmse_train=4.9896 rmse_val=5.9503 r2_train=0.8719 r2_val=0.8249\n",
      "alpha=1e-05    rmse_train=4.9898 rmse_val=5.9513 r2_train=0.8719 r2_val=0.8249\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Pequeña grilla de Ridge: distintos alpha\n",
    "# ============================================================\n",
    "\n",
    "ridge_alphas = [0.0, 1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\n",
    "ridge_results = []\n",
    "\n",
    "print(\"Tuning Ridge (L2) sobre distintos alpha:\\n\")\n",
    "\n",
    "for a in ridge_alphas:\n",
    "    print(f\"--- alpha = {a} ---\")\n",
    "    ridge_model = LinearRegressorScratch(\n",
    "        learning_rate=1e-3,\n",
    "        max_iter=40,\n",
    "        batch_size=2048,\n",
    "        penalty=\"l2\" if a > 0 else None,\n",
    "        alpha=a,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    ridge_model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_np,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val_np\n",
    "    )\n",
    "\n",
    "    y_train_pred = ridge_model.predict(X_train_scaled)\n",
    "    y_val_pred   = ridge_model.predict(X_val_scaled)\n",
    "\n",
    "    rmse_tr = rmse(y_train_np, y_train_pred)\n",
    "    rmse_v  = rmse(y_val_np, y_val_pred)\n",
    "    r2_tr   = r2(y_train_np, y_train_pred)\n",
    "    r2_v    = r2(y_val_np, y_val_pred)\n",
    "\n",
    "    ridge_results.append({\n",
    "        \"penalty\": \"l2\" if a > 0 else \"none\",\n",
    "        \"alpha\": a,\n",
    "        \"rmse_train\": rmse_tr,\n",
    "        \"rmse_val\": rmse_v,\n",
    "        \"r2_train\": r2_tr,\n",
    "        \"r2_val\": r2_v,\n",
    "        \"model\": ridge_model,\n",
    "    })\n",
    "\n",
    "    print(f\"RMSE_train={rmse_tr:.4f} | RMSE_val={rmse_v:.4f} | \"\n",
    "          f\"R2_train={r2_tr:.4f} | R2_val={r2_v:.4f}\\n\")\n",
    "\n",
    "# Ordenar resultados por RMSE_val\n",
    "ridge_results_sorted = sorted(ridge_results, key=lambda d: d[\"rmse_val\"])\n",
    "\n",
    "print(\"\\n=== Resumen Ridge ordenado por RMSE_val ===\")\n",
    "for r in ridge_results_sorted:\n",
    "    print(\n",
    "        f\"alpha={r['alpha']:<8} \"\n",
    "        f\"rmse_train={r['rmse_train']:.4f} \"\n",
    "        f\"rmse_val={r['rmse_val']:.4f} \"\n",
    "        f\"r2_train={r['r2_train']:.4f} \"\n",
    "        f\"r2_val={r['r2_val']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e72bfe-7471-4b0c-a53f-984581414c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Lasso (L1):\n",
      "\n",
      "--- alpha = 1e-05 ---\n",
      "Epoch 1/40 - Train loss: 79.2056 - Val loss: 66.4591\n",
      "Epoch 2/40 - Train loss: 35.0244 - Val loss: 31.7083\n",
      "Epoch 3/40 - Train loss: 27.5922 - Val loss: 32.0437\n",
      "Epoch 4/40 - Train loss: 25.7903 - Val loss: 33.6406\n",
      "Epoch 5/40 - Train loss: 25.2244 - Val loss: 34.4703\n",
      "Epoch 6/40 - Train loss: 25.0176 - Val loss: 34.9160\n",
      "Epoch 7/40 - Train loss: 24.9434 - Val loss: 35.0920\n",
      "Epoch 8/40 - Train loss: 24.9141 - Val loss: 35.2043\n",
      "Epoch 9/40 - Train loss: 24.9037 - Val loss: 35.2647\n",
      "Epoch 10/40 - Train loss: 24.8990 - Val loss: 35.2959\n",
      "Epoch 11/40 - Train loss: 24.8971 - Val loss: 35.3546\n",
      "Epoch 12/40 - Train loss: 24.8966 - Val loss: 35.3856\n",
      "Epoch 13/40 - Train loss: 24.8970 - Val loss: 35.3575\n",
      "Epoch 14/40 - Train loss: 24.8970 - Val loss: 35.3738\n",
      "Epoch 15/40 - Train loss: 24.8978 - Val loss: 35.3603\n",
      "Epoch 16/40 - Train loss: 24.8961 - Val loss: 35.3616\n",
      "Epoch 17/40 - Train loss: 24.8962 - Val loss: 35.3871\n",
      "Epoch 18/40 - Train loss: 24.8969 - Val loss: 35.4126\n",
      "Epoch 19/40 - Train loss: 24.8960 - Val loss: 35.3963\n",
      "Epoch 20/40 - Train loss: 24.8965 - Val loss: 35.3794\n",
      "Epoch 21/40 - Train loss: 24.8966 - Val loss: 35.4026\n",
      "Epoch 22/40 - Train loss: 24.8966 - Val loss: 35.3734\n",
      "Epoch 23/40 - Train loss: 24.8975 - Val loss: 35.3898\n",
      "Epoch 24/40 - Train loss: 24.8976 - Val loss: 35.3662\n",
      "Epoch 25/40 - Train loss: 24.8967 - Val loss: 35.3725\n",
      "Epoch 26/40 - Train loss: 24.8959 - Val loss: 35.3688\n",
      "Epoch 27/40 - Train loss: 24.8965 - Val loss: 35.3902\n",
      "Epoch 28/40 - Train loss: 24.8963 - Val loss: 35.3628\n",
      "Epoch 29/40 - Train loss: 24.8966 - Val loss: 35.3589\n",
      "Epoch 30/40 - Train loss: 24.8962 - Val loss: 35.3935\n",
      "Epoch 31/40 - Train loss: 24.8969 - Val loss: 35.3865\n",
      "Epoch 32/40 - Train loss: 24.8973 - Val loss: 35.3861\n",
      "Epoch 33/40 - Train loss: 24.8961 - Val loss: 35.3749\n",
      "Epoch 34/40 - Train loss: 24.8960 - Val loss: 35.3759\n",
      "Epoch 35/40 - Train loss: 24.8961 - Val loss: 35.3477\n",
      "Epoch 36/40 - Train loss: 24.8972 - Val loss: 35.3617\n",
      "Epoch 37/40 - Train loss: 24.8965 - Val loss: 35.3354\n",
      "Epoch 38/40 - Train loss: 24.8975 - Val loss: 35.3864\n",
      "Epoch 39/40 - Train loss: 24.8961 - Val loss: 35.3474\n",
      "Epoch 40/40 - Train loss: 24.8962 - Val loss: 35.3164\n",
      "RMSE_train=4.9896 | RMSE_val=5.9427 | R2_train=0.8719 | R2_val=0.8254\n",
      "\n",
      "--- alpha = 5e-05 ---\n",
      "Epoch 1/40 - Train loss: 79.2221 - Val loss: 66.5234\n",
      "Epoch 2/40 - Train loss: 35.0473 - Val loss: 31.7304\n",
      "Epoch 3/40 - Train loss: 27.5980 - Val loss: 32.0273\n",
      "Epoch 4/40 - Train loss: 25.7906 - Val loss: 33.6024\n",
      "Epoch 5/40 - Train loss: 25.2242 - Val loss: 34.4512\n",
      "Epoch 6/40 - Train loss: 25.0214 - Val loss: 34.8756\n",
      "Epoch 7/40 - Train loss: 24.9469 - Val loss: 35.0780\n",
      "Epoch 8/40 - Train loss: 24.9163 - Val loss: 35.1854\n",
      "Epoch 9/40 - Train loss: 24.9039 - Val loss: 35.2605\n",
      "Epoch 10/40 - Train loss: 24.8993 - Val loss: 35.3214\n",
      "Epoch 11/40 - Train loss: 24.8982 - Val loss: 35.3357\n",
      "Epoch 12/40 - Train loss: 24.8976 - Val loss: 35.3447\n",
      "Epoch 13/40 - Train loss: 24.8975 - Val loss: 35.3404\n",
      "Epoch 14/40 - Train loss: 24.8976 - Val loss: 35.3630\n",
      "Epoch 15/40 - Train loss: 24.8970 - Val loss: 35.3485\n",
      "Epoch 16/40 - Train loss: 24.8968 - Val loss: 35.3620\n",
      "Epoch 17/40 - Train loss: 24.8974 - Val loss: 35.3280\n",
      "Epoch 18/40 - Train loss: 24.8976 - Val loss: 35.3383\n",
      "Epoch 19/40 - Train loss: 24.8967 - Val loss: 35.3846\n",
      "Epoch 20/40 - Train loss: 24.8970 - Val loss: 35.3608\n",
      "Epoch 21/40 - Train loss: 24.8968 - Val loss: 35.3896\n",
      "Epoch 22/40 - Train loss: 24.8967 - Val loss: 35.4040\n",
      "Epoch 23/40 - Train loss: 24.8970 - Val loss: 35.4025\n",
      "Epoch 24/40 - Train loss: 24.8970 - Val loss: 35.4220\n",
      "Epoch 25/40 - Train loss: 24.8970 - Val loss: 35.3978\n",
      "Epoch 26/40 - Train loss: 24.8977 - Val loss: 35.3815\n",
      "Epoch 27/40 - Train loss: 24.8976 - Val loss: 35.4147\n",
      "Epoch 28/40 - Train loss: 24.8972 - Val loss: 35.3625\n",
      "Epoch 29/40 - Train loss: 24.8985 - Val loss: 35.3962\n",
      "Epoch 30/40 - Train loss: 24.8969 - Val loss: 35.3652\n",
      "Epoch 31/40 - Train loss: 24.8972 - Val loss: 35.3797\n",
      "Epoch 32/40 - Train loss: 24.8979 - Val loss: 35.4279\n",
      "Epoch 33/40 - Train loss: 24.8979 - Val loss: 35.3843\n",
      "Epoch 34/40 - Train loss: 24.8967 - Val loss: 35.3939\n",
      "Epoch 35/40 - Train loss: 24.8971 - Val loss: 35.3895\n",
      "Epoch 36/40 - Train loss: 24.8976 - Val loss: 35.3282\n",
      "Epoch 37/40 - Train loss: 24.8979 - Val loss: 35.3404\n",
      "Epoch 38/40 - Train loss: 24.8967 - Val loss: 35.3472\n",
      "Epoch 39/40 - Train loss: 24.8973 - Val loss: 35.3523\n",
      "Epoch 40/40 - Train loss: 24.8974 - Val loss: 35.3631\n",
      "RMSE_train=4.9896 | RMSE_val=5.9466 | R2_train=0.8719 | R2_val=0.8251\n",
      "\n",
      "--- alpha = 0.0001 ---\n",
      "Epoch 1/40 - Train loss: 79.2211 - Val loss: 66.4929\n",
      "Epoch 2/40 - Train loss: 35.0329 - Val loss: 31.7091\n",
      "Epoch 3/40 - Train loss: 27.5930 - Val loss: 32.0605\n",
      "Epoch 4/40 - Train loss: 25.7928 - Val loss: 33.6666\n",
      "Epoch 5/40 - Train loss: 25.2257 - Val loss: 34.4982\n",
      "Epoch 6/40 - Train loss: 25.0210 - Val loss: 34.8929\n",
      "Epoch 7/40 - Train loss: 24.9457 - Val loss: 35.0671\n",
      "Epoch 8/40 - Train loss: 24.9170 - Val loss: 35.1790\n",
      "Epoch 9/40 - Train loss: 24.9073 - Val loss: 35.2546\n",
      "Epoch 10/40 - Train loss: 24.9012 - Val loss: 35.2978\n",
      "Epoch 11/40 - Train loss: 24.8986 - Val loss: 35.3099\n",
      "Epoch 12/40 - Train loss: 24.8996 - Val loss: 35.3691\n",
      "Epoch 13/40 - Train loss: 24.8978 - Val loss: 35.3164\n",
      "Epoch 14/40 - Train loss: 24.8979 - Val loss: 35.3568\n",
      "Epoch 15/40 - Train loss: 24.8977 - Val loss: 35.3765\n",
      "Epoch 16/40 - Train loss: 24.8977 - Val loss: 35.3975\n",
      "Epoch 17/40 - Train loss: 24.8978 - Val loss: 35.3745\n",
      "Epoch 18/40 - Train loss: 24.8982 - Val loss: 35.3696\n",
      "Epoch 19/40 - Train loss: 24.8982 - Val loss: 35.4135\n",
      "Epoch 20/40 - Train loss: 24.8976 - Val loss: 35.3416\n",
      "Epoch 21/40 - Train loss: 24.8978 - Val loss: 35.3768\n",
      "Epoch 22/40 - Train loss: 24.8979 - Val loss: 35.4009\n",
      "Epoch 23/40 - Train loss: 24.8977 - Val loss: 35.3676\n",
      "Epoch 24/40 - Train loss: 24.8983 - Val loss: 35.4187\n",
      "Epoch 25/40 - Train loss: 24.8982 - Val loss: 35.3544\n",
      "Epoch 26/40 - Train loss: 24.8979 - Val loss: 35.3685\n",
      "Epoch 27/40 - Train loss: 24.8981 - Val loss: 35.3581\n",
      "Epoch 28/40 - Train loss: 24.8982 - Val loss: 35.3452\n",
      "Epoch 29/40 - Train loss: 24.8981 - Val loss: 35.3819\n",
      "Epoch 30/40 - Train loss: 24.8976 - Val loss: 35.3768\n",
      "Epoch 31/40 - Train loss: 24.8977 - Val loss: 35.3823\n",
      "Epoch 32/40 - Train loss: 24.8981 - Val loss: 35.3916\n",
      "Epoch 33/40 - Train loss: 24.8981 - Val loss: 35.3485\n",
      "Epoch 34/40 - Train loss: 24.8978 - Val loss: 35.3569\n",
      "Epoch 35/40 - Train loss: 24.8981 - Val loss: 35.4020\n",
      "Epoch 36/40 - Train loss: 24.8978 - Val loss: 35.3724\n",
      "Epoch 37/40 - Train loss: 24.8984 - Val loss: 35.3519\n",
      "Epoch 38/40 - Train loss: 24.8988 - Val loss: 35.3987\n",
      "Epoch 39/40 - Train loss: 24.8993 - Val loss: 35.4019\n",
      "Epoch 40/40 - Train loss: 24.8980 - Val loss: 35.3962\n",
      "RMSE_train=4.9896 | RMSE_val=5.9493 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha = 0.0005 ---\n",
      "Epoch 1/40 - Train loss: 79.1546 - Val loss: 66.4724\n",
      "Epoch 2/40 - Train loss: 35.0427 - Val loss: 31.7415\n",
      "Epoch 3/40 - Train loss: 27.6003 - Val loss: 32.0581\n",
      "Epoch 4/40 - Train loss: 25.8021 - Val loss: 33.6743\n",
      "Epoch 5/40 - Train loss: 25.2342 - Val loss: 34.4911\n",
      "Epoch 6/40 - Train loss: 25.0290 - Val loss: 34.9426\n",
      "Epoch 7/40 - Train loss: 24.9522 - Val loss: 35.1307\n",
      "Epoch 8/40 - Train loss: 24.9232 - Val loss: 35.2116\n",
      "Epoch 9/40 - Train loss: 24.9119 - Val loss: 35.3059\n",
      "Epoch 10/40 - Train loss: 24.9076 - Val loss: 35.3280\n",
      "Epoch 11/40 - Train loss: 24.9062 - Val loss: 35.3656\n",
      "Epoch 12/40 - Train loss: 24.9054 - Val loss: 35.3852\n",
      "Epoch 13/40 - Train loss: 24.9062 - Val loss: 35.3965\n",
      "Epoch 14/40 - Train loss: 24.9051 - Val loss: 35.4111\n",
      "Epoch 15/40 - Train loss: 24.9051 - Val loss: 35.3997\n",
      "Epoch 16/40 - Train loss: 24.9073 - Val loss: 35.3415\n",
      "Epoch 17/40 - Train loss: 24.9065 - Val loss: 35.3937\n",
      "Epoch 18/40 - Train loss: 24.9060 - Val loss: 35.3716\n",
      "Epoch 19/40 - Train loss: 24.9052 - Val loss: 35.3992\n",
      "Epoch 20/40 - Train loss: 24.9052 - Val loss: 35.3426\n",
      "Epoch 21/40 - Train loss: 24.9050 - Val loss: 35.3726\n",
      "Epoch 22/40 - Train loss: 24.9050 - Val loss: 35.4204\n",
      "Epoch 23/40 - Train loss: 24.9053 - Val loss: 35.3812\n",
      "Epoch 24/40 - Train loss: 24.9052 - Val loss: 35.3675\n",
      "Epoch 25/40 - Train loss: 24.9053 - Val loss: 35.3805\n",
      "Epoch 26/40 - Train loss: 24.9051 - Val loss: 35.3524\n",
      "Epoch 27/40 - Train loss: 24.9063 - Val loss: 35.3615\n",
      "Epoch 28/40 - Train loss: 24.9054 - Val loss: 35.3797\n",
      "Epoch 29/40 - Train loss: 24.9053 - Val loss: 35.3981\n",
      "Epoch 30/40 - Train loss: 24.9054 - Val loss: 35.4149\n",
      "Epoch 31/40 - Train loss: 24.9051 - Val loss: 35.3519\n",
      "Epoch 32/40 - Train loss: 24.9054 - Val loss: 35.3714\n",
      "Epoch 33/40 - Train loss: 24.9050 - Val loss: 35.3659\n",
      "Epoch 34/40 - Train loss: 24.9054 - Val loss: 35.4030\n",
      "Epoch 35/40 - Train loss: 24.9061 - Val loss: 35.3944\n",
      "Epoch 36/40 - Train loss: 24.9053 - Val loss: 35.3655\n",
      "Epoch 37/40 - Train loss: 24.9058 - Val loss: 35.3505\n",
      "Epoch 38/40 - Train loss: 24.9054 - Val loss: 35.3861\n",
      "Epoch 39/40 - Train loss: 24.9053 - Val loss: 35.3981\n",
      "Epoch 40/40 - Train loss: 24.9051 - Val loss: 35.3752\n",
      "RMSE_train=4.9896 | RMSE_val=5.9469 | R2_train=0.8719 | R2_val=0.8251\n",
      "\n",
      "--- alpha = 0.001 ---\n",
      "Epoch 1/40 - Train loss: 79.2020 - Val loss: 66.4493\n",
      "Epoch 2/40 - Train loss: 35.0615 - Val loss: 31.7448\n",
      "Epoch 3/40 - Train loss: 27.6155 - Val loss: 32.0630\n",
      "Epoch 4/40 - Train loss: 25.8117 - Val loss: 33.6581\n",
      "Epoch 5/40 - Train loss: 25.2416 - Val loss: 34.5331\n",
      "Epoch 6/40 - Train loss: 25.0373 - Val loss: 34.9087\n",
      "Epoch 7/40 - Train loss: 24.9620 - Val loss: 35.0999\n",
      "Epoch 8/40 - Train loss: 24.9325 - Val loss: 35.2214\n",
      "Epoch 9/40 - Train loss: 24.9208 - Val loss: 35.2702\n",
      "Epoch 10/40 - Train loss: 24.9187 - Val loss: 35.3597\n",
      "Epoch 11/40 - Train loss: 24.9155 - Val loss: 35.3276\n",
      "Epoch 12/40 - Train loss: 24.9149 - Val loss: 35.3450\n",
      "Epoch 13/40 - Train loss: 24.9145 - Val loss: 35.4095\n",
      "Epoch 14/40 - Train loss: 24.9143 - Val loss: 35.3572\n",
      "Epoch 15/40 - Train loss: 24.9160 - Val loss: 35.3888\n",
      "Epoch 16/40 - Train loss: 24.9186 - Val loss: 35.4043\n",
      "Epoch 17/40 - Train loss: 24.9157 - Val loss: 35.3577\n",
      "Epoch 18/40 - Train loss: 24.9149 - Val loss: 35.3924\n",
      "Epoch 19/40 - Train loss: 24.9143 - Val loss: 35.3859\n",
      "Epoch 20/40 - Train loss: 24.9141 - Val loss: 35.3814\n",
      "Epoch 21/40 - Train loss: 24.9145 - Val loss: 35.3946\n",
      "Epoch 22/40 - Train loss: 24.9146 - Val loss: 35.4102\n",
      "Epoch 23/40 - Train loss: 24.9149 - Val loss: 35.3986\n",
      "Epoch 24/40 - Train loss: 24.9157 - Val loss: 35.4135\n",
      "Epoch 25/40 - Train loss: 24.9146 - Val loss: 35.4060\n",
      "Epoch 26/40 - Train loss: 24.9145 - Val loss: 35.4075\n",
      "Epoch 27/40 - Train loss: 24.9141 - Val loss: 35.3511\n",
      "Epoch 28/40 - Train loss: 24.9141 - Val loss: 35.3998\n",
      "Epoch 29/40 - Train loss: 24.9148 - Val loss: 35.3901\n",
      "Epoch 30/40 - Train loss: 24.9142 - Val loss: 35.3883\n",
      "Epoch 31/40 - Train loss: 24.9145 - Val loss: 35.3835\n",
      "Epoch 32/40 - Train loss: 24.9146 - Val loss: 35.4018\n",
      "Epoch 33/40 - Train loss: 24.9142 - Val loss: 35.3974\n",
      "Epoch 34/40 - Train loss: 24.9142 - Val loss: 35.3724\n",
      "Epoch 35/40 - Train loss: 24.9154 - Val loss: 35.4121\n",
      "Epoch 36/40 - Train loss: 24.9145 - Val loss: 35.3788\n",
      "Epoch 37/40 - Train loss: 24.9143 - Val loss: 35.3948\n",
      "Epoch 38/40 - Train loss: 24.9145 - Val loss: 35.3634\n",
      "Epoch 39/40 - Train loss: 24.9148 - Val loss: 35.3527\n",
      "Epoch 40/40 - Train loss: 24.9154 - Val loss: 35.4248\n",
      "RMSE_train=4.9897 | RMSE_val=5.9503 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "\n",
      "=== Resumen Lasso ordenado por RMSE_val ===\n",
      "alpha=1e-05    rmse_train=4.9896 rmse_val=5.9427 r2_train=0.8719 r2_val=0.8254\n",
      "alpha=5e-05    rmse_train=4.9896 rmse_val=5.9466 r2_train=0.8719 r2_val=0.8251\n",
      "alpha=0.0005   rmse_train=4.9896 rmse_val=5.9469 r2_train=0.8719 r2_val=0.8251\n",
      "alpha=0.0001   rmse_train=4.9896 rmse_val=5.9493 r2_train=0.8719 r2_val=0.8250\n",
      "alpha=0.001    rmse_train=4.9897 rmse_val=5.9503 r2_train=0.8719 r2_val=0.8249\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. Lasso (from-scratch) - tuning de alpha L1\n",
    "# ============================================================\n",
    "\n",
    "lasso_alphas = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]\n",
    "lasso_results = []\n",
    "\n",
    "print(\"Tuning Lasso (L1):\\n\")\n",
    "\n",
    "for a in lasso_alphas:\n",
    "    print(f\"--- alpha = {a} ---\")\n",
    "    lasso_model = LinearRegressorScratch(\n",
    "        learning_rate=1e-3,\n",
    "        max_iter=40,\n",
    "        batch_size=2048,\n",
    "        penalty=\"l1\",\n",
    "        alpha=a,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    lasso_model.fit(\n",
    "        X_train_scaled,\n",
    "        y_train_np,\n",
    "        X_val=X_val_scaled,\n",
    "        y_val=y_val_np\n",
    "    )\n",
    "\n",
    "    y_train_pred = lasso_model.predict(X_train_scaled)\n",
    "    y_val_pred   = lasso_model.predict(X_val_scaled)\n",
    "\n",
    "    rmse_tr = rmse(y_train_np, y_train_pred)\n",
    "    rmse_v  = rmse(y_val_np, y_val_pred)\n",
    "    r2_tr   = r2(y_train_np, y_train_pred)\n",
    "    r2_v    = r2(y_val_np, y_val_pred)\n",
    "\n",
    "    lasso_results.append({\n",
    "        \"alpha\": a,\n",
    "        \"rmse_train\": rmse_tr,\n",
    "        \"rmse_val\": rmse_v,\n",
    "        \"r2_train\": r2_tr,\n",
    "        \"r2_val\": r2_v,\n",
    "        \"model\": lasso_model,\n",
    "    })\n",
    "\n",
    "    print(f\"RMSE_train={rmse_tr:.4f} | RMSE_val={rmse_v:.4f} | \"\n",
    "          f\"R2_train={r2_tr:.4f} | R2_val={r2_v:.4f}\\n\")\n",
    "\n",
    "# Ordenar resultados por RMSE de validación\n",
    "lasso_results_sorted = sorted(lasso_results, key=lambda d: d[\"rmse_val\"])\n",
    "\n",
    "print(\"\\n=== Resumen Lasso ordenado por RMSE_val ===\")\n",
    "for r in lasso_results_sorted:\n",
    "    print(\n",
    "        f\"alpha={r['alpha']:<8} \"\n",
    "        f\"rmse_train={r['rmse_train']:.4f} \"\n",
    "        f\"rmse_val={r['rmse_val']:.4f} \"\n",
    "        f\"r2_train={r['r2_train']:.4f} \"\n",
    "        f\"r2_val={r['r2_val']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f2d7db-bd74-46b4-a3c6-5695d5fedd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning ElasticNet (L1 + L2):\n",
      "\n",
      "--- alpha=1e-05, l1_ratio=0.1 ---\n",
      "Epoch 1/40 - Train loss: 79.1265 - Val loss: 66.3992\n",
      "Epoch 2/40 - Train loss: 35.0409 - Val loss: 31.7331\n",
      "Epoch 3/40 - Train loss: 27.5979 - Val loss: 32.0724\n",
      "Epoch 4/40 - Train loss: 25.7930 - Val loss: 33.6545\n",
      "Epoch 5/40 - Train loss: 25.2238 - Val loss: 34.5183\n",
      "Epoch 6/40 - Train loss: 25.0211 - Val loss: 34.9019\n",
      "Epoch 7/40 - Train loss: 24.9444 - Val loss: 35.1096\n",
      "Epoch 8/40 - Train loss: 24.9160 - Val loss: 35.2297\n",
      "Epoch 9/40 - Train loss: 24.9052 - Val loss: 35.3109\n",
      "Epoch 10/40 - Train loss: 24.9000 - Val loss: 35.3539\n",
      "Epoch 11/40 - Train loss: 24.8984 - Val loss: 35.3688\n",
      "Epoch 12/40 - Train loss: 24.8980 - Val loss: 35.3537\n",
      "Epoch 13/40 - Train loss: 24.8974 - Val loss: 35.3287\n",
      "Epoch 14/40 - Train loss: 24.8973 - Val loss: 35.3535\n",
      "Epoch 15/40 - Train loss: 24.8975 - Val loss: 35.3401\n",
      "Epoch 16/40 - Train loss: 24.8974 - Val loss: 35.3918\n",
      "Epoch 17/40 - Train loss: 24.8978 - Val loss: 35.3915\n",
      "Epoch 18/40 - Train loss: 24.8974 - Val loss: 35.3340\n",
      "Epoch 19/40 - Train loss: 24.8975 - Val loss: 35.3526\n",
      "Epoch 20/40 - Train loss: 24.8975 - Val loss: 35.3896\n",
      "Epoch 21/40 - Train loss: 24.8976 - Val loss: 35.3869\n",
      "Epoch 22/40 - Train loss: 24.8975 - Val loss: 35.3691\n",
      "Epoch 23/40 - Train loss: 24.8977 - Val loss: 35.3893\n",
      "Epoch 24/40 - Train loss: 24.8984 - Val loss: 35.3751\n",
      "Epoch 25/40 - Train loss: 24.8973 - Val loss: 35.3881\n",
      "Epoch 26/40 - Train loss: 24.8983 - Val loss: 35.3587\n",
      "Epoch 27/40 - Train loss: 24.8977 - Val loss: 35.3757\n",
      "Epoch 28/40 - Train loss: 24.8976 - Val loss: 35.3733\n",
      "Epoch 29/40 - Train loss: 24.8973 - Val loss: 35.3880\n",
      "Epoch 30/40 - Train loss: 24.8976 - Val loss: 35.3764\n",
      "Epoch 31/40 - Train loss: 24.8977 - Val loss: 35.3624\n",
      "Epoch 32/40 - Train loss: 24.8974 - Val loss: 35.3532\n",
      "Epoch 33/40 - Train loss: 24.8973 - Val loss: 35.3493\n",
      "Epoch 34/40 - Train loss: 24.8975 - Val loss: 35.4017\n",
      "Epoch 35/40 - Train loss: 24.8972 - Val loss: 35.3688\n",
      "Epoch 36/40 - Train loss: 24.8975 - Val loss: 35.3784\n",
      "Epoch 37/40 - Train loss: 24.8975 - Val loss: 35.3915\n",
      "Epoch 38/40 - Train loss: 24.8975 - Val loss: 35.3593\n",
      "Epoch 39/40 - Train loss: 24.8987 - Val loss: 35.3280\n",
      "Epoch 40/40 - Train loss: 24.8974 - Val loss: 35.3507\n",
      "RMSE_train=4.9896 | RMSE_val=5.9455 | R2_train=0.8719 | R2_val=0.8252\n",
      "\n",
      "--- alpha=1e-05, l1_ratio=0.5 ---\n",
      "Epoch 1/40 - Train loss: 79.1798 - Val loss: 66.3632\n",
      "Epoch 2/40 - Train loss: 35.0324 - Val loss: 31.7160\n",
      "Epoch 3/40 - Train loss: 27.5987 - Val loss: 32.0417\n",
      "Epoch 4/40 - Train loss: 25.7922 - Val loss: 33.6528\n",
      "Epoch 5/40 - Train loss: 25.2231 - Val loss: 34.5075\n",
      "Epoch 6/40 - Train loss: 25.0201 - Val loss: 34.9003\n",
      "Epoch 7/40 - Train loss: 24.9436 - Val loss: 35.0879\n",
      "Epoch 8/40 - Train loss: 24.9146 - Val loss: 35.1834\n",
      "Epoch 9/40 - Train loss: 24.9039 - Val loss: 35.2846\n",
      "Epoch 10/40 - Train loss: 24.8997 - Val loss: 35.3012\n",
      "Epoch 11/40 - Train loss: 24.8980 - Val loss: 35.3490\n",
      "Epoch 12/40 - Train loss: 24.8970 - Val loss: 35.3713\n",
      "Epoch 13/40 - Train loss: 24.8969 - Val loss: 35.3937\n",
      "Epoch 14/40 - Train loss: 24.8972 - Val loss: 35.3865\n",
      "Epoch 15/40 - Train loss: 24.8972 - Val loss: 35.3837\n",
      "Epoch 16/40 - Train loss: 24.8967 - Val loss: 35.3797\n",
      "Epoch 17/40 - Train loss: 24.8968 - Val loss: 35.3734\n",
      "Epoch 18/40 - Train loss: 24.8978 - Val loss: 35.3793\n",
      "Epoch 19/40 - Train loss: 24.8973 - Val loss: 35.3779\n",
      "Epoch 20/40 - Train loss: 24.8967 - Val loss: 35.3790\n",
      "Epoch 21/40 - Train loss: 24.8970 - Val loss: 35.4043\n",
      "Epoch 22/40 - Train loss: 24.8972 - Val loss: 35.3733\n",
      "Epoch 23/40 - Train loss: 24.8968 - Val loss: 35.4108\n",
      "Epoch 24/40 - Train loss: 24.8967 - Val loss: 35.4023\n",
      "Epoch 25/40 - Train loss: 24.8970 - Val loss: 35.3990\n",
      "Epoch 26/40 - Train loss: 24.8972 - Val loss: 35.3979\n",
      "Epoch 27/40 - Train loss: 24.8970 - Val loss: 35.3745\n",
      "Epoch 28/40 - Train loss: 24.8972 - Val loss: 35.3685\n",
      "Epoch 29/40 - Train loss: 24.8970 - Val loss: 35.3476\n",
      "Epoch 30/40 - Train loss: 24.8974 - Val loss: 35.3568\n",
      "Epoch 31/40 - Train loss: 24.8968 - Val loss: 35.3941\n",
      "Epoch 32/40 - Train loss: 24.8967 - Val loss: 35.3625\n",
      "Epoch 33/40 - Train loss: 24.8971 - Val loss: 35.3749\n",
      "Epoch 34/40 - Train loss: 24.8972 - Val loss: 35.4020\n",
      "Epoch 35/40 - Train loss: 24.8974 - Val loss: 35.3577\n",
      "Epoch 36/40 - Train loss: 24.8968 - Val loss: 35.3803\n",
      "Epoch 37/40 - Train loss: 24.8973 - Val loss: 35.3798\n",
      "Epoch 38/40 - Train loss: 24.8972 - Val loss: 35.3571\n",
      "Epoch 39/40 - Train loss: 24.8978 - Val loss: 35.3552\n",
      "Epoch 40/40 - Train loss: 24.8970 - Val loss: 35.3920\n",
      "RMSE_train=4.9896 | RMSE_val=5.9490 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha=1e-05, l1_ratio=0.9 ---\n",
      "Epoch 1/40 - Train loss: 79.2461 - Val loss: 66.5430\n",
      "Epoch 2/40 - Train loss: 35.0501 - Val loss: 31.7285\n",
      "Epoch 3/40 - Train loss: 27.5937 - Val loss: 32.0718\n",
      "Epoch 4/40 - Train loss: 25.7902 - Val loss: 33.6121\n",
      "Epoch 5/40 - Train loss: 25.2210 - Val loss: 34.4852\n",
      "Epoch 6/40 - Train loss: 25.0199 - Val loss: 34.9606\n",
      "Epoch 7/40 - Train loss: 24.9443 - Val loss: 35.1173\n",
      "Epoch 8/40 - Train loss: 24.9151 - Val loss: 35.2587\n",
      "Epoch 9/40 - Train loss: 24.9039 - Val loss: 35.2859\n",
      "Epoch 10/40 - Train loss: 24.8991 - Val loss: 35.3732\n",
      "Epoch 11/40 - Train loss: 24.8974 - Val loss: 35.3707\n",
      "Epoch 12/40 - Train loss: 24.8976 - Val loss: 35.3670\n",
      "Epoch 13/40 - Train loss: 24.8968 - Val loss: 35.3931\n",
      "Epoch 14/40 - Train loss: 24.8970 - Val loss: 35.4034\n",
      "Epoch 15/40 - Train loss: 24.8969 - Val loss: 35.3954\n",
      "Epoch 16/40 - Train loss: 24.8962 - Val loss: 35.3979\n",
      "Epoch 17/40 - Train loss: 24.8963 - Val loss: 35.3517\n",
      "Epoch 18/40 - Train loss: 24.8963 - Val loss: 35.3578\n",
      "Epoch 19/40 - Train loss: 24.8963 - Val loss: 35.3846\n",
      "Epoch 20/40 - Train loss: 24.8967 - Val loss: 35.3816\n",
      "Epoch 21/40 - Train loss: 24.8962 - Val loss: 35.3509\n",
      "Epoch 22/40 - Train loss: 24.8964 - Val loss: 35.3635\n",
      "Epoch 23/40 - Train loss: 24.8962 - Val loss: 35.3816\n",
      "Epoch 24/40 - Train loss: 24.8969 - Val loss: 35.3790\n",
      "Epoch 25/40 - Train loss: 24.8970 - Val loss: 35.3682\n",
      "Epoch 26/40 - Train loss: 24.8965 - Val loss: 35.3631\n",
      "Epoch 27/40 - Train loss: 24.8967 - Val loss: 35.3929\n",
      "Epoch 28/40 - Train loss: 24.8969 - Val loss: 35.3912\n",
      "Epoch 29/40 - Train loss: 24.8974 - Val loss: 35.3663\n",
      "Epoch 30/40 - Train loss: 24.8961 - Val loss: 35.3575\n",
      "Epoch 31/40 - Train loss: 24.8970 - Val loss: 35.3499\n",
      "Epoch 32/40 - Train loss: 24.8968 - Val loss: 35.3786\n",
      "Epoch 33/40 - Train loss: 24.8976 - Val loss: 35.3636\n",
      "Epoch 34/40 - Train loss: 24.8968 - Val loss: 35.3368\n",
      "Epoch 35/40 - Train loss: 24.8968 - Val loss: 35.3600\n",
      "Epoch 36/40 - Train loss: 24.8963 - Val loss: 35.3703\n",
      "Epoch 37/40 - Train loss: 24.8963 - Val loss: 35.3923\n",
      "Epoch 38/40 - Train loss: 24.8963 - Val loss: 35.3700\n",
      "Epoch 39/40 - Train loss: 24.8968 - Val loss: 35.4014\n",
      "Epoch 40/40 - Train loss: 24.8966 - Val loss: 35.4065\n",
      "RMSE_train=4.9896 | RMSE_val=5.9503 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "--- alpha=5e-05, l1_ratio=0.1 ---\n",
      "Epoch 1/40 - Train loss: 79.2242 - Val loss: 66.5129\n",
      "Epoch 2/40 - Train loss: 35.0538 - Val loss: 31.7329\n",
      "Epoch 3/40 - Train loss: 27.6070 - Val loss: 32.0255\n",
      "Epoch 4/40 - Train loss: 25.8015 - Val loss: 33.6531\n",
      "Epoch 5/40 - Train loss: 25.2316 - Val loss: 34.5071\n",
      "Epoch 6/40 - Train loss: 25.0274 - Val loss: 34.9107\n",
      "Epoch 7/40 - Train loss: 24.9509 - Val loss: 35.1118\n",
      "Epoch 8/40 - Train loss: 24.9215 - Val loss: 35.1989\n",
      "Epoch 9/40 - Train loss: 24.9114 - Val loss: 35.2339\n",
      "Epoch 10/40 - Train loss: 24.9063 - Val loss: 35.2959\n",
      "Epoch 11/40 - Train loss: 24.9057 - Val loss: 35.3647\n",
      "Epoch 12/40 - Train loss: 24.9050 - Val loss: 35.3608\n",
      "Epoch 13/40 - Train loss: 24.9038 - Val loss: 35.3436\n",
      "Epoch 14/40 - Train loss: 24.9034 - Val loss: 35.3650\n",
      "Epoch 15/40 - Train loss: 24.9042 - Val loss: 35.3839\n",
      "Epoch 16/40 - Train loss: 24.9044 - Val loss: 35.4086\n",
      "Epoch 17/40 - Train loss: 24.9035 - Val loss: 35.3911\n",
      "Epoch 18/40 - Train loss: 24.9035 - Val loss: 35.3764\n",
      "Epoch 19/40 - Train loss: 24.9034 - Val loss: 35.4206\n",
      "Epoch 20/40 - Train loss: 24.9032 - Val loss: 35.4180\n",
      "Epoch 21/40 - Train loss: 24.9038 - Val loss: 35.4302\n",
      "Epoch 22/40 - Train loss: 24.9037 - Val loss: 35.3860\n",
      "Epoch 23/40 - Train loss: 24.9032 - Val loss: 35.4048\n",
      "Epoch 24/40 - Train loss: 24.9039 - Val loss: 35.3610\n",
      "Epoch 25/40 - Train loss: 24.9059 - Val loss: 35.3443\n",
      "Epoch 26/40 - Train loss: 24.9043 - Val loss: 35.3926\n",
      "Epoch 27/40 - Train loss: 24.9032 - Val loss: 35.3938\n",
      "Epoch 28/40 - Train loss: 24.9035 - Val loss: 35.3744\n",
      "Epoch 29/40 - Train loss: 24.9033 - Val loss: 35.3866\n",
      "Epoch 30/40 - Train loss: 24.9032 - Val loss: 35.3766\n",
      "Epoch 31/40 - Train loss: 24.9046 - Val loss: 35.3889\n",
      "Epoch 32/40 - Train loss: 24.9042 - Val loss: 35.3896\n",
      "Epoch 33/40 - Train loss: 24.9045 - Val loss: 35.3675\n",
      "Epoch 34/40 - Train loss: 24.9038 - Val loss: 35.4083\n",
      "Epoch 35/40 - Train loss: 24.9033 - Val loss: 35.3845\n",
      "Epoch 36/40 - Train loss: 24.9040 - Val loss: 35.3834\n",
      "Epoch 37/40 - Train loss: 24.9038 - Val loss: 35.3846\n",
      "Epoch 38/40 - Train loss: 24.9038 - Val loss: 35.4126\n",
      "Epoch 39/40 - Train loss: 24.9035 - Val loss: 35.4348\n",
      "Epoch 40/40 - Train loss: 24.9033 - Val loss: 35.3988\n",
      "RMSE_train=4.9896 | RMSE_val=5.9491 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha=5e-05, l1_ratio=0.5 ---\n",
      "Epoch 1/40 - Train loss: 79.2108 - Val loss: 66.5376\n",
      "Epoch 2/40 - Train loss: 35.0392 - Val loss: 31.7281\n",
      "Epoch 3/40 - Train loss: 27.5988 - Val loss: 32.0516\n",
      "Epoch 4/40 - Train loss: 25.7973 - Val loss: 33.6381\n",
      "Epoch 5/40 - Train loss: 25.2291 - Val loss: 34.5165\n",
      "Epoch 6/40 - Train loss: 25.0253 - Val loss: 34.8719\n",
      "Epoch 7/40 - Train loss: 24.9486 - Val loss: 35.0800\n",
      "Epoch 8/40 - Train loss: 24.9190 - Val loss: 35.2150\n",
      "Epoch 9/40 - Train loss: 24.9082 - Val loss: 35.3289\n",
      "Epoch 10/40 - Train loss: 24.9032 - Val loss: 35.3336\n",
      "Epoch 11/40 - Train loss: 24.9017 - Val loss: 35.3170\n",
      "Epoch 12/40 - Train loss: 24.9009 - Val loss: 35.3367\n",
      "Epoch 13/40 - Train loss: 24.9007 - Val loss: 35.3620\n",
      "Epoch 14/40 - Train loss: 24.9006 - Val loss: 35.3531\n",
      "Epoch 15/40 - Train loss: 24.9005 - Val loss: 35.3632\n",
      "Epoch 16/40 - Train loss: 24.9008 - Val loss: 35.3532\n",
      "Epoch 17/40 - Train loss: 24.9006 - Val loss: 35.3573\n",
      "Epoch 18/40 - Train loss: 24.9010 - Val loss: 35.3556\n",
      "Epoch 19/40 - Train loss: 24.9005 - Val loss: 35.3673\n",
      "Epoch 20/40 - Train loss: 24.9016 - Val loss: 35.3886\n",
      "Epoch 21/40 - Train loss: 24.9010 - Val loss: 35.3979\n",
      "Epoch 22/40 - Train loss: 24.9009 - Val loss: 35.3882\n",
      "Epoch 23/40 - Train loss: 24.9007 - Val loss: 35.3691\n",
      "Epoch 24/40 - Train loss: 24.9004 - Val loss: 35.4103\n",
      "Epoch 25/40 - Train loss: 24.9004 - Val loss: 35.4127\n",
      "Epoch 26/40 - Train loss: 24.9023 - Val loss: 35.4207\n",
      "Epoch 27/40 - Train loss: 24.9007 - Val loss: 35.3749\n",
      "Epoch 28/40 - Train loss: 24.9008 - Val loss: 35.3717\n",
      "Epoch 29/40 - Train loss: 24.9015 - Val loss: 35.3631\n",
      "Epoch 30/40 - Train loss: 24.9004 - Val loss: 35.3940\n",
      "Epoch 31/40 - Train loss: 24.9004 - Val loss: 35.3683\n",
      "Epoch 32/40 - Train loss: 24.9005 - Val loss: 35.3786\n",
      "Epoch 33/40 - Train loss: 24.9007 - Val loss: 35.3669\n",
      "Epoch 34/40 - Train loss: 24.9006 - Val loss: 35.3760\n",
      "Epoch 35/40 - Train loss: 24.9004 - Val loss: 35.3666\n",
      "Epoch 36/40 - Train loss: 24.9005 - Val loss: 35.3736\n",
      "Epoch 37/40 - Train loss: 24.9015 - Val loss: 35.4058\n",
      "Epoch 38/40 - Train loss: 24.9008 - Val loss: 35.3883\n",
      "Epoch 39/40 - Train loss: 24.9005 - Val loss: 35.3751\n",
      "Epoch 40/40 - Train loss: 24.9011 - Val loss: 35.3623\n",
      "RMSE_train=4.9896 | RMSE_val=5.9462 | R2_train=0.8719 | R2_val=0.8252\n",
      "\n",
      "--- alpha=5e-05, l1_ratio=0.9 ---\n",
      "Epoch 1/40 - Train loss: 79.1761 - Val loss: 66.4969\n",
      "Epoch 2/40 - Train loss: 35.0518 - Val loss: 31.7243\n",
      "Epoch 3/40 - Train loss: 27.5988 - Val loss: 32.0428\n",
      "Epoch 4/40 - Train loss: 25.7925 - Val loss: 33.6563\n",
      "Epoch 5/40 - Train loss: 25.2241 - Val loss: 34.4745\n",
      "Epoch 6/40 - Train loss: 25.0212 - Val loss: 34.9102\n",
      "Epoch 7/40 - Train loss: 24.9449 - Val loss: 35.0995\n",
      "Epoch 8/40 - Train loss: 24.9158 - Val loss: 35.2629\n",
      "Epoch 9/40 - Train loss: 24.9050 - Val loss: 35.3233\n",
      "Epoch 10/40 - Train loss: 24.9011 - Val loss: 35.3532\n",
      "Epoch 11/40 - Train loss: 24.8994 - Val loss: 35.3530\n",
      "Epoch 12/40 - Train loss: 24.8979 - Val loss: 35.3874\n",
      "Epoch 13/40 - Train loss: 24.8978 - Val loss: 35.3679\n",
      "Epoch 14/40 - Train loss: 24.8976 - Val loss: 35.3741\n",
      "Epoch 15/40 - Train loss: 24.8975 - Val loss: 35.3770\n",
      "Epoch 16/40 - Train loss: 24.8977 - Val loss: 35.4045\n",
      "Epoch 17/40 - Train loss: 24.8981 - Val loss: 35.3536\n",
      "Epoch 18/40 - Train loss: 24.8978 - Val loss: 35.3938\n",
      "Epoch 19/40 - Train loss: 24.8988 - Val loss: 35.3557\n",
      "Epoch 20/40 - Train loss: 24.8981 - Val loss: 35.3521\n",
      "Epoch 21/40 - Train loss: 24.8980 - Val loss: 35.3530\n",
      "Epoch 22/40 - Train loss: 24.8983 - Val loss: 35.3705\n",
      "Epoch 23/40 - Train loss: 24.8975 - Val loss: 35.3672\n",
      "Epoch 24/40 - Train loss: 24.8974 - Val loss: 35.3518\n",
      "Epoch 25/40 - Train loss: 24.8980 - Val loss: 35.3600\n",
      "Epoch 26/40 - Train loss: 24.8996 - Val loss: 35.3865\n",
      "Epoch 27/40 - Train loss: 24.8994 - Val loss: 35.3761\n",
      "Epoch 28/40 - Train loss: 24.8978 - Val loss: 35.3862\n",
      "Epoch 29/40 - Train loss: 24.8985 - Val loss: 35.3936\n",
      "Epoch 30/40 - Train loss: 24.8976 - Val loss: 35.3851\n",
      "Epoch 31/40 - Train loss: 24.8985 - Val loss: 35.4070\n",
      "Epoch 32/40 - Train loss: 24.8979 - Val loss: 35.3517\n",
      "Epoch 33/40 - Train loss: 24.8978 - Val loss: 35.4058\n",
      "Epoch 34/40 - Train loss: 24.8976 - Val loss: 35.3676\n",
      "Epoch 35/40 - Train loss: 24.8979 - Val loss: 35.3695\n",
      "Epoch 36/40 - Train loss: 24.8992 - Val loss: 35.3964\n",
      "Epoch 37/40 - Train loss: 24.8977 - Val loss: 35.3902\n",
      "Epoch 38/40 - Train loss: 24.8983 - Val loss: 35.3432\n",
      "Epoch 39/40 - Train loss: 24.8975 - Val loss: 35.3908\n",
      "Epoch 40/40 - Train loss: 24.8976 - Val loss: 35.3963\n",
      "RMSE_train=4.9896 | RMSE_val=5.9493 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha=0.0001, l1_ratio=0.1 ---\n",
      "Epoch 1/40 - Train loss: 79.1810 - Val loss: 66.4522\n",
      "Epoch 2/40 - Train loss: 35.0363 - Val loss: 31.7394\n",
      "Epoch 3/40 - Train loss: 27.6057 - Val loss: 32.0585\n",
      "Epoch 4/40 - Train loss: 25.8064 - Val loss: 33.6731\n",
      "Epoch 5/40 - Train loss: 25.2391 - Val loss: 34.5260\n",
      "Epoch 6/40 - Train loss: 25.0337 - Val loss: 34.8763\n",
      "Epoch 7/40 - Train loss: 24.9576 - Val loss: 35.0869\n",
      "Epoch 8/40 - Train loss: 24.9285 - Val loss: 35.2095\n",
      "Epoch 9/40 - Train loss: 24.9181 - Val loss: 35.2936\n",
      "Epoch 10/40 - Train loss: 24.9140 - Val loss: 35.3146\n",
      "Epoch 11/40 - Train loss: 24.9117 - Val loss: 35.3184\n",
      "Epoch 12/40 - Train loss: 24.9111 - Val loss: 35.3557\n",
      "Epoch 13/40 - Train loss: 24.9108 - Val loss: 35.3677\n",
      "Epoch 14/40 - Train loss: 24.9112 - Val loss: 35.3922\n",
      "Epoch 15/40 - Train loss: 24.9115 - Val loss: 35.4085\n",
      "Epoch 16/40 - Train loss: 24.9107 - Val loss: 35.3688\n",
      "Epoch 17/40 - Train loss: 24.9109 - Val loss: 35.3785\n",
      "Epoch 18/40 - Train loss: 24.9108 - Val loss: 35.4237\n",
      "Epoch 19/40 - Train loss: 24.9114 - Val loss: 35.4393\n",
      "Epoch 20/40 - Train loss: 24.9119 - Val loss: 35.3772\n",
      "Epoch 21/40 - Train loss: 24.9110 - Val loss: 35.3996\n",
      "Epoch 22/40 - Train loss: 24.9108 - Val loss: 35.4013\n",
      "Epoch 23/40 - Train loss: 24.9108 - Val loss: 35.3810\n",
      "Epoch 24/40 - Train loss: 24.9117 - Val loss: 35.3913\n",
      "Epoch 25/40 - Train loss: 24.9107 - Val loss: 35.4101\n",
      "Epoch 26/40 - Train loss: 24.9106 - Val loss: 35.3680\n",
      "Epoch 27/40 - Train loss: 24.9125 - Val loss: 35.3697\n",
      "Epoch 28/40 - Train loss: 24.9107 - Val loss: 35.3684\n",
      "Epoch 29/40 - Train loss: 24.9114 - Val loss: 35.3988\n",
      "Epoch 30/40 - Train loss: 24.9113 - Val loss: 35.4091\n",
      "Epoch 31/40 - Train loss: 24.9117 - Val loss: 35.3956\n",
      "Epoch 32/40 - Train loss: 24.9111 - Val loss: 35.4062\n",
      "Epoch 33/40 - Train loss: 24.9113 - Val loss: 35.4067\n",
      "Epoch 34/40 - Train loss: 24.9110 - Val loss: 35.3748\n",
      "Epoch 35/40 - Train loss: 24.9115 - Val loss: 35.3249\n",
      "Epoch 36/40 - Train loss: 24.9111 - Val loss: 35.3451\n",
      "Epoch 37/40 - Train loss: 24.9114 - Val loss: 35.4205\n",
      "Epoch 38/40 - Train loss: 24.9109 - Val loss: 35.4167\n",
      "Epoch 39/40 - Train loss: 24.9109 - Val loss: 35.4116\n",
      "Epoch 40/40 - Train loss: 24.9107 - Val loss: 35.3973\n",
      "RMSE_train=4.9896 | RMSE_val=5.9483 | R2_train=0.8719 | R2_val=0.8250\n",
      "\n",
      "--- alpha=0.0001, l1_ratio=0.5 ---\n",
      "Epoch 1/40 - Train loss: 79.2423 - Val loss: 66.5429\n",
      "Epoch 2/40 - Train loss: 35.0518 - Val loss: 31.7436\n",
      "Epoch 3/40 - Train loss: 27.6080 - Val loss: 32.0491\n",
      "Epoch 4/40 - Train loss: 25.7995 - Val loss: 33.6856\n",
      "Epoch 5/40 - Train loss: 25.2319 - Val loss: 34.5222\n",
      "Epoch 6/40 - Train loss: 25.0279 - Val loss: 34.9448\n",
      "Epoch 7/40 - Train loss: 24.9526 - Val loss: 35.1089\n",
      "Epoch 8/40 - Train loss: 24.9236 - Val loss: 35.2161\n",
      "Epoch 9/40 - Train loss: 24.9124 - Val loss: 35.2865\n",
      "Epoch 10/40 - Train loss: 24.9076 - Val loss: 35.3683\n",
      "Epoch 11/40 - Train loss: 24.9059 - Val loss: 35.3678\n",
      "Epoch 12/40 - Train loss: 24.9058 - Val loss: 35.3548\n",
      "Epoch 13/40 - Train loss: 24.9051 - Val loss: 35.3557\n",
      "Epoch 14/40 - Train loss: 24.9052 - Val loss: 35.3749\n",
      "Epoch 15/40 - Train loss: 24.9055 - Val loss: 35.4175\n",
      "Epoch 16/40 - Train loss: 24.9057 - Val loss: 35.3655\n",
      "Epoch 17/40 - Train loss: 24.9052 - Val loss: 35.3828\n",
      "Epoch 18/40 - Train loss: 24.9050 - Val loss: 35.3919\n",
      "Epoch 19/40 - Train loss: 24.9055 - Val loss: 35.3849\n",
      "Epoch 20/40 - Train loss: 24.9070 - Val loss: 35.4071\n",
      "Epoch 21/40 - Train loss: 24.9049 - Val loss: 35.3908\n",
      "Epoch 22/40 - Train loss: 24.9052 - Val loss: 35.3969\n",
      "Epoch 23/40 - Train loss: 24.9050 - Val loss: 35.3847\n",
      "Epoch 24/40 - Train loss: 24.9052 - Val loss: 35.3847\n",
      "Epoch 25/40 - Train loss: 24.9050 - Val loss: 35.3907\n",
      "Epoch 26/40 - Train loss: 24.9066 - Val loss: 35.3947\n",
      "Epoch 27/40 - Train loss: 24.9051 - Val loss: 35.3808\n",
      "Epoch 28/40 - Train loss: 24.9050 - Val loss: 35.3661\n",
      "Epoch 29/40 - Train loss: 24.9052 - Val loss: 35.3916\n",
      "Epoch 30/40 - Train loss: 24.9057 - Val loss: 35.3923\n",
      "Epoch 31/40 - Train loss: 24.9049 - Val loss: 35.3691\n",
      "Epoch 32/40 - Train loss: 24.9049 - Val loss: 35.3707\n",
      "Epoch 33/40 - Train loss: 24.9054 - Val loss: 35.3907\n",
      "Epoch 34/40 - Train loss: 24.9054 - Val loss: 35.3865\n",
      "Epoch 35/40 - Train loss: 24.9050 - Val loss: 35.3948\n",
      "Epoch 36/40 - Train loss: 24.9050 - Val loss: 35.3683\n",
      "Epoch 37/40 - Train loss: 24.9049 - Val loss: 35.3760\n",
      "Epoch 38/40 - Train loss: 24.9054 - Val loss: 35.4058\n",
      "Epoch 39/40 - Train loss: 24.9049 - Val loss: 35.4140\n",
      "Epoch 40/40 - Train loss: 24.9049 - Val loss: 35.4197\n",
      "RMSE_train=4.9896 | RMSE_val=5.9507 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "--- alpha=0.0001, l1_ratio=0.9 ---\n",
      "Epoch 1/40 - Train loss: 79.2005 - Val loss: 66.5129\n",
      "Epoch 2/40 - Train loss: 35.0294 - Val loss: 31.7243\n",
      "Epoch 3/40 - Train loss: 27.5936 - Val loss: 32.0428\n",
      "Epoch 4/40 - Train loss: 25.7918 - Val loss: 33.6867\n",
      "Epoch 5/40 - Train loss: 25.2245 - Val loss: 34.4985\n",
      "Epoch 6/40 - Train loss: 25.0221 - Val loss: 34.8975\n",
      "Epoch 7/40 - Train loss: 24.9467 - Val loss: 35.0995\n",
      "Epoch 8/40 - Train loss: 24.9174 - Val loss: 35.1958\n",
      "Epoch 9/40 - Train loss: 24.9062 - Val loss: 35.2751\n",
      "Epoch 10/40 - Train loss: 24.9023 - Val loss: 35.3280\n",
      "Epoch 11/40 - Train loss: 24.9003 - Val loss: 35.3662\n",
      "Epoch 12/40 - Train loss: 24.8998 - Val loss: 35.3854\n",
      "Epoch 13/40 - Train loss: 24.8996 - Val loss: 35.3787\n",
      "Epoch 14/40 - Train loss: 24.8994 - Val loss: 35.3852\n",
      "Epoch 15/40 - Train loss: 24.8993 - Val loss: 35.3774\n",
      "Epoch 16/40 - Train loss: 24.8996 - Val loss: 35.3846\n",
      "Epoch 17/40 - Train loss: 24.8996 - Val loss: 35.3774\n",
      "Epoch 18/40 - Train loss: 24.8994 - Val loss: 35.3825\n",
      "Epoch 19/40 - Train loss: 24.8997 - Val loss: 35.3809\n",
      "Epoch 20/40 - Train loss: 24.8993 - Val loss: 35.3512\n",
      "Epoch 21/40 - Train loss: 24.8992 - Val loss: 35.3729\n",
      "Epoch 22/40 - Train loss: 24.8996 - Val loss: 35.3695\n",
      "Epoch 23/40 - Train loss: 24.9001 - Val loss: 35.3853\n",
      "Epoch 24/40 - Train loss: 24.8993 - Val loss: 35.3686\n",
      "Epoch 25/40 - Train loss: 24.8992 - Val loss: 35.3857\n",
      "Epoch 26/40 - Train loss: 24.8993 - Val loss: 35.3615\n",
      "Epoch 27/40 - Train loss: 24.8994 - Val loss: 35.3834\n",
      "Epoch 28/40 - Train loss: 24.9002 - Val loss: 35.4030\n",
      "Epoch 29/40 - Train loss: 24.8992 - Val loss: 35.3939\n",
      "Epoch 30/40 - Train loss: 24.9013 - Val loss: 35.3881\n",
      "Epoch 31/40 - Train loss: 24.8992 - Val loss: 35.4022\n",
      "Epoch 32/40 - Train loss: 24.8998 - Val loss: 35.3833\n",
      "Epoch 33/40 - Train loss: 24.9002 - Val loss: 35.3753\n",
      "Epoch 34/40 - Train loss: 24.9011 - Val loss: 35.3760\n",
      "Epoch 35/40 - Train loss: 24.8992 - Val loss: 35.3776\n",
      "Epoch 36/40 - Train loss: 24.8999 - Val loss: 35.3902\n",
      "Epoch 37/40 - Train loss: 24.8996 - Val loss: 35.3777\n",
      "Epoch 38/40 - Train loss: 24.8993 - Val loss: 35.3797\n",
      "Epoch 39/40 - Train loss: 24.8998 - Val loss: 35.3975\n",
      "Epoch 40/40 - Train loss: 24.8999 - Val loss: 35.3608\n",
      "RMSE_train=4.9896 | RMSE_val=5.9462 | R2_train=0.8719 | R2_val=0.8252\n",
      "\n",
      "--- alpha=0.0005, l1_ratio=0.1 ---\n",
      "Epoch 1/40 - Train loss: 79.2846 - Val loss: 66.5200\n",
      "Epoch 2/40 - Train loss: 35.0899 - Val loss: 31.7798\n",
      "Epoch 3/40 - Train loss: 27.6601 - Val loss: 32.1251\n",
      "Epoch 4/40 - Train loss: 25.8591 - Val loss: 33.7258\n",
      "Epoch 5/40 - Train loss: 25.2933 - Val loss: 34.5551\n",
      "Epoch 6/40 - Train loss: 25.0928 - Val loss: 34.9655\n",
      "Epoch 7/40 - Train loss: 25.0172 - Val loss: 35.1627\n",
      "Epoch 8/40 - Train loss: 24.9883 - Val loss: 35.2821\n",
      "Epoch 9/40 - Train loss: 24.9784 - Val loss: 35.3152\n",
      "Epoch 10/40 - Train loss: 24.9731 - Val loss: 35.3620\n",
      "Epoch 11/40 - Train loss: 24.9710 - Val loss: 35.3950\n",
      "Epoch 12/40 - Train loss: 24.9707 - Val loss: 35.4239\n",
      "Epoch 13/40 - Train loss: 24.9711 - Val loss: 35.4445\n",
      "Epoch 14/40 - Train loss: 24.9714 - Val loss: 35.4387\n",
      "Epoch 15/40 - Train loss: 24.9704 - Val loss: 35.3945\n",
      "Epoch 16/40 - Train loss: 24.9699 - Val loss: 35.4191\n",
      "Epoch 17/40 - Train loss: 24.9702 - Val loss: 35.4500\n",
      "Epoch 18/40 - Train loss: 24.9704 - Val loss: 35.4385\n",
      "Epoch 19/40 - Train loss: 24.9706 - Val loss: 35.4681\n",
      "Epoch 20/40 - Train loss: 24.9711 - Val loss: 35.4738\n",
      "Epoch 21/40 - Train loss: 24.9733 - Val loss: 35.4428\n",
      "Epoch 22/40 - Train loss: 24.9705 - Val loss: 35.4287\n",
      "Epoch 23/40 - Train loss: 24.9712 - Val loss: 35.4537\n",
      "Epoch 24/40 - Train loss: 24.9703 - Val loss: 35.4256\n",
      "Epoch 25/40 - Train loss: 24.9703 - Val loss: 35.4176\n",
      "Epoch 26/40 - Train loss: 24.9714 - Val loss: 35.4351\n",
      "Epoch 27/40 - Train loss: 24.9702 - Val loss: 35.4287\n",
      "Epoch 28/40 - Train loss: 24.9710 - Val loss: 35.4022\n",
      "Epoch 29/40 - Train loss: 24.9705 - Val loss: 35.4016\n",
      "Epoch 30/40 - Train loss: 24.9701 - Val loss: 35.4014\n",
      "Epoch 31/40 - Train loss: 24.9702 - Val loss: 35.3978\n",
      "Epoch 32/40 - Train loss: 24.9705 - Val loss: 35.4409\n",
      "Epoch 33/40 - Train loss: 24.9706 - Val loss: 35.4672\n",
      "Epoch 34/40 - Train loss: 24.9707 - Val loss: 35.4728\n",
      "Epoch 35/40 - Train loss: 24.9703 - Val loss: 35.4484\n",
      "Epoch 36/40 - Train loss: 24.9705 - Val loss: 35.5096\n",
      "Epoch 37/40 - Train loss: 24.9708 - Val loss: 35.4831\n",
      "Epoch 38/40 - Train loss: 24.9702 - Val loss: 35.4367\n",
      "Epoch 39/40 - Train loss: 24.9705 - Val loss: 35.3714\n",
      "Epoch 40/40 - Train loss: 24.9707 - Val loss: 35.3818\n",
      "RMSE_train=4.9896 | RMSE_val=5.9420 | R2_train=0.8719 | R2_val=0.8254\n",
      "\n",
      "--- alpha=0.0005, l1_ratio=0.5 ---\n",
      "Epoch 1/40 - Train loss: 79.2572 - Val loss: 66.5112\n",
      "Epoch 2/40 - Train loss: 35.0751 - Val loss: 31.7550\n",
      "Epoch 3/40 - Train loss: 27.6403 - Val loss: 32.0700\n",
      "Epoch 4/40 - Train loss: 25.8347 - Val loss: 33.6791\n",
      "Epoch 5/40 - Train loss: 25.2677 - Val loss: 34.5121\n",
      "Epoch 6/40 - Train loss: 25.0637 - Val loss: 34.9092\n",
      "Epoch 7/40 - Train loss: 24.9880 - Val loss: 35.1109\n",
      "Epoch 8/40 - Train loss: 24.9590 - Val loss: 35.2047\n",
      "Epoch 9/40 - Train loss: 24.9480 - Val loss: 35.3070\n",
      "Epoch 10/40 - Train loss: 24.9438 - Val loss: 35.3493\n",
      "Epoch 11/40 - Train loss: 24.9438 - Val loss: 35.3876\n",
      "Epoch 12/40 - Train loss: 24.9425 - Val loss: 35.4120\n",
      "Epoch 13/40 - Train loss: 24.9421 - Val loss: 35.4349\n",
      "Epoch 14/40 - Train loss: 24.9412 - Val loss: 35.4171\n",
      "Epoch 15/40 - Train loss: 24.9417 - Val loss: 35.4184\n",
      "Epoch 16/40 - Train loss: 24.9413 - Val loss: 35.4076\n",
      "Epoch 17/40 - Train loss: 24.9412 - Val loss: 35.3676\n",
      "Epoch 18/40 - Train loss: 24.9416 - Val loss: 35.3991\n",
      "Epoch 19/40 - Train loss: 24.9412 - Val loss: 35.3872\n",
      "Epoch 20/40 - Train loss: 24.9421 - Val loss: 35.4264\n",
      "Epoch 21/40 - Train loss: 24.9416 - Val loss: 35.4024\n",
      "Epoch 22/40 - Train loss: 24.9413 - Val loss: 35.4121\n",
      "Epoch 23/40 - Train loss: 24.9416 - Val loss: 35.3805\n",
      "Epoch 24/40 - Train loss: 24.9415 - Val loss: 35.3768\n",
      "Epoch 25/40 - Train loss: 24.9412 - Val loss: 35.4052\n",
      "Epoch 26/40 - Train loss: 24.9412 - Val loss: 35.4009\n",
      "Epoch 27/40 - Train loss: 24.9415 - Val loss: 35.3983\n",
      "Epoch 28/40 - Train loss: 24.9413 - Val loss: 35.4339\n",
      "Epoch 29/40 - Train loss: 24.9416 - Val loss: 35.4281\n",
      "Epoch 30/40 - Train loss: 24.9417 - Val loss: 35.3492\n",
      "Epoch 31/40 - Train loss: 24.9413 - Val loss: 35.3758\n",
      "Epoch 32/40 - Train loss: 24.9411 - Val loss: 35.4021\n",
      "Epoch 33/40 - Train loss: 24.9415 - Val loss: 35.4145\n",
      "Epoch 34/40 - Train loss: 24.9413 - Val loss: 35.4021\n",
      "Epoch 35/40 - Train loss: 24.9417 - Val loss: 35.3797\n",
      "Epoch 36/40 - Train loss: 24.9414 - Val loss: 35.3684\n",
      "Epoch 37/40 - Train loss: 24.9417 - Val loss: 35.4042\n",
      "Epoch 38/40 - Train loss: 24.9421 - Val loss: 35.4002\n",
      "Epoch 39/40 - Train loss: 24.9416 - Val loss: 35.4194\n",
      "Epoch 40/40 - Train loss: 24.9412 - Val loss: 35.4456\n",
      "RMSE_train=4.9896 | RMSE_val=5.9498 | R2_train=0.8719 | R2_val=0.8249\n",
      "\n",
      "--- alpha=0.0005, l1_ratio=0.9 ---\n",
      "Epoch 1/40 - Train loss: 79.2818 - Val loss: 66.6418\n",
      "Epoch 2/40 - Train loss: 35.0696 - Val loss: 31.7611\n",
      "Epoch 3/40 - Train loss: 27.6149 - Val loss: 32.0353\n",
      "Epoch 4/40 - Train loss: 25.8096 - Val loss: 33.6682\n",
      "Epoch 5/40 - Train loss: 25.2403 - Val loss: 34.5134\n",
      "Epoch 6/40 - Train loss: 25.0349 - Val loss: 34.9276\n",
      "Epoch 7/40 - Train loss: 24.9593 - Val loss: 35.1275\n",
      "Epoch 8/40 - Train loss: 24.9303 - Val loss: 35.2172\n",
      "Epoch 9/40 - Train loss: 24.9193 - Val loss: 35.3050\n",
      "Epoch 10/40 - Train loss: 24.9150 - Val loss: 35.3098\n",
      "Epoch 11/40 - Train loss: 24.9134 - Val loss: 35.3354\n",
      "Epoch 12/40 - Train loss: 24.9129 - Val loss: 35.3234\n",
      "Epoch 13/40 - Train loss: 24.9126 - Val loss: 35.3456\n",
      "Epoch 14/40 - Train loss: 24.9125 - Val loss: 35.3780\n",
      "Epoch 15/40 - Train loss: 24.9133 - Val loss: 35.4052\n",
      "Epoch 16/40 - Train loss: 24.9124 - Val loss: 35.3867\n",
      "Epoch 17/40 - Train loss: 24.9125 - Val loss: 35.3655\n",
      "Epoch 18/40 - Train loss: 24.9127 - Val loss: 35.3341\n",
      "Epoch 19/40 - Train loss: 24.9124 - Val loss: 35.4013\n",
      "Epoch 20/40 - Train loss: 24.9125 - Val loss: 35.3865\n",
      "Epoch 21/40 - Train loss: 24.9122 - Val loss: 35.3883\n",
      "Epoch 22/40 - Train loss: 24.9126 - Val loss: 35.4018\n",
      "Epoch 23/40 - Train loss: 24.9127 - Val loss: 35.3857\n",
      "Epoch 24/40 - Train loss: 24.9126 - Val loss: 35.3630\n",
      "Epoch 25/40 - Train loss: 24.9126 - Val loss: 35.3887\n",
      "Epoch 26/40 - Train loss: 24.9124 - Val loss: 35.3378\n",
      "Epoch 27/40 - Train loss: 24.9124 - Val loss: 35.4012\n",
      "Epoch 28/40 - Train loss: 24.9123 - Val loss: 35.3969\n",
      "Epoch 29/40 - Train loss: 24.9123 - Val loss: 35.4286\n",
      "Epoch 30/40 - Train loss: 24.9122 - Val loss: 35.4033\n",
      "Epoch 31/40 - Train loss: 24.9131 - Val loss: 35.4035\n",
      "Epoch 32/40 - Train loss: 24.9123 - Val loss: 35.3529\n",
      "Epoch 33/40 - Train loss: 24.9135 - Val loss: 35.3871\n",
      "Epoch 34/40 - Train loss: 24.9131 - Val loss: 35.3907\n",
      "Epoch 35/40 - Train loss: 24.9124 - Val loss: 35.3918\n",
      "Epoch 36/40 - Train loss: 24.9125 - Val loss: 35.4007\n",
      "Epoch 37/40 - Train loss: 24.9129 - Val loss: 35.4382\n",
      "Epoch 38/40 - Train loss: 24.9125 - Val loss: 35.3943\n",
      "Epoch 39/40 - Train loss: 24.9122 - Val loss: 35.3865\n",
      "Epoch 40/40 - Train loss: 24.9127 - Val loss: 35.3867\n",
      "RMSE_train=4.9896 | RMSE_val=5.9473 | R2_train=0.8719 | R2_val=0.8251\n",
      "\n",
      "--- alpha=0.001, l1_ratio=0.1 ---\n",
      "Epoch 1/40 - Train loss: 79.3103 - Val loss: 66.5677\n",
      "Epoch 2/40 - Train loss: 35.1508 - Val loss: 31.8190\n",
      "Epoch 3/40 - Train loss: 27.7293 - Val loss: 32.1332\n",
      "Epoch 4/40 - Train loss: 25.9297 - Val loss: 33.7630\n",
      "Epoch 5/40 - Train loss: 25.3654 - Val loss: 34.5833\n",
      "Epoch 6/40 - Train loss: 25.1650 - Val loss: 34.9997\n",
      "Epoch 7/40 - Train loss: 25.0912 - Val loss: 35.1826\n",
      "Epoch 8/40 - Train loss: 25.0627 - Val loss: 35.3287\n",
      "Epoch 9/40 - Train loss: 25.0515 - Val loss: 35.3836\n",
      "Epoch 10/40 - Train loss: 25.0471 - Val loss: 35.4130\n",
      "Epoch 11/40 - Train loss: 25.0477 - Val loss: 35.4808\n",
      "Epoch 12/40 - Train loss: 25.0449 - Val loss: 35.4889\n",
      "Epoch 13/40 - Train loss: 25.0443 - Val loss: 35.4513\n",
      "Epoch 14/40 - Train loss: 25.0445 - Val loss: 35.4921\n",
      "Epoch 15/40 - Train loss: 25.0444 - Val loss: 35.4972\n",
      "Epoch 16/40 - Train loss: 25.0444 - Val loss: 35.4850\n",
      "Epoch 17/40 - Train loss: 25.0446 - Val loss: 35.4854\n",
      "Epoch 18/40 - Train loss: 25.0445 - Val loss: 35.4904\n",
      "Epoch 19/40 - Train loss: 25.0446 - Val loss: 35.5139\n",
      "Epoch 20/40 - Train loss: 25.0447 - Val loss: 35.4799\n",
      "Epoch 21/40 - Train loss: 25.0445 - Val loss: 35.4752\n",
      "Epoch 22/40 - Train loss: 25.0442 - Val loss: 35.5192\n",
      "Epoch 23/40 - Train loss: 25.0447 - Val loss: 35.4938\n",
      "Epoch 24/40 - Train loss: 25.0443 - Val loss: 35.4690\n",
      "Epoch 25/40 - Train loss: 25.0443 - Val loss: 35.4799\n",
      "Epoch 26/40 - Train loss: 25.0446 - Val loss: 35.4814\n",
      "Epoch 27/40 - Train loss: 25.0447 - Val loss: 35.5082\n",
      "Epoch 28/40 - Train loss: 25.0454 - Val loss: 35.4851\n",
      "Epoch 29/40 - Train loss: 25.0454 - Val loss: 35.4809\n",
      "Epoch 30/40 - Train loss: 25.0447 - Val loss: 35.4954\n",
      "Epoch 31/40 - Train loss: 25.0452 - Val loss: 35.4935\n",
      "Epoch 32/40 - Train loss: 25.0449 - Val loss: 35.4773\n",
      "Epoch 33/40 - Train loss: 25.0449 - Val loss: 35.4988\n",
      "Epoch 34/40 - Train loss: 25.0467 - Val loss: 35.4430\n",
      "Epoch 35/40 - Train loss: 25.0445 - Val loss: 35.4784\n",
      "Epoch 36/40 - Train loss: 25.0443 - Val loss: 35.5089\n",
      "Epoch 37/40 - Train loss: 25.0443 - Val loss: 35.4713\n",
      "Epoch 38/40 - Train loss: 25.0444 - Val loss: 35.4913\n",
      "Epoch 39/40 - Train loss: 25.0442 - Val loss: 35.4891\n",
      "Epoch 40/40 - Train loss: 25.0444 - Val loss: 35.4646\n",
      "RMSE_train=4.9896 | RMSE_val=5.9428 | R2_train=0.8719 | R2_val=0.8254\n",
      "\n",
      "--- alpha=0.001, l1_ratio=0.5 ---\n",
      "Epoch 1/40 - Train loss: 79.2884 - Val loss: 66.6010\n",
      "Epoch 2/40 - Train loss: 35.1180 - Val loss: 31.7977\n",
      "Epoch 3/40 - Train loss: 27.6773 - Val loss: 32.0745\n",
      "Epoch 4/40 - Train loss: 25.8797 - Val loss: 33.6982\n",
      "Epoch 5/40 - Train loss: 25.3118 - Val loss: 34.5466\n",
      "Epoch 6/40 - Train loss: 25.1080 - Val loss: 34.9638\n",
      "Epoch 7/40 - Train loss: 25.0319 - Val loss: 35.1896\n",
      "Epoch 8/40 - Train loss: 25.0041 - Val loss: 35.2506\n",
      "Epoch 9/40 - Train loss: 24.9931 - Val loss: 35.3560\n",
      "Epoch 10/40 - Train loss: 24.9895 - Val loss: 35.3655\n",
      "Epoch 11/40 - Train loss: 24.9904 - Val loss: 35.4166\n",
      "Epoch 12/40 - Train loss: 24.9874 - Val loss: 35.4231\n",
      "Epoch 13/40 - Train loss: 24.9868 - Val loss: 35.4122\n",
      "Epoch 14/40 - Train loss: 24.9878 - Val loss: 35.3876\n",
      "Epoch 15/40 - Train loss: 24.9869 - Val loss: 35.4360\n",
      "Epoch 16/40 - Train loss: 24.9864 - Val loss: 35.4464\n",
      "Epoch 17/40 - Train loss: 24.9872 - Val loss: 35.4752\n",
      "Epoch 18/40 - Train loss: 24.9869 - Val loss: 35.4554\n",
      "Epoch 19/40 - Train loss: 24.9865 - Val loss: 35.4289\n",
      "Epoch 20/40 - Train loss: 24.9865 - Val loss: 35.4269\n",
      "Epoch 21/40 - Train loss: 24.9872 - Val loss: 35.4618\n",
      "Epoch 22/40 - Train loss: 24.9865 - Val loss: 35.4262\n",
      "Epoch 23/40 - Train loss: 24.9871 - Val loss: 35.4388\n",
      "Epoch 24/40 - Train loss: 24.9871 - Val loss: 35.4147\n",
      "Epoch 25/40 - Train loss: 24.9865 - Val loss: 35.3948\n",
      "Epoch 26/40 - Train loss: 24.9867 - Val loss: 35.4339\n",
      "Epoch 27/40 - Train loss: 24.9874 - Val loss: 35.4441\n",
      "Epoch 28/40 - Train loss: 24.9868 - Val loss: 35.4422\n",
      "Epoch 29/40 - Train loss: 24.9872 - Val loss: 35.4119\n",
      "Epoch 30/40 - Train loss: 24.9871 - Val loss: 35.4231\n",
      "Epoch 31/40 - Train loss: 24.9865 - Val loss: 35.4574\n",
      "Epoch 32/40 - Train loss: 24.9867 - Val loss: 35.4132\n",
      "Epoch 33/40 - Train loss: 24.9881 - Val loss: 35.4453\n",
      "Epoch 34/40 - Train loss: 24.9868 - Val loss: 35.4419\n",
      "Epoch 35/40 - Train loss: 24.9865 - Val loss: 35.4504\n",
      "Epoch 36/40 - Train loss: 24.9869 - Val loss: 35.4535\n",
      "Epoch 37/40 - Train loss: 24.9868 - Val loss: 35.4558\n",
      "Epoch 38/40 - Train loss: 24.9865 - Val loss: 35.4167\n",
      "Epoch 39/40 - Train loss: 24.9868 - Val loss: 35.4303\n",
      "Epoch 40/40 - Train loss: 24.9863 - Val loss: 35.4606\n",
      "RMSE_train=4.9896 | RMSE_val=5.9473 | R2_train=0.8719 | R2_val=0.8251\n",
      "\n",
      "--- alpha=0.001, l1_ratio=0.9 ---\n",
      "Epoch 1/40 - Train loss: 79.2609 - Val loss: 66.5611\n",
      "Epoch 2/40 - Train loss: 35.0802 - Val loss: 31.7492\n",
      "Epoch 3/40 - Train loss: 27.6294 - Val loss: 32.0492\n",
      "Epoch 4/40 - Train loss: 25.8230 - Val loss: 33.6394\n",
      "Epoch 5/40 - Train loss: 25.2540 - Val loss: 34.5147\n",
      "Epoch 6/40 - Train loss: 25.0517 - Val loss: 34.9069\n",
      "Epoch 7/40 - Train loss: 24.9771 - Val loss: 35.1007\n",
      "Epoch 8/40 - Train loss: 24.9468 - Val loss: 35.2338\n",
      "Epoch 9/40 - Train loss: 24.9356 - Val loss: 35.2802\n",
      "Epoch 10/40 - Train loss: 24.9313 - Val loss: 35.3365\n",
      "Epoch 11/40 - Train loss: 24.9299 - Val loss: 35.3431\n",
      "Epoch 12/40 - Train loss: 24.9299 - Val loss: 35.3530\n",
      "Epoch 13/40 - Train loss: 24.9293 - Val loss: 35.4023\n",
      "Epoch 14/40 - Train loss: 24.9287 - Val loss: 35.4347\n",
      "Epoch 15/40 - Train loss: 24.9288 - Val loss: 35.3907\n",
      "Epoch 16/40 - Train loss: 24.9291 - Val loss: 35.3852\n",
      "Epoch 17/40 - Train loss: 24.9291 - Val loss: 35.3758\n",
      "Epoch 18/40 - Train loss: 24.9287 - Val loss: 35.3558\n",
      "Epoch 19/40 - Train loss: 24.9293 - Val loss: 35.3759\n",
      "Epoch 20/40 - Train loss: 24.9297 - Val loss: 35.3998\n",
      "Epoch 21/40 - Train loss: 24.9293 - Val loss: 35.3777\n",
      "Epoch 22/40 - Train loss: 24.9290 - Val loss: 35.3835\n",
      "Epoch 23/40 - Train loss: 24.9286 - Val loss: 35.4100\n",
      "Epoch 24/40 - Train loss: 24.9288 - Val loss: 35.3963\n",
      "Epoch 25/40 - Train loss: 24.9290 - Val loss: 35.3713\n",
      "Epoch 26/40 - Train loss: 24.9288 - Val loss: 35.4104\n",
      "Epoch 27/40 - Train loss: 24.9286 - Val loss: 35.3886\n",
      "Epoch 28/40 - Train loss: 24.9292 - Val loss: 35.3915\n",
      "Epoch 29/40 - Train loss: 24.9290 - Val loss: 35.3855\n",
      "Epoch 30/40 - Train loss: 24.9295 - Val loss: 35.3826\n",
      "Epoch 31/40 - Train loss: 24.9298 - Val loss: 35.3948\n",
      "Epoch 32/40 - Train loss: 24.9292 - Val loss: 35.4008\n",
      "Epoch 33/40 - Train loss: 24.9288 - Val loss: 35.3780\n",
      "Epoch 34/40 - Train loss: 24.9293 - Val loss: 35.4107\n",
      "Epoch 35/40 - Train loss: 24.9289 - Val loss: 35.3969\n",
      "Epoch 36/40 - Train loss: 24.9287 - Val loss: 35.4013\n",
      "Epoch 37/40 - Train loss: 24.9294 - Val loss: 35.4353\n",
      "Epoch 38/40 - Train loss: 24.9294 - Val loss: 35.4341\n",
      "Epoch 39/40 - Train loss: 24.9287 - Val loss: 35.3868\n",
      "Epoch 40/40 - Train loss: 24.9296 - Val loss: 35.3751\n",
      "RMSE_train=4.9897 | RMSE_val=5.9449 | R2_train=0.8719 | R2_val=0.8252\n",
      "\n",
      "\n",
      "=== Resumen ElasticNet ordenado por RMSE_val ===\n",
      "alpha=0.0005   l1_ratio=0.1 rmse_train=4.9896 rmse_val=5.9420 r2_train=0.8719 r2_val=0.8254\n",
      "alpha=0.001    l1_ratio=0.1 rmse_train=4.9896 rmse_val=5.9428 r2_train=0.8719 r2_val=0.8254\n",
      "alpha=0.001    l1_ratio=0.9 rmse_train=4.9897 rmse_val=5.9449 r2_train=0.8719 r2_val=0.8252\n",
      "alpha=1e-05    l1_ratio=0.1 rmse_train=4.9896 rmse_val=5.9455 r2_train=0.8719 r2_val=0.8252\n",
      "alpha=0.0001   l1_ratio=0.9 rmse_train=4.9896 rmse_val=5.9462 r2_train=0.8719 r2_val=0.8252\n",
      "alpha=5e-05    l1_ratio=0.5 rmse_train=4.9896 rmse_val=5.9462 r2_train=0.8719 r2_val=0.8252\n",
      "alpha=0.001    l1_ratio=0.5 rmse_train=4.9896 rmse_val=5.9473 r2_train=0.8719 r2_val=0.8251\n",
      "alpha=0.0005   l1_ratio=0.9 rmse_train=4.9896 rmse_val=5.9473 r2_train=0.8719 r2_val=0.8251\n",
      "alpha=0.0001   l1_ratio=0.1 rmse_train=4.9896 rmse_val=5.9483 r2_train=0.8719 r2_val=0.8250\n",
      "alpha=1e-05    l1_ratio=0.5 rmse_train=4.9896 rmse_val=5.9490 r2_train=0.8719 r2_val=0.8250\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. ElasticNet (from-scratch) - Pickup Only\n",
    "# ============================================================\n",
    "\n",
    "enet_results = []\n",
    "\n",
    "enet_alpha_list = [1e-5, 5e-5, 1e-4, 5e-4, 1e-3]      # fuerza total de regularización\n",
    "l1_ratio_list  = [0.1, 0.5, 0.9]                     # mezcla L1/L2\n",
    "\n",
    "print(\"Tuning ElasticNet (L1 + L2):\\n\")\n",
    "\n",
    "for a in enet_alpha_list:\n",
    "    for l1r in l1_ratio_list:\n",
    "\n",
    "        print(f\"--- alpha={a}, l1_ratio={l1r} ---\")\n",
    "\n",
    "        enet = LinearRegressorScratch(\n",
    "            learning_rate=1e-3,\n",
    "            max_iter=40,\n",
    "            batch_size=2048,\n",
    "            penalty=\"elasticnet\",\n",
    "            alpha=a,\n",
    "            l1_ratio=l1r,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        enet.fit(\n",
    "            X_train_scaled,\n",
    "            y_train_np,\n",
    "            X_val=X_val_scaled,\n",
    "            y_val=y_val_np\n",
    "        )\n",
    "\n",
    "        y_train_pred = enet.predict(X_train_scaled)\n",
    "        y_val_pred   = enet.predict(X_val_scaled)\n",
    "\n",
    "        rmse_tr = rmse(y_train_np, y_train_pred)\n",
    "        rmse_v  = rmse(y_val_np, y_val_pred)\n",
    "        r2_tr   = r2(y_train_np, y_train_pred)\n",
    "        r2_v    = r2(y_val_np, y_val_pred)\n",
    "\n",
    "        enet_results.append({\n",
    "            \"alpha\": a,\n",
    "            \"l1_ratio\": l1r,\n",
    "            \"rmse_train\": rmse_tr,\n",
    "            \"rmse_val\": rmse_v,\n",
    "            \"r2_train\": r2_tr,\n",
    "            \"r2_val\": r2_v,\n",
    "            \"model\": enet\n",
    "        })\n",
    "\n",
    "        print(f\"RMSE_train={rmse_tr:.4f} | RMSE_val={rmse_v:.4f} | \"\n",
    "              f\"R2_train={r2_tr:.4f} | R2_val={r2_v:.4f}\\n\")\n",
    "\n",
    "\n",
    "# Ordenar por el mejor RMSE de validación\n",
    "enet_sorted = sorted(enet_results, key=lambda d: d[\"rmse_val\"])\n",
    "\n",
    "print(\"\\n=== Resumen ElasticNet ordenado por RMSE_val ===\")\n",
    "for r in enet_sorted[:10]:    # mostramos los mejores 10\n",
    "    print(\n",
    "        f\"alpha={r['alpha']:<8} \"\n",
    "        f\"l1_ratio={r['l1_ratio']} \"\n",
    "        f\"rmse_train={r['rmse_train']:.4f} \"\n",
    "        f\"rmse_val={r['rmse_val']:.4f} \"\n",
    "        f\"r2_train={r['r2_train']:.4f} \"\n",
    "        f\"r2_val={r['r2_val']:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1596fdf-72f4-44ec-af27-4bb2e7139646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset usado: (200000, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': {'rmse_val': 5.954805108610575,\n",
       "  'r2_val': 0.8246494767416739,\n",
       "  'time': 0.11396646499633789},\n",
       " 'Ridge alpha=1e-05': {'rmse_val': 5.949834800279131,\n",
       "  'r2_val': 0.8249420748782204,\n",
       "  'time': 0.07804679870605469},\n",
       " 'Ridge alpha=0.0001': {'rmse_val': 5.949834798690889,\n",
       "  'r2_val': 0.8249420749716799,\n",
       "  'time': 0.07930827140808105},\n",
       " 'Ridge alpha=0.001': {'rmse_val': 5.949834782808492,\n",
       "  'r2_val': 0.8249420759062738,\n",
       "  'time': 0.07838869094848633},\n",
       " 'Lasso alpha=1e-05': {'rmse_val': 5.9498129517985605,\n",
       "  'r2_val': 0.824943360541699,\n",
       "  'time': 25.337653398513794},\n",
       " 'Lasso alpha=0.0001': {'rmse_val': 5.949616318346497,\n",
       "  'r2_val': 0.8249549311314979,\n",
       "  'time': 8.140036821365356},\n",
       " 'Elastic alpha=1e-05': {'rmse_val': 5.949806381752295,\n",
       "  'r2_val': 0.8249437471520327,\n",
       "  'time': 26.522173166275024},\n",
       " 'Elastic alpha=0.0001': {'rmse_val': 5.94954911048062,\n",
       "  'r2_val': 0.8249588857861967,\n",
       "  'time': 20.194182634353638}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 14B. Modelos sklearn con SUBSET (200k filas)\n",
    "# ============================================================\n",
    "\n",
    "subset = 200_000\n",
    "idx = np.random.choice(len(X_train_scaled), subset, replace=False)\n",
    "\n",
    "Xtr_s = X_train_scaled[idx]\n",
    "ytr_s = y_train_np[idx]\n",
    "\n",
    "print(\"Subset usado:\", Xtr_s.shape)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings, time, numpy as np\n",
    "\n",
    "def eval_model(model):\n",
    "    t0 = time.time()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "        model.fit(Xtr_s, ytr_s)\n",
    "    t1 = time.time()\n",
    "\n",
    "    pred_v = model.predict(X_val_scaled)\n",
    "\n",
    "    return {\n",
    "        \"rmse_val\": np.sqrt(mean_squared_error(y_val_np, pred_v)),\n",
    "        \"r2_val\": r2_score(y_val_np, pred_v),\n",
    "        \"time\": t1 - t0\n",
    "    }\n",
    "\n",
    "sk_results = {}\n",
    "\n",
    "sk_results[\"LinearRegression\"] = eval_model(LinearRegression())\n",
    "\n",
    "for a in [1e-5, 1e-4, 1e-3]:\n",
    "    sk_results[f\"Ridge alpha={a}\"] = eval_model(Ridge(alpha=a))\n",
    "\n",
    "for a in [1e-5, 1e-4]:\n",
    "    sk_results[f\"Lasso alpha={a}\"] = eval_model(Lasso(alpha=a, max_iter=5000))\n",
    "\n",
    "for a in [1e-5, 1e-4]:\n",
    "    sk_results[f\"Elastic alpha={a}\"] = eval_model(ElasticNet(alpha=a, l1_ratio=0.5, max_iter=5000))\n",
    "\n",
    "sk_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a1e2d71-f420-4748-a2be-60f24fc23575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Modelos sklearn ordenados por RMSE_val ===\n",
      "          model_name  rmse_val  r2_val    time\n",
      "Elastic alpha=0.0001    5.9495  0.8250 20.1942\n",
      "  Lasso alpha=0.0001    5.9496  0.8250  8.1400\n",
      " Elastic alpha=1e-05    5.9498  0.8249 26.5222\n",
      "   Lasso alpha=1e-05    5.9498  0.8249 25.3377\n",
      "   Ridge alpha=0.001    5.9498  0.8249  0.0784\n",
      "  Ridge alpha=0.0001    5.9498  0.8249  0.0793\n",
      "   Ridge alpha=1e-05    5.9498  0.8249  0.0780\n",
      "    LinearRegression    5.9548  0.8246  0.1140\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir sk_results (dict) a DataFrame\n",
    "sk_df = (\n",
    "    pd.DataFrame.from_dict(sk_results, orient=\"index\")\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"model_name\"})\n",
    ")\n",
    "\n",
    "# Ordenar por RMSE de validación (menor es mejor)\n",
    "sk_df_sorted = sk_df.sort_values(\"rmse_val\")\n",
    "\n",
    "print(\"=== Modelos sklearn ordenados por RMSE_val ===\")\n",
    "print(sk_df_sorted.to_string(index=False, float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd9854cf-40aa-4d33-9785-ab9ac81d4629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>alpha</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>16.5403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0931</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>7.5403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.8176</td>\n",
       "      <td>0.0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ElasticNet Scratch</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5.0102</td>\n",
       "      <td>6.1091</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Scratch</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0103</td>\n",
       "      <td>6.1093</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8166</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso Scratch</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0102</td>\n",
       "      <td>6.1122</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0102</td>\n",
       "      <td>6.1130</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>0.8164</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model   alpha  l1_ratio  rmse_train  rmse_val  r2_train  \\\n",
       "7          ElasticNet  0.0001       NaN         NaN    6.0930       NaN   \n",
       "6               Lasso  0.0001       NaN         NaN    6.0931       NaN   \n",
       "4    LinearRegression     NaN       NaN         NaN    6.0933       NaN   \n",
       "5               Ridge  0.0010       NaN         NaN    6.0933       NaN   \n",
       "3  ElasticNet Scratch  0.0010       0.9      5.0102    6.1091    0.8714   \n",
       "1       Ridge Scratch  0.0005       NaN      5.0103    6.1093    0.8714   \n",
       "2       Lasso Scratch  0.0001       NaN      5.0102    6.1122    0.8714   \n",
       "0      Linear Scratch     NaN       NaN      5.0102    6.1130    0.8714   \n",
       "\n",
       "   r2_val     time  \n",
       "7  0.8176  16.5403  \n",
       "6  0.8176   7.5403  \n",
       "4  0.8176   0.1232  \n",
       "5  0.8176   0.0809  \n",
       "3  0.8166      NaN  \n",
       "1  0.8166      NaN  \n",
       "2  0.8164      NaN  \n",
       "0  0.8164      NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TABLA COMPARATIVA FINAL – MODELOS SCRATCH VS SKLEARN\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================\n",
    "# 1. Resultados scratch (manual)\n",
    "# ============================================\n",
    "\n",
    "scratch_results = [\n",
    "    {\n",
    "        \"model\": \"Linear Scratch\",\n",
    "        \"alpha\": None,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_train\": 5.0102,\n",
    "        \"rmse_val\": 6.1130,\n",
    "        \"r2_train\": 0.8714,\n",
    "        \"r2_val\": 0.8164,\n",
    "        \"time\": None,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Ridge Scratch\",\n",
    "        \"alpha\": 0.0005,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_train\": 5.0103,\n",
    "        \"rmse_val\": 6.1093,\n",
    "        \"r2_train\": 0.8714,\n",
    "        \"r2_val\": 0.8166,\n",
    "        \"time\": None,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Lasso Scratch\",\n",
    "        \"alpha\": 0.0001,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_train\": 5.0102,\n",
    "        \"rmse_val\": 6.1122,\n",
    "        \"r2_train\": 0.8714,\n",
    "        \"r2_val\": 0.8164,\n",
    "        \"time\": None,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ElasticNet Scratch\",\n",
    "        \"alpha\": 0.001,\n",
    "        \"l1_ratio\": 0.9,   # cualquiera de los tres valores top\n",
    "        \"rmse_train\": 5.0102,\n",
    "        \"rmse_val\": 6.1091,\n",
    "        \"r2_train\": 0.8714,\n",
    "        \"r2_val\": 0.8166,\n",
    "        \"time\": None,\n",
    "    }\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# 2. Resultados sklearn (de tu resumen)\n",
    "# ============================================\n",
    "\n",
    "sk_results = [\n",
    "    {\n",
    "        \"model\": \"LinearRegression\",\n",
    "        \"alpha\": None,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_val\": 6.0933,\n",
    "        \"r2_val\": 0.8176,\n",
    "        \"time\": 0.1232,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Ridge\",\n",
    "        \"alpha\": 0.001,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_val\": 6.0933,\n",
    "        \"r2_val\": 0.8176,\n",
    "        \"time\": 0.0809,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Lasso\",\n",
    "        \"alpha\": 0.0001,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_val\": 6.0931,\n",
    "        \"r2_val\": 0.8176,\n",
    "        \"time\": 7.5403,\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"ElasticNet\",\n",
    "        \"alpha\": 0.0001,\n",
    "        \"l1_ratio\": None,\n",
    "        \"rmse_val\": 6.0930,\n",
    "        \"r2_val\": 0.8176,\n",
    "        \"time\": 16.5403,\n",
    "    },\n",
    "]\n",
    "\n",
    "# ============================================\n",
    "# 3. Unir todo en un DataFrame\n",
    "# ============================================\n",
    "\n",
    "df_final = pd.DataFrame(scratch_results + sk_results)\n",
    "\n",
    "# Ordenar por RMSE_val\n",
    "df_final = df_final.sort_values(by=\"rmse_val\")\n",
    "\n",
    "df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a032a88a-a1fc-48b0-9b04-9704063d4be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Train loss: 79.2041 - Val loss: 66.4637\n",
      "Epoch 2/20 - Train loss: 35.0463 - Val loss: 31.7317\n",
      "Epoch 3/20 - Train loss: 27.5970 - Val loss: 32.0584\n",
      "Epoch 4/20 - Train loss: 25.7917 - Val loss: 33.6669\n",
      "Epoch 5/20 - Train loss: 25.2239 - Val loss: 34.4895\n",
      "Epoch 6/20 - Train loss: 25.0202 - Val loss: 34.9120\n",
      "Epoch 7/20 - Train loss: 24.9439 - Val loss: 35.1119\n",
      "Epoch 8/20 - Train loss: 24.9142 - Val loss: 35.2159\n",
      "Epoch 9/20 - Train loss: 24.9029 - Val loss: 35.2905\n",
      "Epoch 10/20 - Train loss: 24.8992 - Val loss: 35.3243\n",
      "Epoch 11/20 - Train loss: 24.8971 - Val loss: 35.3124\n",
      "Epoch 12/20 - Train loss: 24.8962 - Val loss: 35.3182\n",
      "Epoch 13/20 - Train loss: 24.8962 - Val loss: 35.3472\n",
      "Epoch 14/20 - Train loss: 24.8968 - Val loss: 35.3853\n",
      "Epoch 15/20 - Train loss: 24.8973 - Val loss: 35.3782\n",
      "Epoch 16/20 - Train loss: 24.8960 - Val loss: 35.3620\n",
      "Epoch 17/20 - Train loss: 24.8961 - Val loss: 35.3490\n",
      "Epoch 18/20 - Train loss: 24.8961 - Val loss: 35.3830\n",
      "Epoch 19/20 - Train loss: 24.8963 - Val loss: 35.3486\n",
      "Epoch 20/20 - Train loss: 24.8966 - Val loss: 35.3308\n",
      "Modelo base listo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base = LinearRegressorScratch(\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=20,\n",
    "    batch_size=2048,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "base.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_np,\n",
    "    X_val=X_val_scaled,\n",
    "    y_val=y_val_np\n",
    ")\n",
    "\n",
    "print(\"Modelo base listo.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38ff0926-5fe2-4597-9464-9f4ac91bf593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 2) MÉTRICAS SKLEARN (corregido para tu variable sk_results)\n",
    "# ============================================================\n",
    "\n",
    "sk_rows = []\n",
    "\n",
    "for item in sk_results:\n",
    "    sk_rows.append({\n",
    "        \"model\": item[\"model\"],\n",
    "        \"type\": \"sklearn\",\n",
    "        \"alpha\": item.get(\"alpha\", None),\n",
    "        \"l1_ratio\": item.get(\"l1_ratio\", None),\n",
    "        \"rmse_train\": None,\n",
    "        \"rmse_val\": item[\"rmse_val\"],\n",
    "        \"mae_train\": None,\n",
    "        \"mae_val\": None,\n",
    "        \"r2_train\": None,\n",
    "        \"r2_val\": item[\"r2_val\"],\n",
    "        \"time_sec\": item[\"time\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9362bbc8-825a-466a-a7af-71278481f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tipo: <class 'list'>\n",
      "Longitud: 4\n",
      "Ejemplo de elemento: <class 'dict'>\n",
      "{'model': 'LinearRegression', 'alpha': None, 'l1_ratio': None, 'rmse_val': 6.0933, 'r2_val': 0.8176, 'time': 0.1232}\n"
     ]
    }
   ],
   "source": [
    "  print(\"Tipo:\", type(sk_results_dict))\n",
    "  print(\"Longitud:\", len(sk_results_dict))\n",
    "\n",
    "  if isinstance(sk_results_dict, dict):\n",
    "      print(\"Primeras claves:\", list(sk_results_dict.keys())[:5])\n",
    "      sample_value = next(iter(sk_results_dict.values()))\n",
    "      print(\"Ejemplo de valor:\", type(sample_value), sample_value)\n",
    "  elif isinstance(sk_results_dict, list):\n",
    "      print(\"Ejemplo de elemento:\", type(sk_results_dict[0]))\n",
    "      print(sk_results_dict[0])\n",
    "  else:\n",
    "      print(\"Contenido:\", sk_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6589187-9c37-4a52-9cbe-8fa2ae393f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40 - Train loss: 79.2804\n",
      "Epoch 2/40 - Train loss: 35.0945\n",
      "Epoch 3/40 - Train loss: 27.6673\n",
      "Epoch 4/40 - Train loss: 25.8655\n",
      "Epoch 5/40 - Train loss: 25.3025\n",
      "Epoch 6/40 - Train loss: 25.1007\n",
      "Epoch 7/40 - Train loss: 25.0234\n",
      "Epoch 8/40 - Train loss: 24.9956\n",
      "Epoch 9/40 - Train loss: 24.9843\n",
      "Epoch 10/40 - Train loss: 24.9801\n",
      "Epoch 11/40 - Train loss: 24.9782\n",
      "Epoch 12/40 - Train loss: 24.9775\n",
      "Epoch 13/40 - Train loss: 24.9776\n",
      "Epoch 14/40 - Train loss: 24.9778\n",
      "Epoch 15/40 - Train loss: 24.9774\n",
      "Epoch 16/40 - Train loss: 24.9774\n",
      "Epoch 17/40 - Train loss: 24.9775\n",
      "Epoch 18/40 - Train loss: 24.9775\n",
      "Epoch 19/40 - Train loss: 24.9776\n",
      "Epoch 20/40 - Train loss: 24.9777\n",
      "Epoch 21/40 - Train loss: 24.9783\n",
      "Epoch 22/40 - Train loss: 24.9788\n",
      "Epoch 23/40 - Train loss: 24.9773\n",
      "Epoch 24/40 - Train loss: 24.9781\n",
      "Epoch 25/40 - Train loss: 24.9776\n",
      "Epoch 26/40 - Train loss: 24.9776\n",
      "Epoch 27/40 - Train loss: 24.9776\n",
      "Epoch 28/40 - Train loss: 24.9777\n",
      "Epoch 29/40 - Train loss: 24.9793\n",
      "Epoch 30/40 - Train loss: 24.9787\n",
      "Epoch 31/40 - Train loss: 24.9780\n",
      "Epoch 32/40 - Train loss: 24.9788\n",
      "Epoch 33/40 - Train loss: 24.9775\n",
      "Epoch 34/40 - Train loss: 24.9775\n",
      "Epoch 35/40 - Train loss: 24.9775\n",
      "Epoch 36/40 - Train loss: 24.9773\n",
      "Epoch 37/40 - Train loss: 24.9773\n",
      "Epoch 38/40 - Train loss: 24.9776\n",
      "Epoch 39/40 - Train loss: 24.9776\n",
      "Epoch 40/40 - Train loss: 24.9776\n",
      "Epoch 1/40 - Train loss: 79.1895\n",
      "Epoch 2/40 - Train loss: 35.0566\n",
      "Epoch 3/40 - Train loss: 27.5967\n",
      "Epoch 4/40 - Train loss: 25.7938\n",
      "Epoch 5/40 - Train loss: 25.2243\n",
      "Epoch 6/40 - Train loss: 25.0207\n",
      "Epoch 7/40 - Train loss: 24.9449\n",
      "Epoch 8/40 - Train loss: 24.9159\n",
      "Epoch 9/40 - Train loss: 24.9048\n",
      "Epoch 10/40 - Train loss: 24.9025\n",
      "Epoch 11/40 - Train loss: 24.8990\n",
      "Epoch 12/40 - Train loss: 24.8985\n",
      "Epoch 13/40 - Train loss: 24.8979\n",
      "Epoch 14/40 - Train loss: 24.8983\n",
      "Epoch 15/40 - Train loss: 24.8976\n",
      "Epoch 16/40 - Train loss: 24.8979\n",
      "Epoch 17/40 - Train loss: 24.8976\n",
      "Epoch 18/40 - Train loss: 24.8983\n",
      "Epoch 19/40 - Train loss: 24.8982\n",
      "Epoch 20/40 - Train loss: 24.8978\n",
      "Epoch 21/40 - Train loss: 24.8977\n",
      "Epoch 22/40 - Train loss: 24.8979\n",
      "Epoch 23/40 - Train loss: 24.8981\n",
      "Epoch 24/40 - Train loss: 24.8991\n",
      "Epoch 25/40 - Train loss: 24.8980\n",
      "Epoch 26/40 - Train loss: 24.8980\n",
      "Epoch 27/40 - Train loss: 24.8982\n",
      "Epoch 28/40 - Train loss: 24.9014\n",
      "Epoch 29/40 - Train loss: 24.8996\n",
      "Epoch 30/40 - Train loss: 24.8981\n",
      "Epoch 31/40 - Train loss: 24.8976\n",
      "Epoch 32/40 - Train loss: 24.8980\n",
      "Epoch 33/40 - Train loss: 24.8988\n",
      "Epoch 34/40 - Train loss: 24.8978\n",
      "Epoch 35/40 - Train loss: 24.8984\n",
      "Epoch 36/40 - Train loss: 24.8982\n",
      "Epoch 37/40 - Train loss: 24.8982\n",
      "Epoch 38/40 - Train loss: 24.8977\n",
      "Epoch 39/40 - Train loss: 24.8978\n",
      "Epoch 40/40 - Train loss: 24.8978\n",
      "Epoch 1/40 - Train loss: 79.2767\n",
      "Epoch 2/40 - Train loss: 35.0751\n",
      "Epoch 3/40 - Train loss: 27.6223\n",
      "Epoch 4/40 - Train loss: 25.8213\n",
      "Epoch 5/40 - Train loss: 25.2526\n",
      "Epoch 6/40 - Train loss: 25.0514\n",
      "Epoch 7/40 - Train loss: 24.9763\n",
      "Epoch 8/40 - Train loss: 24.9464\n",
      "Epoch 9/40 - Train loss: 24.9365\n",
      "Epoch 10/40 - Train loss: 24.9314\n",
      "Epoch 11/40 - Train loss: 24.9304\n",
      "Epoch 12/40 - Train loss: 24.9293\n",
      "Epoch 13/40 - Train loss: 24.9292\n",
      "Epoch 14/40 - Train loss: 24.9287\n",
      "Epoch 15/40 - Train loss: 24.9288\n",
      "Epoch 16/40 - Train loss: 24.9287\n",
      "Epoch 17/40 - Train loss: 24.9288\n",
      "Epoch 18/40 - Train loss: 24.9294\n",
      "Epoch 19/40 - Train loss: 24.9292\n",
      "Epoch 20/40 - Train loss: 24.9303\n",
      "Epoch 21/40 - Train loss: 24.9288\n",
      "Epoch 22/40 - Train loss: 24.9289\n",
      "Epoch 23/40 - Train loss: 24.9294\n",
      "Epoch 24/40 - Train loss: 24.9288\n",
      "Epoch 25/40 - Train loss: 24.9292\n",
      "Epoch 26/40 - Train loss: 24.9289\n",
      "Epoch 27/40 - Train loss: 24.9286\n",
      "Epoch 28/40 - Train loss: 24.9292\n",
      "Epoch 29/40 - Train loss: 24.9287\n",
      "Epoch 30/40 - Train loss: 24.9288\n",
      "Epoch 31/40 - Train loss: 24.9296\n",
      "Epoch 32/40 - Train loss: 24.9291\n",
      "Epoch 33/40 - Train loss: 24.9289\n",
      "Epoch 34/40 - Train loss: 24.9292\n",
      "Epoch 35/40 - Train loss: 24.9286\n",
      "Epoch 36/40 - Train loss: 24.9288\n",
      "Epoch 37/40 - Train loss: 24.9288\n",
      "Epoch 38/40 - Train loss: 24.9288\n",
      "Epoch 39/40 - Train loss: 24.9287\n",
      "Epoch 40/40 - Train loss: 24.9292\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>alpha</th>\n",
       "      <th>l1_ratio</th>\n",
       "      <th>rmse_train</th>\n",
       "      <th>rmse_val</th>\n",
       "      <th>mae_train</th>\n",
       "      <th>mae_val</th>\n",
       "      <th>r2_train</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>time_sec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scratch-Ridge</td>\n",
       "      <td>scratch</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.989618</td>\n",
       "      <td>5.943744</td>\n",
       "      <td>2.756459</td>\n",
       "      <td>3.760898</td>\n",
       "      <td>0.871930</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scratch-Linear</td>\n",
       "      <td>scratch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.989649</td>\n",
       "      <td>5.943971</td>\n",
       "      <td>2.755781</td>\n",
       "      <td>3.762042</td>\n",
       "      <td>0.871929</td>\n",
       "      <td>0.825287</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scratch-ElasticNet</td>\n",
       "      <td>scratch</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.9</td>\n",
       "      <td>4.989623</td>\n",
       "      <td>5.946785</td>\n",
       "      <td>2.755206</td>\n",
       "      <td>3.760346</td>\n",
       "      <td>0.871930</td>\n",
       "      <td>0.825122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scratch-Lasso</td>\n",
       "      <td>scratch</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.989581</td>\n",
       "      <td>5.950565</td>\n",
       "      <td>2.754963</td>\n",
       "      <td>3.767082</td>\n",
       "      <td>0.871932</td>\n",
       "      <td>0.824899</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.093000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>16.5403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.093100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>7.5403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.093300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.093300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.0809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                model     type   alpha  l1_ratio  rmse_train  rmse_val  \\\n",
       "0       Scratch-Ridge  scratch  0.0005       NaN    4.989618  5.943744   \n",
       "1      Scratch-Linear  scratch     NaN       NaN    4.989649  5.943971   \n",
       "2  Scratch-ElasticNet  scratch  0.0010       0.9    4.989623  5.946785   \n",
       "3       Scratch-Lasso  scratch  0.0001       NaN    4.989581  5.950565   \n",
       "4          ElasticNet  sklearn  0.0001       NaN         NaN  6.093000   \n",
       "5               Lasso  sklearn  0.0001       NaN         NaN  6.093100   \n",
       "6    LinearRegression  sklearn     NaN       NaN         NaN  6.093300   \n",
       "7               Ridge  sklearn  0.0010       NaN         NaN  6.093300   \n",
       "\n",
       "   mae_train   mae_val  r2_train    r2_val  time_sec  \n",
       "0   2.756459  3.760898  0.871930  0.825300       NaN  \n",
       "1   2.755781  3.762042  0.871929  0.825287       NaN  \n",
       "2   2.755206  3.760346  0.871930  0.825122       NaN  \n",
       "3   2.754963  3.767082  0.871932  0.824899       NaN  \n",
       "4        NaN       NaN       NaN  0.817600   16.5403  \n",
       "5        NaN       NaN       NaN  0.817600    7.5403  \n",
       "6        NaN       NaN       NaN  0.817600    0.1232  \n",
       "7        NaN       NaN       NaN  0.817600    0.0809  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 11. Tabla comparativa final: scratch vs sklearn\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def mae(y, y_pred):\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1) Métricas de MODELOS SCRATCH (ya entrenados)\n",
    "# ============================================================\n",
    "\n",
    "# --- Modelo Lineal Base ---\n",
    "y_train_pred_base = base.predict(X_train_scaled)\n",
    "y_val_pred_base   = base.predict(X_val_scaled)\n",
    "\n",
    "scratch_results = []\n",
    "\n",
    "scratch_results.append({\n",
    "    \"model\": \"Scratch-Linear\",\n",
    "    \"type\": \"scratch\",\n",
    "    \"alpha\": None,\n",
    "    \"l1_ratio\": None,\n",
    "    \"rmse_train\": rmse(y_train_np, y_train_pred_base),\n",
    "    \"rmse_val\": rmse(y_val_np, y_val_pred_base),\n",
    "    \"mae_train\": mae(y_train_np, y_train_pred_base),\n",
    "    \"mae_val\": mae(y_val_np, y_val_pred_base),\n",
    "    \"r2_train\": r2(y_train_np, y_train_pred_base),\n",
    "    \"r2_val\": r2(y_val_np, y_val_pred_base),\n",
    "})\n",
    "\n",
    "\n",
    "# --- Ridge Scratch (mejor alpha ya encontrado) ---\n",
    "best_alpha_ridge = 0.0005   # viene del ranking scratch que ya generaste\n",
    "ridge_best = LinearRegressorScratch(\n",
    "    learning_rate=1e-3, max_iter=40, batch_size=2048,\n",
    "    penalty=\"l2\", alpha=best_alpha_ridge, random_state=42\n",
    ")\n",
    "ridge_best.fit(X_train_scaled, y_train_np)\n",
    "\n",
    "y_train_pred_r = ridge_best.predict(X_train_scaled)\n",
    "y_val_pred_r   = ridge_best.predict(X_val_scaled)\n",
    "\n",
    "scratch_results.append({\n",
    "    \"model\": \"Scratch-Ridge\",\n",
    "    \"type\": \"scratch\",\n",
    "    \"alpha\": best_alpha_ridge,\n",
    "    \"l1_ratio\": None,\n",
    "    \"rmse_train\": rmse(y_train_np, y_train_pred_r),\n",
    "    \"rmse_val\": rmse(y_val_np, y_val_pred_r),\n",
    "    \"mae_train\": mae(y_train_np, y_train_pred_r),\n",
    "    \"mae_val\": mae(y_val_np, y_val_pred_r),\n",
    "    \"r2_train\": r2(y_train_np, y_train_pred_r),\n",
    "    \"r2_val\": r2(y_val_np, y_val_pred_r),\n",
    "})\n",
    "\n",
    "\n",
    "# --- Lasso Scratch (mejor alpha) ---\n",
    "best_alpha_lasso = 0.0001\n",
    "lasso_best = LinearRegressorScratch(\n",
    "    learning_rate=1e-3, max_iter=40, batch_size=2048,\n",
    "    penalty=\"l1\", alpha=best_alpha_lasso, random_state=42\n",
    ")\n",
    "lasso_best.fit(X_train_scaled, y_train_np)\n",
    "\n",
    "y_train_pred_l = lasso_best.predict(X_train_scaled)\n",
    "y_val_pred_l   = lasso_best.predict(X_val_scaled)\n",
    "\n",
    "scratch_results.append({\n",
    "    \"model\": \"Scratch-Lasso\",\n",
    "    \"type\": \"scratch\",\n",
    "    \"alpha\": best_alpha_lasso,\n",
    "    \"l1_ratio\": None,\n",
    "    \"rmse_train\": rmse(y_train_np, y_train_pred_l),\n",
    "    \"rmse_val\": rmse(y_val_np, y_val_pred_l),\n",
    "    \"mae_train\": mae(y_train_np, y_train_pred_l),\n",
    "    \"mae_val\": mae(y_val_np, y_val_pred_l),\n",
    "    \"r2_train\": r2(y_train_np, y_train_pred_l),\n",
    "    \"r2_val\": r2(y_val_np, y_val_pred_l),\n",
    "})\n",
    "\n",
    "\n",
    "# --- ElasticNet Scratch (mejor alpha + l1_ratio) ---\n",
    "best_alpha_en  = 0.001\n",
    "best_l1_ratio  = 0.9\n",
    "\n",
    "elastic_best = LinearRegressorScratch(\n",
    "    learning_rate=1e-3, max_iter=40, batch_size=2048,\n",
    "    penalty=\"elasticnet\", alpha=best_alpha_en, l1_ratio=best_l1_ratio,\n",
    "    random_state=42\n",
    ")\n",
    "elastic_best.fit(X_train_scaled, y_train_np)\n",
    "\n",
    "y_train_pred_e = elastic_best.predict(X_train_scaled)\n",
    "y_val_pred_e   = elastic_best.predict(X_val_scaled)\n",
    "\n",
    "scratch_results.append({\n",
    "    \"model\": \"Scratch-ElasticNet\",\n",
    "    \"type\": \"scratch\",\n",
    "    \"alpha\": best_alpha_en,\n",
    "    \"l1_ratio\": best_l1_ratio,\n",
    "    \"rmse_train\": rmse(y_train_np, y_train_pred_e),\n",
    "    \"rmse_val\": rmse(y_val_np, y_val_pred_e),\n",
    "    \"mae_train\": mae(y_train_np, y_train_pred_e),\n",
    "    \"mae_val\": mae(y_val_np, y_val_pred_e),\n",
    "    \"r2_train\": r2(y_train_np, y_train_pred_e),\n",
    "    \"r2_val\": r2(y_val_np, y_val_pred_e),\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) MÉTRICAS SKLEARN (ya calculadas en sk_results_dict)\n",
    "# ============================================================\n",
    "\n",
    "sk_rows = []\n",
    "for entry in sk_results_dict:  # ahora sabemos que es list[dict]\n",
    "    sk_rows.append({\n",
    "        \"model\": entry[\"model\"],\n",
    "        \"type\": \"sklearn\",\n",
    "        \"alpha\": entry.get(\"alpha\"),\n",
    "        \"l1_ratio\": entry.get(\"l1_ratio\"),\n",
    "        \"rmse_train\": None,\n",
    "        \"rmse_val\": entry[\"rmse_val\"],\n",
    "        \"mae_train\": None,\n",
    "        \"mae_val\": None,\n",
    "        \"r2_train\": None,\n",
    "        \"r2_val\": entry[\"r2_val\"],\n",
    "        \"time_sec\": entry[\"time\"],\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) TABLA FINAL\n",
    "# ============================================================\n",
    "\n",
    "df_compare = pd.DataFrame(scratch_results + sk_rows)\n",
    "\n",
    "df_compare_sorted = df_compare.sort_values(\"rmse_val\")\n",
    "df_compare_sorted.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_compare_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5edf9ee5-2202-46ee-863e-d5d5df927670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes Train+Val: (1249612, 24) (1249612,)\n",
      "Epoch 1/60 - Train loss: 69.1559\n",
      "Epoch 2/60 - Train loss: 33.4973\n",
      "Epoch 3/60 - Train loss: 27.5988\n",
      "Epoch 4/60 - Train loss: 26.2533\n",
      "Epoch 5/60 - Train loss: 25.8743\n",
      "Epoch 6/60 - Train loss: 25.7510\n",
      "Epoch 7/60 - Train loss: 25.7113\n",
      "Epoch 8/60 - Train loss: 25.6974\n",
      "Epoch 9/60 - Train loss: 25.6929\n",
      "Epoch 10/60 - Train loss: 25.6915\n",
      "Epoch 11/60 - Train loss: 25.6907\n",
      "Epoch 12/60 - Train loss: 25.6916\n",
      "Epoch 13/60 - Train loss: 25.6921\n",
      "Epoch 14/60 - Train loss: 25.6895\n",
      "Epoch 15/60 - Train loss: 25.6896\n",
      "Epoch 16/60 - Train loss: 25.6899\n",
      "Epoch 17/60 - Train loss: 25.6931\n",
      "Epoch 18/60 - Train loss: 25.6893\n",
      "Epoch 19/60 - Train loss: 25.6904\n",
      "Epoch 20/60 - Train loss: 25.6905\n",
      "Epoch 21/60 - Train loss: 25.6906\n",
      "Epoch 22/60 - Train loss: 25.6904\n",
      "Epoch 23/60 - Train loss: 25.6892\n",
      "Epoch 24/60 - Train loss: 25.6892\n",
      "Epoch 25/60 - Train loss: 25.6917\n",
      "Epoch 26/60 - Train loss: 25.6927\n",
      "Epoch 27/60 - Train loss: 25.6898\n",
      "Epoch 28/60 - Train loss: 25.6896\n",
      "Epoch 29/60 - Train loss: 25.6896\n",
      "Epoch 30/60 - Train loss: 25.6897\n",
      "Epoch 31/60 - Train loss: 25.6893\n",
      "Epoch 32/60 - Train loss: 25.6921\n",
      "Epoch 33/60 - Train loss: 25.6894\n",
      "Epoch 34/60 - Train loss: 25.6911\n",
      "Epoch 35/60 - Train loss: 25.6903\n",
      "Epoch 36/60 - Train loss: 25.6893\n",
      "Epoch 37/60 - Train loss: 25.6894\n",
      "Epoch 38/60 - Train loss: 25.6897\n",
      "Epoch 39/60 - Train loss: 25.6896\n",
      "Epoch 40/60 - Train loss: 25.6907\n",
      "Epoch 41/60 - Train loss: 25.6893\n",
      "Epoch 42/60 - Train loss: 25.6905\n",
      "Epoch 43/60 - Train loss: 25.6903\n",
      "Epoch 44/60 - Train loss: 25.6896\n",
      "Epoch 45/60 - Train loss: 25.6896\n",
      "Epoch 46/60 - Train loss: 25.6901\n",
      "Epoch 47/60 - Train loss: 25.6892\n",
      "Epoch 48/60 - Train loss: 25.6899\n",
      "Epoch 49/60 - Train loss: 25.6901\n",
      "Epoch 50/60 - Train loss: 25.6894\n",
      "Epoch 51/60 - Train loss: 25.6893\n",
      "Epoch 52/60 - Train loss: 25.6893\n",
      "Epoch 53/60 - Train loss: 25.6895\n",
      "Epoch 54/60 - Train loss: 25.6894\n",
      "Epoch 55/60 - Train loss: 25.6900\n",
      "Epoch 56/60 - Train loss: 25.6897\n",
      "Epoch 57/60 - Train loss: 25.6902\n",
      "Epoch 58/60 - Train loss: 25.6910\n",
      "Epoch 59/60 - Train loss: 25.6896\n",
      "Epoch 60/60 - Train loss: 25.6894\n",
      "\n",
      "=== Ridge SCRATCH – Evaluación en TEST ===\n",
      "RMSE_test: 8.9756\n",
      "MAE_test : 4.9786\n",
      "R2_test  : 0.8019\n",
      "\n",
      "=== ElasticNet SKLEARN – Evaluación en TEST ===\n",
      "RMSE_test: 8.9666\n",
      "MAE_test : 4.9744\n",
      "R2_test  : 0.8023\n",
      "\n",
      "\n",
      "=== Tabla final: Test ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>r2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scratch-Ridge α=0.0005</td>\n",
       "      <td>8.975565</td>\n",
       "      <td>4.978615</td>\n",
       "      <td>0.801887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sklearn-ElasticNet α=0.0001</td>\n",
       "      <td>8.966626</td>\n",
       "      <td>4.974371</td>\n",
       "      <td>0.802282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  rmse_test  mae_test   r2_test\n",
       "0       Scratch-Ridge α=0.0005   8.975565  4.978615  0.801887\n",
       "1  Sklearn-ElasticNet α=0.0001   8.966626  4.974371  0.802282"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 12. Reentrenar modelos ganadores en TRAIN + VAL y evaluar en TEST\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet as SK_ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# ------------------------\n",
    "# Unir TRAIN + VAL\n",
    "# ------------------------\n",
    "X_trainval = np.vstack([X_train_scaled, X_val_scaled])\n",
    "y_trainval = np.concatenate([y_train_np, y_val_np])\n",
    "\n",
    "print(\"Shapes Train+Val:\", X_trainval.shape, y_trainval.shape)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# A) Modelo ganador SCRATCH: Ridge α = 0.0005\n",
    "# ============================================================\n",
    "\n",
    "ridge_final = LinearRegressorScratch(\n",
    "    learning_rate=1e-3,\n",
    "    max_iter=60,\n",
    "    batch_size=2048,\n",
    "    penalty=\"l2\",\n",
    "    alpha=0.0005,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ridge_final.fit(X_trainval, y_trainval)\n",
    "\n",
    "y_test_pred_ridge = ridge_final.predict(X_test_scaled)\n",
    "\n",
    "ridge_test_rmse = np.sqrt(mean_squared_error(y_test_np, y_test_pred_ridge))\n",
    "ridge_test_mae  = mean_absolute_error(y_test_np, y_test_pred_ridge)\n",
    "ridge_test_r2   = r2_score(y_test_np, y_test_pred_ridge)\n",
    "\n",
    "print(\"\\n=== Ridge SCRATCH – Evaluación en TEST ===\")\n",
    "print(f\"RMSE_test: {ridge_test_rmse:.4f}\")\n",
    "print(f\"MAE_test : {ridge_test_mae:.4f}\")\n",
    "print(f\"R2_test  : {ridge_test_r2:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# B) Modelo ganador SKLEARN: ElasticNet α = 0.0001\n",
    "# ============================================================\n",
    "\n",
    "sk_elastic_final = SK_ElasticNet(\n",
    "    alpha=0.0001,\n",
    "    l1_ratio=0.5,     # (tu mejor probada)\n",
    "    max_iter=5000,\n",
    "    tol=1e-4\n",
    ")\n",
    "\n",
    "sk_elastic_final.fit(X_trainval, y_trainval)\n",
    "\n",
    "y_test_pred_sk = sk_elastic_final.predict(X_test_scaled)\n",
    "\n",
    "sk_test_rmse = np.sqrt(mean_squared_error(y_test_np, y_test_pred_sk))\n",
    "sk_test_mae  = mean_absolute_error(y_test_np, y_test_pred_sk)\n",
    "sk_test_r2   = r2_score(y_test_np, y_test_pred_sk)\n",
    "\n",
    "print(\"\\n=== ElasticNet SKLEARN – Evaluación en TEST ===\")\n",
    "print(f\"RMSE_test: {sk_test_rmse:.4f}\")\n",
    "print(f\"MAE_test : {sk_test_mae:.4f}\")\n",
    "print(f\"R2_test  : {sk_test_r2:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# RESUMEN FINAL\n",
    "# ============================================================\n",
    "\n",
    "final_summary = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": \"Scratch-Ridge α=0.0005\",\n",
    "        \"rmse_test\": ridge_test_rmse,\n",
    "        \"mae_test\": ridge_test_mae,\n",
    "        \"r2_test\": ridge_test_r2\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Sklearn-ElasticNet α=0.0001\",\n",
    "        \"rmse_test\": sk_test_rmse,\n",
    "        \"mae_test\": sk_test_mae,\n",
    "        \"r2_test\": sk_test_r2\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\n\\n=== Tabla final: Test ===\")\n",
    "final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30a97cb9-cdc5-4417-b3a3-0c529a6c2951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residuals summary:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Error por bucket ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0–10</th>\n",
       "      <td>4.969469</td>\n",
       "      <td>4.048107</td>\n",
       "      <td>22382.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10–20</th>\n",
       "      <td>2.394056</td>\n",
       "      <td>1.898035</td>\n",
       "      <td>182105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20–30</th>\n",
       "      <td>5.289497</td>\n",
       "      <td>4.400577</td>\n",
       "      <td>80964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30–40</th>\n",
       "      <td>9.452421</td>\n",
       "      <td>8.117912</td>\n",
       "      <td>25936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40–60</th>\n",
       "      <td>13.308624</td>\n",
       "      <td>10.960375</td>\n",
       "      <td>19900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60–100</th>\n",
       "      <td>22.595377</td>\n",
       "      <td>18.501507</td>\n",
       "      <td>24349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100–200</th>\n",
       "      <td>38.651872</td>\n",
       "      <td>32.814686</td>\n",
       "      <td>2925.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rmse        mae     count\n",
       "bucket                                 \n",
       "0–10      4.969469   4.048107   22382.0\n",
       "10–20     2.394056   1.898035  182105.0\n",
       "20–30     5.289497   4.400577   80964.0\n",
       "30–40     9.452421   8.117912   25936.0\n",
       "40–60    13.308624  10.960375   19900.0\n",
       "60–100   22.595377  18.501507   24349.0\n",
       "100–200  38.651872  32.814686    2925.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 13. Diagnóstico del modelo ganador (ElasticNet)\n",
    "# ============================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "y_true = y_test_np\n",
    "y_pred = y_test_pred_sk  # modelo ganador\n",
    "\n",
    "residuals = y_true - y_pred\n",
    "\n",
    "print(\"Residuals summary:\")\n",
    "pd.DataFrame({\n",
    "    \"mean\": [residuals.mean()],\n",
    "    \"std\": [residuals.std()],\n",
    "    \"p5\": [np.percentile(residuals, 5)],\n",
    "    \"p95\": [np.percentile(residuals, 95)],\n",
    "    \"min\": [residuals.min()],\n",
    "    \"max\": [residuals.max()]\n",
    "})\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. Residuales vs verdadero\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_true, residuals, s=3, alpha=0.2)\n",
    "plt.axhline(0, color='red')\n",
    "plt.xlabel(\"Total Amount real\")\n",
    "plt.ylabel(\"Residual (y_true - y_pred)\")\n",
    "plt.title(\"Residuals vs Real Total Amount\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Histograma de residuales\n",
    "# ============================================================\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.hist(residuals, bins=60, alpha=0.7)\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histograma de Residuales\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Error por bucket de tarifa\n",
    "# ============================================================\n",
    "\n",
    "df_test_tmp = pd.DataFrame({\n",
    "    \"y_true\": y_true,\n",
    "    \"y_pred\": y_pred\n",
    "})\n",
    "df_test_tmp[\"bucket\"] = pd.cut(\n",
    "    df_test_tmp[\"y_true\"],\n",
    "    bins=[0, 10, 20, 30, 40, 60, 100, 200],\n",
    "    labels=[\"0–10\",\"10–20\",\"20–30\",\"30–40\",\"40–60\",\"60–100\",\"100–200\"]\n",
    ")\n",
    "\n",
    "bucket_stats = df_test_tmp.groupby(\"bucket\").apply(\n",
    "    lambda g: pd.Series({\n",
    "        \"rmse\": np.sqrt(mean_squared_error(g.y_true, g.y_pred)),\n",
    "        \"mae\": mean_absolute_error(g.y_true, g.y_pred),\n",
    "        \"count\": len(g)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"\\n=== Error por bucket ===\")\n",
    "bucket_stats\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb4354f4-0257-43ea-8e70-48a1cd68c187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes con polinomiales:\n",
      "(1105692, 29) (143920, 29) (358725, 29)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Añadir PolynomialFeatures SOLO pickup\n",
    "# ============================================================\n",
    "\n",
    "def add_poly_features(df):\n",
    "    df = df.copy()\n",
    "    # variables permitidas\n",
    "    df[\"trip_distance_sq\"]   = df[\"trip_distance\"] ** 2\n",
    "    df[\"pickup_hour_sq\"]     = df[\"pickup_hour\"] ** 2\n",
    "    df[\"passenger_count_sq\"] = df[\"passenger_count\"] ** 2\n",
    "\n",
    "    # interacciones permitidas\n",
    "    df[\"trip_dist_x_hour\"]   = df[\"trip_distance\"] * df[\"pickup_hour\"]\n",
    "    df[\"trip_dist_x_pass\"]   = df[\"trip_distance\"] * df[\"passenger_count\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "# aplicar\n",
    "X_train_poly = add_poly_features(X_train)\n",
    "X_val_poly   = add_poly_features(X_val)\n",
    "X_test_poly  = add_poly_features(X_test)\n",
    "\n",
    "print(\"Shapes con polinomiales:\")\n",
    "print(X_train_poly.shape, X_val_poly.shape, X_test_poly.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f046d1e-ea87-440e-8955-d6c7cf1410ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escalado OK: (1105692, 29)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Re-escalar con polinomiales\n",
    "# ============================================================\n",
    "\n",
    "scaler_poly = StandardScalerScratch()\n",
    "scaler_poly.fit(X_train_poly.values.astype(float))\n",
    "\n",
    "X_train_poly_scaled = scaler_poly.transform(X_train_poly.values.astype(float))\n",
    "X_val_poly_scaled   = scaler_poly.transform(X_val_poly.values.astype(float))\n",
    "X_test_poly_scaled  = scaler_poly.transform(X_test_poly.values.astype(float))\n",
    "\n",
    "y_train_np = y_train_np\n",
    "y_val_np   = y_val_np\n",
    "y_test_np  = y_test_np\n",
    "\n",
    "print(\"Escalado OK:\", X_train_poly_scaled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "301c6257-145e-4fe6-bf9f-65b25c59e4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train loss: 68.3550 - Val loss: 64.5552\n",
      "Epoch 2/30 - Train loss: 35.3606 - Val loss: 40.5200\n",
      "Epoch 3/30 - Train loss: 30.7589 - Val loss: 41.6533\n",
      "Epoch 4/30 - Train loss: 29.5467 - Val loss: 42.2797\n",
      "Epoch 5/30 - Train loss: 28.8374 - Val loss: 41.7856\n",
      "Epoch 6/30 - Train loss: 28.2732 - Val loss: 41.0137\n",
      "Epoch 7/30 - Train loss: 27.7930 - Val loss: 40.1274\n",
      "Epoch 8/30 - Train loss: 27.3798 - Val loss: 39.2573\n",
      "Epoch 9/30 - Train loss: 27.0192 - Val loss: 38.5388\n",
      "Epoch 10/30 - Train loss: 26.7045 - Val loss: 37.9819\n",
      "Epoch 11/30 - Train loss: 26.4321 - Val loss: 37.4817\n",
      "Epoch 12/30 - Train loss: 26.1909 - Val loss: 36.9938\n",
      "Epoch 13/30 - Train loss: 25.9762 - Val loss: 36.3819\n",
      "Epoch 14/30 - Train loss: 25.7849 - Val loss: 36.0697\n",
      "Epoch 15/30 - Train loss: 25.6180 - Val loss: 35.6813\n",
      "Epoch 16/30 - Train loss: 25.4701 - Val loss: 35.3696\n",
      "Epoch 17/30 - Train loss: 25.3384 - Val loss: 35.1455\n",
      "Epoch 18/30 - Train loss: 25.2250 - Val loss: 34.9219\n",
      "Epoch 19/30 - Train loss: 25.1194 - Val loss: 34.7295\n",
      "Epoch 20/30 - Train loss: 25.0281 - Val loss: 34.5297\n",
      "Epoch 21/30 - Train loss: 24.9473 - Val loss: 34.2751\n",
      "Epoch 22/30 - Train loss: 24.8711 - Val loss: 34.1599\n",
      "Epoch 23/30 - Train loss: 24.8066 - Val loss: 34.0099\n",
      "Epoch 24/30 - Train loss: 24.7507 - Val loss: 33.8566\n",
      "Epoch 25/30 - Train loss: 24.6951 - Val loss: 33.8507\n",
      "Epoch 26/30 - Train loss: 24.6491 - Val loss: 33.7298\n",
      "Epoch 27/30 - Train loss: 24.6079 - Val loss: 33.6609\n",
      "Epoch 28/30 - Train loss: 24.5697 - Val loss: 33.4846\n",
      "Epoch 29/30 - Train loss: 24.5364 - Val loss: 33.4414\n",
      "Epoch 30/30 - Train loss: 24.5057 - Val loss: 33.3886\n",
      "Epoch 1/40 - Train loss: 68.3375 - Val loss: 64.4884\n",
      "Epoch 2/40 - Train loss: 35.3586 - Val loss: 40.5260\n",
      "Epoch 3/40 - Train loss: 30.7602 - Val loss: 41.5116\n",
      "Epoch 4/40 - Train loss: 29.5521 - Val loss: 42.2281\n",
      "Epoch 5/40 - Train loss: 28.8438 - Val loss: 41.7241\n",
      "Epoch 6/40 - Train loss: 28.2797 - Val loss: 40.8960\n",
      "Epoch 7/40 - Train loss: 27.8002 - Val loss: 40.1182\n",
      "Epoch 8/40 - Train loss: 27.3869 - Val loss: 39.2404\n",
      "Epoch 9/40 - Train loss: 27.0292 - Val loss: 38.6231\n",
      "Epoch 10/40 - Train loss: 26.7156 - Val loss: 37.9817\n",
      "Epoch 11/40 - Train loss: 26.4404 - Val loss: 37.3971\n",
      "Epoch 12/40 - Train loss: 26.1988 - Val loss: 36.9284\n",
      "Epoch 13/40 - Train loss: 25.9860 - Val loss: 36.4622\n",
      "Epoch 14/40 - Train loss: 25.7981 - Val loss: 36.1163\n",
      "Epoch 15/40 - Train loss: 25.6317 - Val loss: 35.7896\n",
      "Epoch 16/40 - Train loss: 25.4849 - Val loss: 35.4461\n",
      "Epoch 17/40 - Train loss: 25.3570 - Val loss: 35.0512\n",
      "Epoch 18/40 - Train loss: 25.2419 - Val loss: 34.8871\n",
      "Epoch 19/40 - Train loss: 25.1345 - Val loss: 34.7040\n",
      "Epoch 20/40 - Train loss: 25.0427 - Val loss: 34.5126\n",
      "Epoch 21/40 - Train loss: 24.9608 - Val loss: 34.2970\n",
      "Epoch 22/40 - Train loss: 24.8875 - Val loss: 34.1165\n",
      "Epoch 23/40 - Train loss: 24.8226 - Val loss: 34.0750\n",
      "Epoch 24/40 - Train loss: 24.7649 - Val loss: 33.9467\n",
      "Epoch 25/40 - Train loss: 24.7129 - Val loss: 33.8186\n",
      "Epoch 26/40 - Train loss: 24.6668 - Val loss: 33.7288\n",
      "Epoch 27/40 - Train loss: 24.6277 - Val loss: 33.5640\n",
      "Epoch 28/40 - Train loss: 24.5892 - Val loss: 33.6243\n",
      "Epoch 29/40 - Train loss: 24.5540 - Val loss: 33.5052\n",
      "Epoch 30/40 - Train loss: 24.5244 - Val loss: 33.4047\n",
      "Epoch 31/40 - Train loss: 24.4985 - Val loss: 33.4050\n",
      "Epoch 32/40 - Train loss: 24.4732 - Val loss: 33.2986\n",
      "Epoch 33/40 - Train loss: 24.4521 - Val loss: 33.2677\n",
      "Epoch 34/40 - Train loss: 24.4322 - Val loss: 33.2272\n",
      "Epoch 35/40 - Train loss: 24.4173 - Val loss: 33.2177\n",
      "Epoch 36/40 - Train loss: 24.3997 - Val loss: 33.0622\n",
      "Epoch 37/40 - Train loss: 24.3862 - Val loss: 33.0873\n",
      "Epoch 38/40 - Train loss: 24.3741 - Val loss: 32.9691\n",
      "Epoch 39/40 - Train loss: 24.3611 - Val loss: 33.0094\n",
      "Epoch 40/40 - Train loss: 24.3508 - Val loss: 33.0411\n",
      "Epoch 1/40 - Train loss: 68.2977 - Val loss: 64.6542\n",
      "Epoch 2/40 - Train loss: 35.3639 - Val loss: 40.5396\n",
      "Epoch 3/40 - Train loss: 30.7591 - Val loss: 41.5801\n",
      "Epoch 4/40 - Train loss: 29.5479 - Val loss: 42.2151\n",
      "Epoch 5/40 - Train loss: 28.8394 - Val loss: 41.7607\n",
      "Epoch 6/40 - Train loss: 28.2755 - Val loss: 41.0260\n",
      "Epoch 7/40 - Train loss: 27.7951 - Val loss: 40.1391\n",
      "Epoch 8/40 - Train loss: 27.3806 - Val loss: 39.2117\n",
      "Epoch 9/40 - Train loss: 27.0207 - Val loss: 38.5413\n",
      "Epoch 10/40 - Train loss: 26.7076 - Val loss: 38.0159\n",
      "Epoch 11/40 - Train loss: 26.4315 - Val loss: 37.2918\n",
      "Epoch 12/40 - Train loss: 26.1879 - Val loss: 36.8494\n",
      "Epoch 13/40 - Train loss: 25.9744 - Val loss: 36.3971\n",
      "Epoch 14/40 - Train loss: 25.7874 - Val loss: 36.0778\n",
      "Epoch 15/40 - Train loss: 25.6200 - Val loss: 35.7806\n",
      "Epoch 16/40 - Train loss: 25.4724 - Val loss: 35.4171\n",
      "Epoch 17/40 - Train loss: 25.3426 - Val loss: 35.0702\n",
      "Epoch 18/40 - Train loss: 25.2247 - Val loss: 34.8784\n",
      "Epoch 19/40 - Train loss: 25.1238 - Val loss: 34.7397\n",
      "Epoch 20/40 - Train loss: 25.0296 - Val loss: 34.5692\n",
      "Epoch 21/40 - Train loss: 24.9468 - Val loss: 34.3231\n",
      "Epoch 22/40 - Train loss: 24.8750 - Val loss: 34.1065\n",
      "Epoch 23/40 - Train loss: 24.8094 - Val loss: 34.0339\n",
      "Epoch 24/40 - Train loss: 24.7531 - Val loss: 33.9993\n",
      "Epoch 25/40 - Train loss: 24.6989 - Val loss: 33.9093\n",
      "Epoch 26/40 - Train loss: 24.6518 - Val loss: 33.7110\n",
      "Epoch 27/40 - Train loss: 24.6148 - Val loss: 33.7587\n",
      "Epoch 28/40 - Train loss: 24.5719 - Val loss: 33.5407\n",
      "Epoch 29/40 - Train loss: 24.5381 - Val loss: 33.4256\n",
      "Epoch 30/40 - Train loss: 24.5088 - Val loss: 33.4139\n",
      "Epoch 31/40 - Train loss: 24.4813 - Val loss: 33.3290\n",
      "Epoch 32/40 - Train loss: 24.4573 - Val loss: 33.2165\n",
      "Epoch 33/40 - Train loss: 24.4357 - Val loss: 33.1814\n",
      "Epoch 34/40 - Train loss: 24.4157 - Val loss: 33.1555\n",
      "Epoch 35/40 - Train loss: 24.3987 - Val loss: 33.1342\n",
      "Epoch 36/40 - Train loss: 24.3835 - Val loss: 33.0788\n",
      "Epoch 37/40 - Train loss: 24.3701 - Val loss: 33.0829\n",
      "Epoch 38/40 - Train loss: 24.3573 - Val loss: 32.9860\n",
      "Epoch 39/40 - Train loss: 24.3448 - Val loss: 32.9680\n",
      "Epoch 40/40 - Train loss: 24.3337 - Val loss: 32.9854\n",
      "Epoch 1/40 - Train loss: 68.3276 - Val loss: 64.6391\n",
      "Epoch 2/40 - Train loss: 35.3575 - Val loss: 40.6326\n",
      "Epoch 3/40 - Train loss: 30.7606 - Val loss: 41.5565\n",
      "Epoch 4/40 - Train loss: 29.5508 - Val loss: 42.2424\n",
      "Epoch 5/40 - Train loss: 28.8458 - Val loss: 41.8713\n",
      "Epoch 6/40 - Train loss: 28.2777 - Val loss: 40.9816\n",
      "Epoch 7/40 - Train loss: 27.7989 - Val loss: 40.1588\n",
      "Epoch 8/40 - Train loss: 27.3839 - Val loss: 39.2588\n",
      "Epoch 9/40 - Train loss: 27.0238 - Val loss: 38.5871\n",
      "Epoch 10/40 - Train loss: 26.7097 - Val loss: 37.9593\n",
      "Epoch 11/40 - Train loss: 26.4350 - Val loss: 37.3888\n",
      "Epoch 12/40 - Train loss: 26.1934 - Val loss: 36.8744\n",
      "Epoch 13/40 - Train loss: 25.9811 - Val loss: 36.4657\n",
      "Epoch 14/40 - Train loss: 25.7925 - Val loss: 36.0365\n",
      "Epoch 15/40 - Train loss: 25.6265 - Val loss: 35.6492\n",
      "Epoch 16/40 - Train loss: 25.4790 - Val loss: 35.3895\n",
      "Epoch 17/40 - Train loss: 25.3470 - Val loss: 35.1844\n",
      "Epoch 18/40 - Train loss: 25.2307 - Val loss: 34.9621\n",
      "Epoch 19/40 - Train loss: 25.1276 - Val loss: 34.7643\n",
      "Epoch 20/40 - Train loss: 25.0356 - Val loss: 34.5338\n",
      "Epoch 21/40 - Train loss: 24.9534 - Val loss: 34.3234\n",
      "Epoch 22/40 - Train loss: 24.8819 - Val loss: 34.1579\n",
      "Epoch 23/40 - Train loss: 24.8152 - Val loss: 34.0545\n",
      "Epoch 24/40 - Train loss: 24.7572 - Val loss: 33.9460\n",
      "Epoch 25/40 - Train loss: 24.7051 - Val loss: 33.8573\n",
      "Epoch 26/40 - Train loss: 24.6589 - Val loss: 33.6998\n",
      "Epoch 27/40 - Train loss: 24.6165 - Val loss: 33.5762\n",
      "Epoch 28/40 - Train loss: 24.5830 - Val loss: 33.4585\n",
      "Epoch 29/40 - Train loss: 24.5468 - Val loss: 33.4358\n",
      "Epoch 30/40 - Train loss: 24.5165 - Val loss: 33.3388\n",
      "Epoch 31/40 - Train loss: 24.4893 - Val loss: 33.3532\n",
      "Epoch 32/40 - Train loss: 24.4657 - Val loss: 33.2883\n",
      "Epoch 33/40 - Train loss: 24.4440 - Val loss: 33.2116\n",
      "Epoch 34/40 - Train loss: 24.4265 - Val loss: 33.2576\n",
      "Epoch 35/40 - Train loss: 24.4070 - Val loss: 33.1937\n",
      "Epoch 36/40 - Train loss: 24.3912 - Val loss: 33.1473\n",
      "Epoch 37/40 - Train loss: 24.3769 - Val loss: 33.0816\n",
      "Epoch 38/40 - Train loss: 24.3659 - Val loss: 33.1027\n",
      "Epoch 39/40 - Train loss: 24.3524 - Val loss: 32.9931\n",
      "Epoch 40/40 - Train loss: 24.3417 - Val loss: 32.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'name': 'Scratch-Linear-Poly',\n",
       "  'rmse_val': 5.778289879693043,\n",
       "  'r2_val': 0.8348910512415516},\n",
       " {'name': 'Scratch-Ridge-Poly',\n",
       "  'rmse_val': 5.7466158812929065,\n",
       "  'r2_val': 0.8366961968324869},\n",
       " {'name': 'Scratch-Lasso-Poly',\n",
       "  'rmse_val': 5.743088892988668,\n",
       "  'r2_val': 0.836896590895561},\n",
       " {'name': 'Scratch-EN-Poly',\n",
       "  'rmse_val': 5.7414049993444705,\n",
       "  'r2_val': 0.836992221847846}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Entrenar modelos SCRATCH con polinomiales\n",
    "# ============================================================\n",
    "\n",
    "def evaluate_scratch(model, Xtr, ytr, Xv, yv):\n",
    "    model.fit(Xtr, ytr, X_val=Xv, y_val=yv)\n",
    "    yval_pred = model.predict(Xv)\n",
    "    return {\n",
    "        \"rmse_val\": np.sqrt(np.mean((yv - yval_pred)**2)),\n",
    "        \"r2_val\": 1 - np.sum((yv - yval_pred)**2) / np.sum((yv - yv.mean())**2)\n",
    "    }\n",
    "\n",
    "\n",
    "scratch_results_poly = []\n",
    "\n",
    "# LINEAL\n",
    "lin_poly = LinearRegressorScratch(max_iter=30, batch_size=2048)\n",
    "scratch_results_poly.append({\n",
    "    \"name\": \"Scratch-Linear-Poly\",\n",
    "    **evaluate_scratch(lin_poly, X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "})\n",
    "\n",
    "# RIDGE\n",
    "ridge_poly = LinearRegressorScratch(max_iter=40, batch_size=2048, penalty=\"l2\", alpha=1e-4)\n",
    "scratch_results_poly.append({\n",
    "    \"name\": \"Scratch-Ridge-Poly\",\n",
    "    **evaluate_scratch(ridge_poly, X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "})\n",
    "\n",
    "# LASSO\n",
    "lasso_poly = LinearRegressorScratch(max_iter=40, batch_size=2048, penalty=\"l1\", alpha=1e-4)\n",
    "scratch_results_poly.append({\n",
    "    \"name\": \"Scratch-Lasso-Poly\",\n",
    "    **evaluate_scratch(lasso_poly, X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "})\n",
    "\n",
    "# ELASTICNET\n",
    "en_poly = LinearRegressorScratch(max_iter=40, batch_size=2048, penalty=\"elasticnet\", alpha=1e-4, l1_ratio=0.5)\n",
    "scratch_results_poly.append({\n",
    "    \"name\": \"Scratch-EN-Poly\",\n",
    "    **evaluate_scratch(en_poly, X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "})\n",
    "\n",
    "scratch_results_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74d50084-efd7-4ce8-9d34-af94c059ffbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LinearRegression': {'rmse_val': 5.698553417449357,\n",
       "  'r2_val': 0.8394163929303153,\n",
       "  'time': 1.1327028274536133},\n",
       " 'Ridge': {'rmse_val': 5.705299118864158,\n",
       "  'r2_val': 0.8390359840308168,\n",
       "  'time': 0.34879374504089355},\n",
       " 'Lasso': {'rmse_val': 5.7051929593107165,\n",
       "  'r2_val': 0.8390419741493789,\n",
       "  'time': 15.037251234054565},\n",
       " 'ElasticNet': {'rmse_val': 5.705416684037004,\n",
       "  'r2_val': 0.8390293502130486,\n",
       "  'time': 29.689030170440674}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "\n",
    "sk_results_poly = {}\n",
    "\n",
    "def eval_sk(model, Xtr, ytr, Xv, yv):\n",
    "    t0 = time.time()\n",
    "    model.fit(Xtr, ytr)\n",
    "    t1 = time.time()\n",
    "    pred = model.predict(Xv)\n",
    "    return {\n",
    "        \"rmse_val\": np.sqrt(np.mean((yv - pred)**2)),\n",
    "        \"r2_val\": 1 - np.sum((yv - pred)**2) / np.sum((yv - yv.mean())**2),\n",
    "        \"time\": t1 - t0\n",
    "    }\n",
    "\n",
    "\n",
    "sk_results_poly[\"LinearRegression\"] = eval_sk(LinearRegression(), X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "sk_results_poly[\"Ridge\"] = eval_sk(Ridge(alpha=1e-4), X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "sk_results_poly[\"Lasso\"] = eval_sk(Lasso(alpha=1e-4, max_iter=2000), X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "sk_results_poly[\"ElasticNet\"] = eval_sk(ElasticNet(alpha=1e-4, l1_ratio=0.5, max_iter=2000),\n",
    "                                        X_train_poly_scaled, y_train_np, X_val_poly_scaled, y_val_np)\n",
    "\n",
    "sk_results_poly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e24a753a-c310-4372-a38e-df85cbf31aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>rmse_val</th>\n",
       "      <th>r2_val</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>5.698553</td>\n",
       "      <td>0.839416</td>\n",
       "      <td>1.132703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>5.705193</td>\n",
       "      <td>0.839042</td>\n",
       "      <td>15.037251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>5.705299</td>\n",
       "      <td>0.839036</td>\n",
       "      <td>0.348794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>5.705417</td>\n",
       "      <td>0.839029</td>\n",
       "      <td>29.689030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scratch-EN-Poly</td>\n",
       "      <td>scratch</td>\n",
       "      <td>5.741405</td>\n",
       "      <td>0.836992</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scratch-Lasso-Poly</td>\n",
       "      <td>scratch</td>\n",
       "      <td>5.743089</td>\n",
       "      <td>0.836897</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scratch-Ridge-Poly</td>\n",
       "      <td>scratch</td>\n",
       "      <td>5.746616</td>\n",
       "      <td>0.836696</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scratch-Linear-Poly</td>\n",
       "      <td>scratch</td>\n",
       "      <td>5.778290</td>\n",
       "      <td>0.834891</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model     type  rmse_val    r2_val       time\n",
       "4     LinearRegression  sklearn  5.698553  0.839416   1.132703\n",
       "6                Lasso  sklearn  5.705193  0.839042  15.037251\n",
       "5                Ridge  sklearn  5.705299  0.839036   0.348794\n",
       "7           ElasticNet  sklearn  5.705417  0.839029  29.689030\n",
       "3      Scratch-EN-Poly  scratch  5.741405  0.836992        NaN\n",
       "2   Scratch-Lasso-Poly  scratch  5.743089  0.836897        NaN\n",
       "1   Scratch-Ridge-Poly  scratch  5.746616  0.836696        NaN\n",
       "0  Scratch-Linear-Poly  scratch  5.778290  0.834891        NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_comp_val = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": r[\"name\"],\n",
    "        \"type\": \"scratch\",\n",
    "        \"rmse_val\": r[\"rmse_val\"],\n",
    "        \"r2_val\": r[\"r2_val\"]\n",
    "    }\n",
    "    for r in scratch_results_poly\n",
    "] + [\n",
    "    {\n",
    "        \"model\": name,\n",
    "        \"type\": \"sklearn\",\n",
    "        \"rmse_val\": vals[\"rmse_val\"],\n",
    "        \"r2_val\": vals[\"r2_val\"],\n",
    "        \"time\": vals[\"time\"]\n",
    "    }\n",
    "    for name, vals in sk_results_poly.items()\n",
    "])\n",
    "\n",
    "df_comp_val.sort_values(\"rmse_val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e04cd270-f979-45d0-8d5f-d84254e2259a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>type</th>\n",
       "      <th>rmse_test</th>\n",
       "      <th>mae_test</th>\n",
       "      <th>r2_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>8.432711</td>\n",
       "      <td>5.054317</td>\n",
       "      <td>0.825127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>8.432828</td>\n",
       "      <td>5.054276</td>\n",
       "      <td>0.825122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>8.433040</td>\n",
       "      <td>5.054626</td>\n",
       "      <td>0.825113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>sklearn</td>\n",
       "      <td>8.436244</td>\n",
       "      <td>5.052923</td>\n",
       "      <td>0.824980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearRegressorScratch</td>\n",
       "      <td>scratch</td>\n",
       "      <td>8.450167</td>\n",
       "      <td>5.095381</td>\n",
       "      <td>0.824402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearRegressorScratch</td>\n",
       "      <td>scratch</td>\n",
       "      <td>8.456696</td>\n",
       "      <td>5.097731</td>\n",
       "      <td>0.824131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegressorScratch</td>\n",
       "      <td>scratch</td>\n",
       "      <td>8.462316</td>\n",
       "      <td>5.103468</td>\n",
       "      <td>0.823897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegressorScratch</td>\n",
       "      <td>scratch</td>\n",
       "      <td>8.485972</td>\n",
       "      <td>5.130179</td>\n",
       "      <td>0.822911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model     type  rmse_test  mae_test   r2_test\n",
       "5                   Ridge  sklearn   8.432711  5.054317  0.825127\n",
       "6                   Lasso  sklearn   8.432828  5.054276  0.825122\n",
       "7              ElasticNet  sklearn   8.433040  5.054626  0.825113\n",
       "4        LinearRegression  sklearn   8.436244  5.052923  0.824980\n",
       "1  LinearRegressorScratch  scratch   8.450167  5.095381  0.824402\n",
       "3  LinearRegressorScratch  scratch   8.456696  5.097731  0.824131\n",
       "2  LinearRegressorScratch  scratch   8.462316  5.103468  0.823897\n",
       "0  LinearRegressorScratch  scratch   8.485972  5.130179  0.822911"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Evaluar TODOS los modelos en TEST\n",
    "# ============================================================\n",
    "\n",
    "def eval_test(model, Xte, yte):\n",
    "    pred = model.predict(Xte)\n",
    "    rmse = np.sqrt(np.mean((yte - pred)**2))\n",
    "    mae  = np.mean(np.abs(yte - pred))\n",
    "    r2   = 1 - np.sum((yte - pred)**2) / np.sum((yte - yte.mean())**2)\n",
    "    return rmse, mae, r2\n",
    "\n",
    "rows = []\n",
    "\n",
    "# SCRATCH\n",
    "for m in [lin_poly, ridge_poly, lasso_poly, en_poly]:\n",
    "    rmse, mae, r2 = eval_test(m, X_test_poly_scaled, y_test_np)\n",
    "    rows.append({\n",
    "        \"model\": m.__class__.__name__,\n",
    "        \"type\": \"scratch\",\n",
    "        \"rmse_test\": rmse,\n",
    "        \"mae_test\": mae,\n",
    "        \"r2_test\": r2\n",
    "    })\n",
    "\n",
    "# SKLEARN\n",
    "for name, cls in [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"Ridge\", Ridge(alpha=1e-4)),\n",
    "    (\"Lasso\", Lasso(alpha=1e-4, max_iter=2000)),\n",
    "    (\"ElasticNet\", ElasticNet(alpha=1e-4, l1_ratio=0.5, max_iter=2000))\n",
    "]:\n",
    "    cls.fit(X_train_poly_scaled, y_train_np)\n",
    "    rmse, mae, r2 = eval_test(cls, X_test_poly_scaled, y_test_np)\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"type\": \"sklearn\",\n",
    "        \"rmse_test\": rmse,\n",
    "        \"mae_test\": mae,\n",
    "        \"r2_test\": r2\n",
    "    })\n",
    "\n",
    "df_comp_test = pd.DataFrame(rows)\n",
    "df_comp_test.sort_values(\"rmse_test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae73f05-357f-4e31-9d96-dc85fef54450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
